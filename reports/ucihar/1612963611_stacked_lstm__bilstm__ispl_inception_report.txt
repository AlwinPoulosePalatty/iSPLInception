This is the report for the ucihar Dataset

Data Distribution: 

Train:  X -> (7352, 128, 9) Class count -> [1226, 1073, 986, 1286, 1374, 1407] 

                    frequency
Walking             16.675735
Walking_Upstairs    14.594668
Walking_Downstairs  13.411316
Sitting             17.491838
Standing            18.688791
Laying              19.137650

Validation:  X -> (991, 128, 9) Class count -> [161, 155, 136, 167, 190, 182] 

                    frequency
Walking             16.246216
Walking_Upstairs    15.640767
Walking_Downstairs  13.723512
Sitting             16.851665
Standing            19.172554
Laying              18.365288

Test:  X -> (1956, 128, 9) Class count -> [335, 316, 284, 324, 342, 355] 

                    frequency
Walking             17.126789
Walking_Upstairs    16.155418
Walking_Downstairs  14.519427
Sitting             16.564417
Standing            17.484661
Laying              18.149284

sLSTM Model : 2021-02-10 22:34:51.484169
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.693916  0.748504  2.345841      0.512614  0.000500
1    0.273373  0.909684  0.850532      0.800202  0.000500
2    0.205485  0.933079  0.384622      0.909183  0.000500
3    0.156805  0.944369  0.192222      0.962664  0.000500
4    0.159125  0.945049  0.126429      0.974773  0.000500
5    0.157725  0.943689  0.124749      0.956609  0.000500
6    0.157331  0.942465  0.135854      0.956609  0.000500
7    0.145714  0.946817  0.154820      0.964682  0.000500
8    0.141230  0.949538  0.119732      0.976791  0.000500
9    0.132575  0.951850  0.152000      0.949546  0.000500
10   0.128980  0.950626  0.134751      0.972755  0.000500
11   0.131110  0.952122  0.127103      0.953582  0.000500
12   0.133351  0.948585  0.117089      0.954591  0.000500
13   0.129261  0.951170  0.138383      0.962664  0.000500
14   0.127530  0.954706  0.126750      0.966700  0.000500
15   0.125134  0.953210  0.121829      0.955600  0.000500
16   0.138628  0.949946  0.444646      0.891019  0.000500
17   0.139489  0.947633  0.101376      0.961655  0.000500
18   0.130415  0.950354  0.110583      0.966700  0.000500
19   0.129297  0.951850  0.103872      0.961655  0.000500
20   0.118317  0.955658  0.097865      0.970737  0.000500
21   0.119292  0.956338  0.101951      0.971746  0.000500
22   0.119111  0.954842  0.166585      0.935419  0.000500
23   0.128575  0.952394  0.116474      0.968718  0.000500
24   0.119588  0.954570  0.110783      0.968718  0.000500
25   0.118195  0.956610  0.107546      0.971746  0.000500
26   0.123372  0.952802  0.102493      0.967709  0.000500
27   0.122765  0.950898  0.107867      0.960646  0.000500
28   0.119578  0.955386  0.107139      0.973764  0.000500
29   0.114536  0.955658  0.114048      0.974773  0.000500
30   0.114779  0.958379  0.123615      0.962664  0.000500
31   0.120139  0.953482  0.112761      0.967709  0.000500
32   0.116779  0.956202  0.110284      0.950555  0.000500
33   0.118381  0.953618  0.118669      0.947528  0.000500
34   0.118520  0.954842  0.100344      0.968718  0.000500
35   0.117231  0.956338  0.132245      0.945510  0.000500
36   0.115518  0.957291  0.116034      0.973764  0.000500
37   0.111032  0.958651  0.103854      0.967709  0.000500
38   0.116283  0.955930  0.101623      0.979818  0.000500
39   0.111347  0.955386  0.132551      0.963673  0.000500
40   0.121941  0.953754  0.110132      0.972755  0.000500
41   0.115490  0.956474  0.101438      0.965691  0.000500
42   0.117232  0.957563  0.097551      0.957619  0.000500
43   0.111822  0.957155  0.099758      0.979818  0.000500
44   0.116327  0.956338  0.097982      0.979818  0.000500
45   0.125927  0.949265  0.110701      0.956609  0.000500
46   0.112175  0.957018  0.119466      0.965691  0.000500
47   0.114936  0.954706  0.099216      0.974773  0.000500
48   0.112384  0.955794  0.111397      0.956609  0.000400
49   0.108678  0.957155  0.098031      0.956609  0.000400
50   0.111122  0.957155  0.107726      0.967709  0.000400
51   0.109781  0.957835  0.096545      0.970737  0.000400
52   0.105920  0.959331  0.092758      0.970737  0.000400
53   0.106999  0.959059  0.087616      0.971746  0.000400
54   0.104020  0.960827  0.094624      0.976791  0.000400
55   0.103607  0.960011  0.100824      0.969728  0.000400
56   0.103701  0.957699  0.109334      0.956609  0.000400
57   0.103687  0.960283  0.117406      0.957619  0.000400
58   0.102478  0.959059  0.093159      0.959637  0.000400
59   0.105428  0.960419  0.158109      0.953582  0.000400
60   0.110840  0.956474  0.091786      0.966700  0.000400
61   0.101330  0.961235  0.088878      0.974773  0.000400
62   0.108462  0.958787  0.083176      0.975782  0.000400
63   0.108790  0.957291  0.089485      0.959637  0.000400
64   0.111251  0.957699  0.110984      0.962664  0.000400
65   0.109145  0.958651  0.121801      0.949546  0.000400
66   0.105050  0.956066  0.098207      0.970737  0.000400
67   0.108228  0.958923  0.087028      0.974773  0.000400
68   0.107085  0.960419  0.094112      0.964682  0.000400
69   0.106030  0.959331  0.090372      0.975782  0.000400
70   0.112130  0.955114  0.097836      0.961655  0.000400
71   0.105089  0.958107  0.089001      0.977800  0.000400
72   0.102151  0.959739  0.170645      0.963673  0.000320
73   0.096726  0.961915  0.087358      0.979818  0.000320
74   0.098618  0.960147  0.083100      0.971746  0.000320
75   0.114434  0.951442  0.106519      0.947528  0.000320
76   0.109251  0.956066  0.091418      0.964682  0.000320
77   0.108946  0.957427  0.099567      0.972755  0.000320
78   0.096530  0.961643  0.114220      0.970737  0.000320
79   0.100170  0.960827  0.075168      0.986882  0.000320
80   0.097444  0.961915  0.084823      0.973764  0.000320
81   0.095305  0.961371  0.095093      0.978809  0.000320
82   0.099491  0.958787  0.078303      0.978809  0.000320
83   0.097427  0.963955  0.078468      0.981837  0.000320
84   0.098709  0.962187  0.081898      0.978809  0.000320
85   0.101390  0.958923  0.092470      0.966700  0.000320
86   0.093567  0.964227  0.087477      0.973764  0.000320
87   0.096030  0.962867  0.091494      0.974773  0.000320
88   0.101386  0.959331  0.093119      0.968718  0.000320
89   0.097927  0.960691  0.095571      0.970737  0.000320
90   0.100098  0.960691  0.102760      0.979818  0.000320
91   0.096633  0.961235  0.095088      0.979818  0.000320
92   0.100863  0.960283  0.079976      0.982846  0.000320
93   0.093016  0.962187  0.111327      0.973764  0.000320
94   0.096075  0.962595  0.088130      0.979818  0.000320
95   0.094838  0.960419  0.096175      0.976791  0.000320
96   0.092679  0.963683  0.112253      0.977800  0.000320
97   0.089499  0.964772  0.101289      0.966700  0.000320
98   0.095506  0.961235  0.088215      0.976791  0.000320
99   0.090809  0.962595  0.082083      0.976791  0.000320
100  0.091353  0.961507  0.097782      0.973764  0.000320
101  0.092761  0.962051  0.100206      0.974773  0.000320
102  0.086854  0.964091  0.117414      0.962664  0.000320
103  0.095055  0.963003  0.106287      0.972755  0.000320
104  0.095241  0.962323  0.105649      0.971746  0.000320
105  0.088676  0.965044  0.096307      0.975782  0.000320
106  0.094898  0.961507  0.088629      0.975782  0.000320
107  0.089710  0.964091  0.082131      0.969728  0.000320
108  0.092457  0.963819  0.093161      0.971746  0.000320
109  0.087214  0.965044  0.104130      0.977800  0.000320
110  0.091580  0.964091  0.103611      0.962664  0.000320
111  0.095810  0.964772  0.125226      0.965691  0.000320
112  0.091077  0.963275  0.097510      0.967709  0.000320
113  0.083301  0.967084  0.091938      0.970737  0.000256
114  0.083915  0.967492  0.104075      0.959637  0.000256
115  0.090240  0.962731  0.097468      0.968718  0.000256
116  0.089411  0.962459  0.110150      0.968718  0.000256
117  0.085455  0.966268  0.102954      0.969728  0.000256
118  0.093852  0.959059  0.093022      0.978809  0.000256
119  0.086743  0.966948  0.090941      0.980827  0.000256
120  0.089344  0.962595  0.111639      0.968718  0.000256
121  0.089427  0.963411  0.106119      0.974773  0.000256
122  0.085683  0.965180  0.115741      0.966700  0.000256
123  0.084048  0.967356  0.088036      0.973764  0.000256
124  0.085859  0.964772  0.105202      0.965691  0.000205
125  0.083552  0.965316  0.093121      0.974773  0.000205
126  0.086064  0.964363  0.114519      0.972755  0.000205
127  0.085057  0.964635  0.106756      0.972755  0.000205
128  0.089166  0.965588  0.100482      0.972755  0.000205
129  0.079412  0.967356  0.128941      0.971746  0.000205
130  0.086602  0.963547  0.099122      0.978809  0.000205
131  0.085700  0.964908  0.109833      0.969728  0.000205
132  0.084064  0.967356  0.095698      0.979818  0.000205
133  0.082951  0.966812  0.105914      0.975782  0.000205
134  0.086920  0.967764  0.104445      0.969728  0.000205
135  0.084174  0.966268  0.106168      0.974773  0.000205
136  0.087702  0.964635  0.106724      0.971746  0.000205
137  0.084682  0.966676  0.110477      0.970737  0.000205
138  0.083472  0.966540  0.100019      0.975782  0.000205
139  0.086264  0.964363  0.095559      0.974773  0.000205
140  0.081660  0.967220  0.091356      0.972755  0.000164
141  0.080614  0.968444  0.112847      0.967709  0.000164
142  0.086927  0.965452  0.110388      0.971746  0.000164
143  0.079241  0.966948  0.111306      0.968718  0.000164
144  0.079537  0.967220  0.117775      0.967709  0.000164
145  0.081889  0.965724  0.121823      0.968718  0.000164
146  0.076779  0.970076  0.135937      0.968718  0.000164
147  0.082207  0.967764  0.091551      0.976791  0.000164
148  0.087063  0.965996  0.117239      0.972755  0.000164
149  0.083236  0.966676  0.109377      0.969728  0.000164
150  0.077276  0.968444  0.116018      0.969728  0.000164
151  0.080596  0.966676  0.133403      0.964682  0.000164
152  0.079598  0.968036  0.110511      0.968718  0.000164
153  0.080829  0.968852  0.147117      0.961655  0.000164
154  0.084538  0.965180  0.125339      0.966700  0.000164
155  0.077521  0.968308  0.109558      0.960646  0.000164
156  0.078297  0.969260  0.103119      0.969728  0.000164
157  0.080147  0.967628  0.098899      0.972755  0.000131
158  0.078072  0.967628  0.104321      0.968718  0.000131
159  0.078999  0.966812  0.104410      0.973764  0.000131
160  0.075189  0.968444  0.130979      0.964682  0.000131
161  0.073011  0.970620  0.128225      0.961655  0.000131
162  0.074641  0.968716  0.116856      0.964682  0.000131
163  0.075274  0.970620  0.118179      0.966700  0.000131
164  0.076659  0.968988  0.141903      0.964682  0.000131
165  0.074507  0.972388  0.120558      0.966700  0.000131
166  0.081063  0.967084  0.158560      0.960646  0.000131
167  0.080169  0.970620  0.116265      0.960646  0.000131
168  0.077278  0.968580  0.137999      0.965691  0.000131
169  0.079761  0.968308  0.146469      0.959637  0.000131
170  0.087039  0.964635  0.101991      0.967709  0.000131
171  0.078714  0.967900  0.094630      0.967709  0.000131
172  0.077203  0.969668  0.095401      0.965691  0.000105
173  0.077260  0.967764  0.094456      0.970737  0.000105
174  0.083067  0.966132  0.099516      0.967709  0.000105
175  0.080382  0.968036  0.112038      0.963673  0.000105
176  0.076704  0.970484  0.097989      0.970737  0.000105
177  0.077325  0.968852  0.096483      0.971746  0.000105
178  0.081426  0.967084  0.096982      0.963673  0.000105
179  0.082349  0.965724  0.096878      0.965691  0.000105

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization (BatchNo (None, 128, 9)            36        
_________________________________________________________________
preprocess (Dense)           (None, 128, 100)          1000      
_________________________________________________________________
lstm (LSTM)                  (None, 128, 128)          117248    
_________________________________________________________________
dropout (Dropout)            (None, 128, 128)          0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense (Dense)                (None, 100)               12900     
_________________________________________________________________
output (Dense)               (None, 6)                 606       
=================================================================
Total params: 263,374
Trainable params: 263,356
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 4


Test Accuracy: 91.8200433254242
Test Loss: 0.39954328536987305



Classification Report
                    precision    recall  f1-score   support

           Walking       0.89      0.95      0.92       335
  Walking_Upstairs       0.98      0.95      0.96       316
Walking_Downstairs       0.96      0.99      0.97       284
           Sitting       0.83      0.77      0.80       324
          Standing       0.86      0.85      0.86       342
            Laying       0.99      1.00      1.00       355

          accuracy                           0.92      1956
         macro avg       0.92      0.92      0.92      1956
      weighted avg       0.92      0.92      0.92      1956



Confusion Matrix
[[318   4  13   0   0   0]
 [ 15 301   0   0   0   0]
 [  2   2 280   0   0   0]
 [ 23   1   0 251  47   2]
 [  0   0   0  51 291   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               94.93              1.19                3.88     0.00   
Walking_Upstairs       4.75             95.25                0.00     0.00   
Walking_Downstairs     0.70              0.70               98.59     0.00   
Sitting                7.10              0.31                0.00    77.47   
Standing               0.00              0.00                0.00    14.91   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                14.51    0.62  
Standing               85.09    0.00  
Laying                  0.00  100.00  


Finished working on: sLSTM at: 2021-02-10 22:34:54.291855 -> 482.8089623451233

BiLSTM Model : 2021-02-10 22:49:21.467498
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.818315  0.699401  2.717034      0.526741  0.000500
1    0.220669  0.921518  1.254982      0.720484  0.000500
2    0.183719  0.931175  0.422450      0.866801  0.000500
3    0.162667  0.935800  0.345041      0.889001  0.000500
4    0.162780  0.940288  0.136641      0.945510  0.000500
5    0.141457  0.948721  0.103928      0.966700  0.000500
6    0.144045  0.945457  0.119153      0.969728  0.000500
7    0.148004  0.943689  0.123319      0.952573  0.000500
8    0.142867  0.947633  0.107837      0.956609  0.000500
9    0.128373  0.952122  0.102109      0.972755  0.000500
10   0.131168  0.949265  0.097335      0.972755  0.000500
11   0.135095  0.946001  0.119225      0.957619  0.000500
12   0.134946  0.949401  0.111413      0.965691  0.000500
13   0.129119  0.952394  0.117414      0.956609  0.000500
14   0.130691  0.949265  0.162395      0.955600  0.000500
15   0.128342  0.951714  0.093882      0.972755  0.000500
16   0.120636  0.953074  0.090670      0.980827  0.000500
17   0.116938  0.954706  0.104156      0.966700  0.000500
18   0.117602  0.952258  0.129661      0.966700  0.000500
19   0.126233  0.951850  0.099126      0.970737  0.000500
20   0.122435  0.954434  0.103877      0.977800  0.000500
21   0.117007  0.954706  0.101367      0.954591  0.000500
22   0.111121  0.957427  0.090277      0.973764  0.000500
23   0.118974  0.953754  0.092202      0.979818  0.000500
24   0.107690  0.958515  0.116131      0.937437  0.000500
25   0.110679  0.957018  0.102407      0.965691  0.000500
26   0.118012  0.955386  0.094525      0.972755  0.000500
27   0.118943  0.951850  0.128378      0.939455  0.000500
28   0.115336  0.954706  0.098073      0.971746  0.000500
29   0.115295  0.952122  0.110975      0.951564  0.000500
30   0.145466  0.941785  0.119786      0.969728  0.000500
31   0.132594  0.947905  0.106773      0.969728  0.000500
32   0.123616  0.951578  0.100604      0.968718  0.000500
33   0.116129  0.955114  0.111731      0.953582  0.000500
34   0.122989  0.948857  0.183504      0.942482  0.000500
35   0.114791  0.955386  0.107039      0.947528  0.000400
36   0.110435  0.956474  0.091784      0.973764  0.000400
37   0.111161  0.957018  0.091349      0.967709  0.000400
38   0.109489  0.954298  0.081632      0.972755  0.000400
39   0.103834  0.959195  0.133552      0.930373  0.000400
40   0.108850  0.955386  0.086554      0.973764  0.000400
41   0.103106  0.958107  0.092701      0.968718  0.000400
42   0.118524  0.951306  0.104443      0.962664  0.000400
43   0.112336  0.955386  0.088004      0.971746  0.000400
44   0.108109  0.958107  0.098300      0.963673  0.000400
45   0.111960  0.956202  0.105465      0.951564  0.000400
46   0.119398  0.954298  0.132381      0.948537  0.000400
47   0.115476  0.954026  0.103982      0.950555  0.000400
48   0.104594  0.958787  0.096863      0.963673  0.000400
49   0.109810  0.958515  0.122904      0.949546  0.000400
50   0.102755  0.959195  0.120547      0.935419  0.000400
51   0.101342  0.958923  0.115084      0.942482  0.000400
52   0.103596  0.957835  0.113161      0.945510  0.000400
53   0.107738  0.955114  0.123690      0.950555  0.000400
54   0.103631  0.959059  0.110446      0.955600  0.000400
55   0.098724  0.960827  0.104099      0.964682  0.000400
56   0.101989  0.958107  0.126385      0.953582  0.000400
57   0.098225  0.960011  0.113437      0.948537  0.000400
58   0.101982  0.960419  0.116831      0.950555  0.000400
59   0.097695  0.959739  0.139503      0.931382  0.000400
60   0.097336  0.962595  0.103551      0.965691  0.000400
61   0.099708  0.962595  0.107328      0.971746  0.000400
62   0.098849  0.962867  0.105612      0.966700  0.000400
63   0.092920  0.964363  0.121378      0.956609  0.000400
64   0.094693  0.963683  0.112774      0.967709  0.000400
65   0.091616  0.963683  0.140728      0.939455  0.000400
66   0.096430  0.963003  0.113560      0.962664  0.000400
67   0.092654  0.964499  0.098360      0.971746  0.000400
68   0.088668  0.965996  0.117117      0.958628  0.000400
69   0.090122  0.963955  0.107842      0.959637  0.000400
70   0.095076  0.961643  0.144242      0.953582  0.000400
71   0.087616  0.966540  0.122139      0.954591  0.000400
72   0.090542  0.964363  0.113505      0.960646  0.000400
73   0.095304  0.963411  0.110484      0.952573  0.000400
74   0.089832  0.964499  0.087579      0.968718  0.000400
75   0.090270  0.965860  0.068610      0.982846  0.000400
76   0.090353  0.965044  0.071253      0.975782  0.000400
77   0.093263  0.961779  0.078770      0.977800  0.000400
78   0.086678  0.965452  0.114422      0.951564  0.000400
79   0.097059  0.963275  0.097131      0.962664  0.000400
80   0.096185  0.960011  0.084761      0.975782  0.000400
81   0.088861  0.963819  0.077907      0.978809  0.000400
82   0.090342  0.963683  0.084745      0.968718  0.000400
83   0.086535  0.966404  0.077463      0.971746  0.000400
84   0.082813  0.967492  0.128457      0.948537  0.000400
85   0.096842  0.962323  0.079436      0.970737  0.000400
86   0.089335  0.965588  0.081294      0.966700  0.000400
87   0.084435  0.964363  0.080550      0.976791  0.000400
88   0.085879  0.965316  0.088316      0.970737  0.000400
89   0.085154  0.967764  0.090075      0.967709  0.000400
90   0.077944  0.968444  0.077424      0.976791  0.000400
91   0.081793  0.968172  0.081187      0.973764  0.000400
92   0.082401  0.967220  0.093896      0.966700  0.000400
93   0.081674  0.968444  0.087015      0.973764  0.000400
94   0.079429  0.970212  0.074844      0.976791  0.000400
95   0.077977  0.968716  0.090660      0.966700  0.000400
96   0.075268  0.971028  0.108691      0.962664  0.000400
97   0.077776  0.971708  0.081657      0.977800  0.000400
98   0.078080  0.970212  0.120637      0.957619  0.000400
99   0.077517  0.969396  0.106084      0.958628  0.000400
100  0.078224  0.971028  0.083258      0.975782  0.000400
101  0.071059  0.972388  0.076986      0.977800  0.000400
102  0.068871  0.973885  0.117274      0.968718  0.000400
103  0.079033  0.971572  0.084237      0.972755  0.000400
104  0.071877  0.974429  0.079400      0.973764  0.000400
105  0.073068  0.972796  0.132001      0.958628  0.000400
106  0.072240  0.972388  0.097603      0.971746  0.000400
107  0.075075  0.970620  0.089775      0.973764  0.000400
108  0.073747  0.970484  0.085987      0.971746  0.000400
109  0.072595  0.970348  0.095611      0.972755  0.000400
110  0.079923  0.972116  0.141520      0.939455  0.000400
111  0.083677  0.968852  0.089630      0.971746  0.000400
112  0.072919  0.970756  0.094770      0.965691  0.000400
113  0.070181  0.971572  0.116336      0.961655  0.000320
114  0.068183  0.974429  0.100649      0.971746  0.000320
115  0.069229  0.972660  0.101011      0.966700  0.000320
116  0.067250  0.973613  0.094776      0.968718  0.000320
117  0.069575  0.973069  0.104553      0.970737  0.000320
118  0.068847  0.971164  0.096220      0.972755  0.000320
119  0.065029  0.973613  0.120372      0.966700  0.000320
120  0.071622  0.971980  0.092025      0.971746  0.000320
121  0.064862  0.974565  0.086948      0.972755  0.000320
122  0.064247  0.974701  0.083503      0.975782  0.000320
123  0.064413  0.974293  0.099228      0.963673  0.000320
124  0.062182  0.976469  0.088846      0.972755  0.000320
125  0.059243  0.976061  0.108988      0.966700  0.000320
126  0.065624  0.976197  0.092543      0.972755  0.000320
127  0.063116  0.974293  0.090904      0.974773  0.000320
128  0.062072  0.977149  0.089247      0.974773  0.000320
129  0.067822  0.974157  0.096566      0.972755  0.000320
130  0.059821  0.976469  0.117202      0.970737  0.000320
131  0.058029  0.977693  0.107298      0.967709  0.000320
132  0.058033  0.976469  0.088262      0.966700  0.000320
133  0.061624  0.975109  0.111316      0.965691  0.000320
134  0.057424  0.979733  0.120242      0.961655  0.000320
135  0.061978  0.976605  0.074738      0.978809  0.000320
136  0.060357  0.977013  0.128689      0.957619  0.000320
137  0.060687  0.976333  0.103440      0.960646  0.000320
138  0.061012  0.974973  0.110016      0.965691  0.000320
139  0.059503  0.977965  0.091026      0.972755  0.000320
140  0.059374  0.977421  0.110456      0.963673  0.000320
141  0.057994  0.976333  0.106038      0.969728  0.000320
142  0.056026  0.978237  0.094879      0.972755  0.000320
143  0.059286  0.975109  0.093797      0.969728  0.000320
144  0.062588  0.974293  0.087387      0.974773  0.000320
145  0.062706  0.976197  0.101845      0.970737  0.000320
146  0.058292  0.978373  0.085866      0.975782  0.000320
147  0.057761  0.977693  0.101644      0.964682  0.000320
148  0.059545  0.976877  0.122628      0.960646  0.000320
149  0.054296  0.979597  0.095674      0.975782  0.000320
150  0.056921  0.978917  0.097348      0.970737  0.000320
151  0.055416  0.979189  0.109729      0.970737  0.000320
152  0.059537  0.978509  0.093807      0.971746  0.000320
153  0.056724  0.976877  0.087997      0.972755  0.000320
154  0.056095  0.977149  0.105537      0.964682  0.000320
155  0.050096  0.979053  0.083299      0.976791  0.000320
156  0.053909  0.979189  0.092273      0.966700  0.000320
157  0.055225  0.978645  0.124798      0.961655  0.000320
158  0.057588  0.978509  0.116727      0.961655  0.000320
159  0.052195  0.980005  0.126890      0.960646  0.000320
160  0.052214  0.980005  0.081766      0.974773  0.000320
161  0.049060  0.982046  0.102668      0.968718  0.000320
162  0.054218  0.979053  0.088215      0.972755  0.000320
163  0.052324  0.980141  0.089584      0.974773  0.000320
164  0.050753  0.980822  0.095789      0.974773  0.000320
165  0.046581  0.981774  0.098356      0.972755  0.000320
166  0.052846  0.980005  0.095092      0.972755  0.000320
167  0.048606  0.982318  0.072397      0.979818  0.000320
168  0.047902  0.982046  0.133133      0.955600  0.000320
169  0.046619  0.981502  0.074686      0.981837  0.000320
170  0.048326  0.981366  0.098306      0.970737  0.000320
171  0.047706  0.983542  0.080703      0.979818  0.000320
172  0.050130  0.980822  0.104203      0.969728  0.000320
173  0.044788  0.982726  0.078068      0.977800  0.000320
174  0.047927  0.982998  0.084379      0.973764  0.000320
175  0.047241  0.982862  0.109853      0.969728  0.000320
176  0.045082  0.983406  0.064957      0.985873  0.000320
177  0.046273  0.983270  0.099809      0.970737  0.000320
178  0.051882  0.980822  0.113523      0.969728  0.000320
179  0.051550  0.979461  0.107010      0.973764  0.000320
180  0.039673  0.983542  0.085531      0.979818  0.000320
181  0.045119  0.982862  0.071155      0.981837  0.000320
182  0.052812  0.978373  0.073199      0.978809  0.000320
183  0.044533  0.983134  0.131953      0.964682  0.000320
184  0.047190  0.983542  0.086309      0.971746  0.000320
185  0.043510  0.984766  0.076683      0.971746  0.000320
186  0.043009  0.984630  0.089677      0.971746  0.000320
187  0.047274  0.983406  0.103743      0.968718  0.000320
188  0.041247  0.984766  0.086660      0.977800  0.000320
189  0.041555  0.985718  0.091553      0.972755  0.000320
190  0.044693  0.984086  0.084795      0.974773  0.000320
191  0.040296  0.984630  0.080499      0.974773  0.000256
192  0.033620  0.987350  0.122681      0.960646  0.000256
193  0.039917  0.985174  0.093588      0.971746  0.000256
194  0.040435  0.984902  0.084241      0.972755  0.000256
195  0.034310  0.987078  0.113732      0.965691  0.000256
196  0.042833  0.984630  0.126247      0.959637  0.000256
197  0.047937  0.983134  0.104610      0.958628  0.000256
198  0.042556  0.984630  0.101552      0.963673  0.000256
199  0.039703  0.987078  0.085457      0.971746  0.000256
200  0.034755  0.987894  0.107862      0.966700  0.000256
201  0.034563  0.987214  0.079480      0.974773  0.000256
202  0.041001  0.986398  0.087558      0.973764  0.000256
203  0.038910  0.986262  0.074585      0.979818  0.000205
204  0.033712  0.988711  0.066172      0.980827  0.000205
205  0.033486  0.988030  0.080707      0.976791  0.000205
206  0.033583  0.989255  0.095552      0.973764  0.000205
207  0.033795  0.987078  0.084200      0.976791  0.000205
208  0.034021  0.988303  0.100664      0.974773  0.000205
209  0.034047  0.988847  0.072240      0.978809  0.000205
210  0.036196  0.986670  0.070874      0.982846  0.000205
211  0.034202  0.988167  0.077859      0.973764  0.000205
212  0.036001  0.988439  0.059285      0.983855  0.000205
213  0.032836  0.989255  0.083899      0.978809  0.000205
214  0.034039  0.989391  0.076334      0.979818  0.000205
215  0.028332  0.990887  0.085066      0.978809  0.000205
216  0.030201  0.989663  0.070254      0.976791  0.000205
217  0.031368  0.989255  0.072704      0.981837  0.000205
218  0.028391  0.991431  0.089417      0.977800  0.000205
219  0.028759  0.991023  0.077945      0.980827  0.000205
220  0.033401  0.988847  0.063832      0.982846  0.000205
221  0.027358  0.990751  0.076056      0.979818  0.000205
222  0.026550  0.991567  0.074913      0.979818  0.000205
223  0.026294  0.990479  0.093174      0.976791  0.000205
224  0.028099  0.989799  0.081817      0.979818  0.000205
225  0.025287  0.991703  0.086502      0.978809  0.000205
226  0.030975  0.988847  0.073404      0.980827  0.000205
227  0.030942  0.989663  0.074058      0.977800  0.000205
228  0.028216  0.990343  0.106686      0.971746  0.000205
229  0.029899  0.990615  0.075448      0.977800  0.000205
230  0.027265  0.991159  0.079790      0.980827  0.000205
231  0.030185  0.989799  0.083987      0.977800  0.000205
232  0.030587  0.989799  0.076272      0.979818  0.000205
233  0.028572  0.991159  0.085030      0.980827  0.000205
234  0.028544  0.988575  0.084015      0.979818  0.000205
235  0.026705  0.992247  0.102756      0.974773  0.000205
236  0.025836  0.991431  0.084685      0.978809  0.000164
237  0.024695  0.991159  0.089683      0.980827  0.000164
238  0.026008  0.991431  0.111467      0.971746  0.000164
239  0.023402  0.993063  0.084532      0.976791  0.000164
240  0.024281  0.991975  0.085236      0.978809  0.000164
241  0.026367  0.991975  0.092579      0.977800  0.000164
242  0.022205  0.992927  0.105695      0.973764  0.000164
243  0.030184  0.991023  0.074124      0.980827  0.000164
244  0.027466  0.990343  0.107169      0.974773  0.000164
245  0.025501  0.991159  0.076258      0.976791  0.000164
246  0.027480  0.991431  0.068635      0.983855  0.000164
247  0.024245  0.991295  0.109149      0.971746  0.000164
248  0.024589  0.992111  0.102137      0.974773  0.000164
249  0.024839  0.992111  0.077404      0.978809  0.000164
250  0.022101  0.994015  0.085256      0.979818  0.000164
251  0.024307  0.991839  0.098482      0.978809  0.000164
252  0.024262  0.992383  0.077699      0.976791  0.000164
253  0.022059  0.993063  0.082867      0.977800  0.000164
254  0.022933  0.992791  0.107034      0.973764  0.000164
255  0.024080  0.991839  0.094200      0.975782  0.000164
256  0.020812  0.993335  0.109756      0.975782  0.000164
257  0.020617  0.994015  0.079683      0.979818  0.000164
258  0.022936  0.991431  0.083179      0.978809  0.000164
259  0.020357  0.993063  0.105648      0.976791  0.000164
260  0.027776  0.990343  0.116975      0.971746  0.000164
261  0.020657  0.993063  0.148014      0.966700  0.000164
262  0.026376  0.992247  0.109624      0.974773  0.000164
263  0.023114  0.993471  0.098668      0.978809  0.000164
264  0.025365  0.991975  0.095599      0.979818  0.000164
265  0.026600  0.991703  0.080116      0.980827  0.000164
266  0.024914  0.992791  0.094366      0.978809  0.000164
267  0.022934  0.992655  0.090279      0.976791  0.000164
268  0.023729  0.991975  0.080515      0.980827  0.000164
269  0.026223  0.990615  0.079110      0.976791  0.000164
270  0.022364  0.992927  0.089201      0.976791  0.000131
271  0.026131  0.992655  0.086822      0.976791  0.000131
272  0.022870  0.993063  0.079688      0.982846  0.000131
273  0.023562  0.992519  0.073265      0.981837  0.000131
274  0.021345  0.993743  0.087189      0.981837  0.000131
275  0.023825  0.992927  0.082055      0.980827  0.000131
276  0.022541  0.992655  0.084621      0.981837  0.000131
277  0.020159  0.994287  0.081276      0.982846  0.000131
278  0.021096  0.993199  0.076405      0.981837  0.000131
279  0.019082  0.993743  0.101444      0.979818  0.000131
280  0.023908  0.992383  0.107548      0.976791  0.000131
281  0.017884  0.994559  0.085558      0.980827  0.000131
282  0.020406  0.993199  0.090942      0.979818  0.000131
283  0.020151  0.993471  0.095959      0.978809  0.000131
284  0.023040  0.994015  0.088481      0.976791  0.000131
285  0.020202  0.993743  0.075802      0.981837  0.000131
286  0.024508  0.992383  0.078054      0.979818  0.000131
287  0.020488  0.993471  0.084707      0.981837  0.000131
288  0.021318  0.992791  0.093647      0.979818  0.000131
289  0.020212  0.992655  0.096500      0.980827  0.000131
290  0.017885  0.994287  0.086862      0.980827  0.000131
291  0.018641  0.994559  0.083078      0.981837  0.000131
292  0.018370  0.994695  0.088177      0.980827  0.000105
293  0.021053  0.993471  0.106391      0.975782  0.000105
294  0.022204  0.993335  0.112973      0.974773  0.000105
295  0.019367  0.993879  0.088791      0.980827  0.000105
296  0.020252  0.993471  0.091295      0.979818  0.000105
297  0.017508  0.994423  0.077807      0.982846  0.000105
298  0.017105  0.995783  0.098444      0.978809  0.000105
299  0.018124  0.994151  0.104012      0.977800  0.000105
300  0.020299  0.992927  0.091646      0.977800  0.000105
301  0.015828  0.995239  0.086777      0.979818  0.000105
302  0.016211  0.995375  0.091584      0.977800  0.000105
303  0.023486  0.992519  0.081751      0.978809  0.000105
304  0.018930  0.994015  0.079730      0.979818  0.000105
305  0.016107  0.995511  0.095020      0.979818  0.000105
306  0.018015  0.994831  0.085273      0.981837  0.000105
307  0.018300  0.995375  0.093604      0.976791  0.000105
308  0.015974  0.994151  0.105214      0.971746  0.000105
309  0.017432  0.994967  0.099793      0.976791  0.000105
310  0.016705  0.994695  0.093745      0.978809  0.000105
311  0.016045  0.995511  0.081962      0.980827  0.000105
312  0.019864  0.993879  0.089948      0.980827  0.000100
313  0.020243  0.994423  0.113188      0.978809  0.000100
314  0.017851  0.994831  0.107219      0.978809  0.000100
315  0.017584  0.994967  0.085512      0.980827  0.000100
316  0.020303  0.993743  0.089889      0.978809  0.000100
317  0.017988  0.995103  0.090757      0.979818  0.000100
318  0.014848  0.996192  0.089140      0.978809  0.000100
319  0.019563  0.994831  0.100835      0.977800  0.000100
320  0.019265  0.994423  0.123946      0.976791  0.000100
321  0.016368  0.994287  0.101462      0.980827  0.000100
322  0.015798  0.996055  0.096064      0.979818  0.000100
323  0.016269  0.995647  0.091716      0.979818  0.000100
324  0.017985  0.995103  0.109111      0.975782  0.000100
325  0.015489  0.995783  0.086929      0.977800  0.000100
326  0.018483  0.993335  0.100753      0.976791  0.000100
327  0.016898  0.994831  0.113235      0.975782  0.000100
328  0.018221  0.994287  0.100878      0.979818  0.000100
329  0.016810  0.994967  0.097272      0.978809  0.000100
330  0.016062  0.995239  0.094264      0.981837  0.000100
331  0.015135  0.995783  0.091697      0.978809  0.000100
332  0.014152  0.996055  0.096094      0.978809  0.000100
333  0.017123  0.994151  0.126495      0.971746  0.000100
334  0.016732  0.994423  0.116765      0.974773  0.000100
335  0.015943  0.994831  0.092666      0.980827  0.000100
336  0.014740  0.995647  0.091018      0.982846  0.000100
337  0.017593  0.994967  0.117064      0.978809  0.000100
338  0.017359  0.994287  0.089870      0.978809  0.000100
339  0.017005  0.994831  0.101680      0.976791  0.000100
340  0.016928  0.994831  0.105245      0.974773  0.000100
341  0.014365  0.995647  0.100913      0.976791  0.000100
342  0.018268  0.994151  0.096712      0.977800  0.000100
343  0.018787  0.994151  0.118409      0.975782  0.000100
344  0.014641  0.996192  0.089299      0.978809  0.000100
345  0.015325  0.995919  0.105077      0.977800  0.000100
346  0.014819  0.996328  0.109828      0.974773  0.000100
347  0.013348  0.996328  0.109981      0.976791  0.000100
348  0.014903  0.995919  0.090176      0.980827  0.000100
349  0.014421  0.996055  0.096992      0.978809  0.000100

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_1 (Batch (None, 128, 9)            36        
_________________________________________________________________
bidirectional (Bidirectional (None, 256)               141312    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               25700     
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 606       
=================================================================
Total params: 167,654
Trainable params: 167,636
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 1
Merge mode: concat


Test Accuracy: 95.14315128326416
Test Loss: 0.2567603290081024



Classification Report
                    precision    recall  f1-score   support

           Walking       0.98      0.99      0.98       335
  Walking_Upstairs       0.96      0.94      0.95       316
Walking_Downstairs       0.97      0.99      0.98       284
           Sitting       0.93      0.84      0.88       324
          Standing       0.88      0.96      0.92       342
            Laying       0.99      1.00      1.00       355

          accuracy                           0.95      1956
         macro avg       0.95      0.95      0.95      1956
      weighted avg       0.95      0.95      0.95      1956



Confusion Matrix
[[330   3   2   0   0   0]
 [  7 296   8   5   0   0]
 [  0   4 280   0   0   0]
 [  0   5   0 272  44   3]
 [  0   0   0  14 328   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               98.51              0.90                0.60     0.00   
Walking_Upstairs       2.22             93.67                2.53     1.58   
Walking_Downstairs     0.00              1.41               98.59     0.00   
Sitting                0.00              1.54                0.00    83.95   
Standing               0.00              0.00                0.00     4.09   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                13.58    0.93  
Standing               95.91    0.00  
Laying                  0.00  100.00  


Finished working on: BiLSTM at: 2021-02-10 22:49:23.812102 -> 869.5202467441559

iSPLInception Model : 2021-02-10 23:24:49.786262
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.453548  0.917709  5.420660      0.471241  0.000500
1    2.902614  0.950354  3.392963      0.499495  0.000500
2    1.748925  0.947497  2.287501      0.662967  0.000500
3    1.174893  0.947361  1.620157      0.796165  0.000500
4    0.867093  0.949129  1.128134      0.854692  0.000500
5    0.662943  0.947769  1.034391      0.775984  0.000500
6    0.512304  0.956610  0.781271      0.935419  0.000500
7    0.452071  0.953210  0.594618      0.909183  0.000500
8    0.363463  0.957835  0.542383      0.900101  0.000500
9    0.295701  0.960827  0.792644      0.842583  0.000500
10   0.277727  0.955522  0.643132      0.835520  0.000500
11   0.261584  0.958107  0.341764      0.951564  0.000500
12   0.228912  0.959195  0.586116      0.864783  0.000500
13   0.206648  0.959331  0.827289      0.811302  0.000500
14   0.194043  0.961235  0.389933      0.880928  0.000500
15   0.189172  0.957835  1.280052      0.764884  0.000500
16   0.196716  0.957835  0.568104      0.829465  0.000500
17   0.189576  0.958107  0.317730      0.967709  0.000500
18   0.189639  0.958651  0.502999      0.895055  0.000500
19   0.191403  0.962051  0.322640      0.970737  0.000500
20   0.181361  0.956882  0.558473      0.846620  0.000500
21   0.182017  0.960963  0.366649      0.939455  0.000500
22   0.160420  0.961371  0.539612      0.872856  0.000500
23   0.168310  0.958515  0.395385      0.933401  0.000500
24   0.155874  0.960827  0.294748      0.934410  0.000500
25   0.175231  0.958923  9.745818      0.641776  0.000500
26   0.179085  0.962187  0.457975      0.933401  0.000500
27   0.151386  0.962323  0.272778      0.969728  0.000500
28   0.139126  0.962867  0.733137      0.724521  0.000500
29   0.160225  0.959467  3.130028      0.621594  0.000500
30   0.179234  0.958923  0.243309      0.966700  0.000500
31   0.151549  0.959875  0.266811      0.949546  0.000500
32   0.143156  0.961099  0.312233      0.946519  0.000500
33   0.126856  0.964908  0.259671      0.943491  0.000500
34   0.142248  0.962459  0.168680      0.977800  0.000500
35   0.136263  0.959195  0.244441      0.952573  0.000500
36   0.129550  0.963003  0.491314      0.829465  0.000500
37   0.141455  0.961915  0.190997      0.972755  0.000500
38   0.146277  0.965996  0.228029      0.972755  0.000500
39   0.150522  0.962867  0.189425      0.967709  0.000500
40   0.138489  0.965452  0.246270      0.965691  0.000500
41   0.132463  0.964908  0.238264      0.938446  0.000500
42   0.141385  0.963003  0.296805      0.913219  0.000500
43   0.143184  0.964363  0.209286      0.954591  0.000500
44   0.127084  0.966404  0.240816      0.928355  0.000400
45   0.115527  0.967220  0.213007      0.952573  0.000400
46   0.107445  0.970484  0.209162      0.957619  0.000400
47   0.117663  0.966676  0.145753      0.971746  0.000400
48   0.109128  0.970756  0.131586      0.972755  0.000400
49   0.117126  0.965316  0.719301      0.864783  0.000400
50   0.120369  0.968308  0.133176      0.977800  0.000400
51   0.110369  0.969804  0.173564      0.959637  0.000400
52   0.111441  0.968444  0.340443      0.899092  0.000400
53   0.114104  0.970484  0.143473      0.967709  0.000400
54   0.114674  0.964635  0.155225      0.962664  0.000400
55   0.126136  0.965044  1.807754      0.642785  0.000400
56   0.119268  0.969532  0.162165      0.953582  0.000400
57   0.107902  0.972796  0.125827      0.977800  0.000320
58   0.100017  0.974565  0.158026      0.959637  0.000320
59   0.093864  0.975245  0.171894      0.964682  0.000320
60   0.099228  0.976333  0.130162      0.970737  0.000320
61   0.096753  0.972660  0.125132      0.973764  0.000320
62   0.093292  0.974429  0.101931      0.983855  0.000320
63   0.084916  0.978645  0.112775      0.973764  0.000320
64   0.081494  0.978645  0.115647      0.972755  0.000320
65   0.080281  0.981774  0.115938      0.979818  0.000320
66   0.084413  0.979461  0.127673      0.972755  0.000320
67   0.082645  0.978237  0.133687      0.970737  0.000320
68   0.080326  0.982726  0.148322      0.963673  0.000320
69   0.084271  0.980141  0.133183      0.967709  0.000320
70   0.091091  0.978781  0.421596      0.903128  0.000320
71   0.093986  0.979189  0.143914      0.965691  0.000320
72   0.078740  0.983406  0.226207      0.944501  0.000320
73   0.086945  0.981366  0.107817      0.979818  0.000320
74   0.079955  0.982726  0.278705      0.916246  0.000320
75   0.081742  0.983270  0.095447      0.980827  0.000320
76   0.073411  0.985174  0.115781      0.967709  0.000320
77   0.071381  0.985038  0.105690      0.982846  0.000320
78   0.072644  0.983814  0.099735      0.975782  0.000320
79   0.084345  0.981774  0.157560      0.964682  0.000320
80   0.085284  0.981638  0.087120      0.987891  0.000320
81   0.080421  0.982046  0.111119      0.982846  0.000320
82   0.074804  0.983270  0.315087      0.893037  0.000320
83   0.071973  0.985718  0.094464      0.983855  0.000320
84   0.072762  0.984358  0.095110      0.986882  0.000320
85   0.072606  0.985038  0.108167      0.981837  0.000320
86   0.073971  0.984358  0.157204      0.960646  0.000320
87   0.076086  0.986126  0.087297      0.989909  0.000320
88   0.071277  0.985718  0.098606      0.978809  0.000256
89   0.069418  0.985038  0.078066      0.989909  0.000256
90   0.062633  0.988167  0.114959      0.967709  0.000256
91   0.063748  0.986534  0.088519      0.980827  0.000256
92   0.061159  0.988575  0.079186      0.984864  0.000256
93   0.062416  0.986398  0.067719      0.989909  0.000256
94   0.067577  0.984902  0.084720      0.981837  0.000256
95   0.066111  0.983950  0.092430      0.988900  0.000256
96   0.065311  0.987078  0.086057      0.982846  0.000256
97   0.067281  0.986398  0.104110      0.973764  0.000256
98   0.062822  0.986942  0.071154      0.990918  0.000256
99   0.064427  0.988439  0.096749      0.984864  0.000256
100  0.063370  0.988303  0.097201      0.978809  0.000256
101  0.063475  0.986398  0.080875      0.984864  0.000256
102  0.060509  0.988711  0.071297      0.986882  0.000256
103  0.066403  0.985582  0.113699      0.972755  0.000256
104  0.063028  0.988167  0.113009      0.970737  0.000256
105  0.057629  0.988303  0.084528      0.990918  0.000256
106  0.062582  0.986942  0.095867      0.988900  0.000256
107  0.063118  0.988167  0.102175      0.976791  0.000256
108  0.062805  0.987894  0.201618      0.944501  0.000256
109  0.062781  0.988167  0.110546      0.967709  0.000256
110  0.059187  0.988847  0.104507      0.982846  0.000256
111  0.065887  0.988167  0.093173      0.979818  0.000256
112  0.061208  0.987622  0.170353      0.956609  0.000256
113  0.062179  0.988847  0.085644      0.988900  0.000256
114  0.067656  0.986126  0.068317      0.993946  0.000256
115  0.059396  0.990207  0.080091      0.986882  0.000256
116  0.057299  0.989255  0.063428      0.991927  0.000205
117  0.058822  0.988847  0.107821      0.979818  0.000205
118  0.052891  0.991839  0.074899      0.986882  0.000205
119  0.055064  0.988575  0.087774      0.979818  0.000205
120  0.055193  0.989799  0.092115      0.973764  0.000205
121  0.054775  0.991839  0.074988      0.984864  0.000205
122  0.058427  0.989527  0.145590      0.960646  0.000205
123  0.056156  0.988847  0.081451      0.986882  0.000205
124  0.053120  0.989799  0.067862      0.992936  0.000205
125  0.049291  0.993335  0.123076      0.971746  0.000205
126  0.053895  0.990071  0.070261      0.990918  0.000205
127  0.047310  0.992247  0.062313      0.989909  0.000205
128  0.052147  0.990343  0.069727      0.991927  0.000205
129  0.064610  0.988847  0.464406      0.899092  0.000205
130  0.064689  0.988983  0.115067      0.969728  0.000205
131  0.057948  0.990479  0.076734      0.989909  0.000205
132  0.056616  0.989391  0.073492      0.986882  0.000205
133  0.051251  0.992383  0.078612      0.988900  0.000205
134  0.054577  0.990887  0.077036      0.986882  0.000205
135  0.052018  0.990887  0.125518      0.973764  0.000205
136  0.052956  0.991159  0.076806      0.985873  0.000205
137  0.048526  0.992519  0.065740      0.989909  0.000205
138  0.048794  0.991839  0.069166      0.989909  0.000164
139  0.045234  0.994015  0.071950      0.989909  0.000164
140  0.045450  0.992791  0.055614      0.992936  0.000164
141  0.047458  0.991159  0.060547      0.990918  0.000164
142  0.052676  0.990479  0.066438      0.988900  0.000164
143  0.047813  0.992655  0.074966      0.983855  0.000164
144  0.049066  0.991431  0.070668      0.988900  0.000164
145  0.041815  0.994831  0.079705      0.978809  0.000164
146  0.046248  0.993743  0.109829      0.974773  0.000164
147  0.046203  0.993879  0.144514      0.965691  0.000164
148  0.046033  0.992791  0.072196      0.986882  0.000164
149  0.045322  0.994287  0.072071      0.987891  0.000164
150  0.044028  0.993743  0.073203      0.986882  0.000164
151  0.043017  0.994015  0.074026      0.986882  0.000164
152  0.047449  0.991975  0.064441      0.988900  0.000164
153  0.055013  0.990343  0.056032      0.993946  0.000164
154  0.049093  0.991703  0.065013      0.988900  0.000164
155  0.044806  0.994151  0.069107      0.989909  0.000164
156  0.040675  0.995919  0.072601      0.986882  0.000131
157  0.039069  0.995783  0.069543      0.987891  0.000131
158  0.041240  0.994151  0.068566      0.987891  0.000131
159  0.044129  0.993199  0.065810      0.987891  0.000131
160  0.044869  0.993199  0.061201      0.989909  0.000131
161  0.042878  0.993199  0.065966      0.990918  0.000131
162  0.041830  0.993471  0.063933      0.988900  0.000131
163  0.041983  0.994287  0.066774      0.986882  0.000131
164  0.038883  0.995511  0.091268      0.981837  0.000131
165  0.041926  0.994151  0.057962      0.989909  0.000131
166  0.039927  0.994695  0.054005      0.993946  0.000131
167  0.043158  0.993199  0.068766      0.987891  0.000131
168  0.044599  0.993063  0.093302      0.973764  0.000131
169  0.039718  0.994151  0.059643      0.990918  0.000131
170  0.042606  0.992927  0.082542      0.979818  0.000131
171  0.039303  0.994695  0.055499      0.991927  0.000131
172  0.040600  0.993471  0.060093      0.991927  0.000131
173  0.039527  0.993607  0.064299      0.988900  0.000131
174  0.043803  0.991839  0.070835      0.979818  0.000131
175  0.036788  0.996055  0.058194      0.988900  0.000105
176  0.036746  0.996464  0.057860      0.991927  0.000105
177  0.038445  0.995647  0.068054      0.984864  0.000105
178  0.041275  0.995647  0.065519      0.988900  0.000105
179  0.035530  0.995919  0.063931      0.988900  0.000105
180  0.034246  0.996464  0.062553      0.986882  0.000105
181  0.034351  0.996464  0.082950      0.976791  0.000105
182  0.036838  0.994967  0.061125      0.989909  0.000105
183  0.035428  0.995375  0.059347      0.990918  0.000105
184  0.035481  0.996055  0.068088      0.984864  0.000105
185  0.035286  0.995919  0.056767      0.990918  0.000105
186  0.035337  0.996328  0.059670      0.987891  0.000105
187  0.038450  0.993879  0.053311      0.991927  0.000105
188  0.035528  0.996055  0.057448      0.989909  0.000105
189  0.037454  0.994831  0.071246      0.983855  0.000105
190  0.032232  0.997552  0.065058      0.987891  0.000105
191  0.033737  0.996600  0.056050      0.992936  0.000105
192  0.034543  0.995511  0.060528      0.988900  0.000105
193  0.036259  0.995239  0.058424      0.987891  0.000105
194  0.036799  0.994559  0.069658      0.986882  0.000105
195  0.033178  0.996464  0.065864      0.989909  0.000105
196  0.036870  0.995239  0.322424      0.893037  0.000105
197  0.035987  0.995647  0.060042      0.990918  0.000105
198  0.035740  0.996328  0.062772      0.984864  0.000105
199  0.035511  0.995375  0.055122      0.991927  0.000105
200  0.034040  0.996192  0.059070      0.990918  0.000105
201  0.034588  0.997280  0.060509      0.986882  0.000100
202  0.037632  0.996192  0.059126      0.986882  0.000100
203  0.034906  0.996464  0.068563      0.987891  0.000100
204  0.032946  0.997552  0.074604      0.979818  0.000100
205  0.037646  0.994831  0.064165      0.988900  0.000100
206  0.038763  0.994559  0.052388      0.991927  0.000100
207  0.033426  0.996328  0.090811      0.979818  0.000100
208  0.033182  0.996464  0.074985      0.983855  0.000100
209  0.035419  0.995783  0.063738      0.982846  0.000100
210  0.039630  0.993879  0.072179      0.981837  0.000100
211  0.036301  0.995783  0.060520      0.988900  0.000100
212  0.035707  0.995511  0.060940      0.987891  0.000100
213  0.036499  0.995375  0.058042      0.991927  0.000100
214  0.035633  0.996192  0.061737      0.988900  0.000100
215  0.036779  0.995375  0.054722      0.988900  0.000100
216  0.035237  0.994695  0.068223      0.984864  0.000100
217  0.034640  0.995919  0.051634      0.991927  0.000100
218  0.032596  0.996736  0.064243      0.986882  0.000100
219  0.033227  0.996600  0.065019      0.985873  0.000100
220  0.035151  0.995783  0.073811      0.983855  0.000100
221  0.033703  0.997008  0.057751      0.990918  0.000100
222  0.031707  0.996872  0.072421      0.982846  0.000100
223  0.035501  0.995919  0.052111      0.990918  0.000100
224  0.031512  0.997144  0.056854      0.989909  0.000100
225  0.038141  0.994559  0.059527      0.985873  0.000100
226  0.031302  0.997552  0.064629      0.985873  0.000100
227  0.032118  0.996464  0.055533      0.989909  0.000100
228  0.035863  0.994695  0.058624      0.987891  0.000100
229  0.033314  0.996328  0.057119      0.989909  0.000100
230  0.033056  0.996464  0.059581      0.988900  0.000100
231  0.035546  0.995239  0.102196      0.973764  0.000100
232  0.032549  0.996600  0.073252      0.981837  0.000100
233  0.034933  0.995103  0.053463      0.992936  0.000100
234  0.031920  0.996736  0.051782      0.993946  0.000100
235  0.031956  0.996872  0.060244      0.984864  0.000100
236  0.030748  0.997008  0.051422      0.991927  0.000100
237  0.029973  0.997552  0.062851      0.987891  0.000100
238  0.030571  0.997416  0.061793      0.988900  0.000100
239  0.033614  0.995511  0.059069      0.988900  0.000100
240  0.031154  0.997144  0.053242      0.987891  0.000100
241  0.029798  0.997824  0.063654      0.988900  0.000100
242  0.033610  0.996872  0.053952      0.991927  0.000100
243  0.034075  0.994967  0.075655      0.984864  0.000100
244  0.033122  0.996328  0.064465      0.986882  0.000100
245  0.031206  0.997280  0.063534      0.989909  0.000100
246  0.033684  0.995783  0.062993      0.987891  0.000100
247  0.034735  0.995239  0.059628      0.989909  0.000100
248  0.030161  0.997824  0.053610      0.992936  0.000100
249  0.031277  0.996872  0.055832      0.985873  0.000100
250  0.031696  0.996736  0.061564      0.986882  0.000100
251  0.032874  0.996600  0.061811      0.984864  0.000100
252  0.036312  0.993471  0.077353      0.984864  0.000100
253  0.032443  0.996328  0.050889      0.988900  0.000100
254  0.032942  0.995783  0.058450      0.986882  0.000100
255  0.032100  0.997008  0.085694      0.978809  0.000100
256  0.035139  0.996192  0.073813      0.987891  0.000100
257  0.030119  0.997688  0.065957      0.983855  0.000100
258  0.031444  0.996600  0.068464      0.982846  0.000100
259  0.034280  0.995511  0.056733      0.988900  0.000100
260  0.028574  0.997824  0.051874      0.990918  0.000100
261  0.030710  0.996736  0.054513      0.991927  0.000100
262  0.032706  0.996055  0.061650      0.989909  0.000100
263  0.033229  0.996055  0.052426      0.987891  0.000100
264  0.029381  0.997008  0.076162      0.981837  0.000100
265  0.028096  0.997960  0.049877      0.989909  0.000100
266  0.029686  0.997144  0.058755      0.986882  0.000100
267  0.030962  0.996872  0.064692      0.985873  0.000100
268  0.029023  0.997688  0.064789      0.985873  0.000100
269  0.028033  0.998096  0.066787      0.985873  0.000100
270  0.033704  0.996328  0.060883      0.984864  0.000100
271  0.030749  0.996736  0.059255      0.986882  0.000100
272  0.031405  0.996736  0.052859      0.989909  0.000100
273  0.029929  0.997824  0.055778      0.988900  0.000100
274  0.030537  0.997416  0.061337      0.988900  0.000100
275  0.030277  0.996600  0.054072      0.988900  0.000100
276  0.028211  0.997552  0.050834      0.987891  0.000100
277  0.028724  0.997416  0.060947      0.988900  0.000100
278  0.028031  0.997688  0.074775      0.980827  0.000100
279  0.030252  0.996872  0.059179      0.987891  0.000100
280  0.026525  0.998776  0.054497      0.990918  0.000100
281  0.028499  0.997552  0.202059      0.959637  0.000100
282  0.029235  0.997280  0.054776      0.990918  0.000100
283  0.027395  0.997824  0.047868      0.992936  0.000100
284  0.028844  0.997144  0.052981      0.989909  0.000100
285  0.025719  0.998504  0.053107      0.990918  0.000100
286  0.030086  0.996464  0.065503      0.986882  0.000100
287  0.027575  0.997824  0.069696      0.984864  0.000100
288  0.027294  0.997416  0.056080      0.990918  0.000100
289  0.028521  0.997144  0.050757      0.987891  0.000100
290  0.026334  0.998504  0.094129      0.979818  0.000100
291  0.034075  0.994559  0.066096      0.985873  0.000100
292  0.029867  0.996872  0.052091      0.988900  0.000100
293  0.029361  0.997280  0.051774      0.990918  0.000100
294  0.029170  0.997008  0.072158      0.983855  0.000100
295  0.029672  0.997280  0.056853      0.988900  0.000100
296  0.026653  0.998776  0.082709      0.978809  0.000100
297  0.027969  0.997552  0.056217      0.989909  0.000100
298  0.029097  0.997008  0.056677      0.990918  0.000100
299  0.026590  0.998096  0.050576      0.987891  0.000100
300  0.028355  0.997280  0.045240      0.988900  0.000100
301  0.031978  0.996464  0.059723      0.988900  0.000100
302  0.030098  0.996600  0.123167      0.969728  0.000100
303  0.032497  0.995783  0.060929      0.984864  0.000100
304  0.027568  0.998232  0.087957      0.975782  0.000100
305  0.028916  0.997824  0.054560      0.987891  0.000100
306  0.027944  0.997824  0.059768      0.986882  0.000100
307  0.027861  0.997824  0.054063      0.986882  0.000100
308  0.025586  0.998912  0.049415      0.990918  0.000100
309  0.027638  0.997552  0.053328      0.988900  0.000100
310  0.029040  0.997144  0.072504      0.987891  0.000100
311  0.028266  0.997008  0.054748      0.989909  0.000100
312  0.028727  0.997416  0.062877      0.983855  0.000100
313  0.030599  0.996328  0.063581      0.983855  0.000100
314  0.027091  0.997280  0.052565      0.990918  0.000100
315  0.030205  0.997008  0.056497      0.988900  0.000100
316  0.028312  0.997144  0.055106      0.986882  0.000100
317  0.030527  0.996736  0.059633      0.986882  0.000100
318  0.032280  0.995919  0.055502      0.985873  0.000100
319  0.029204  0.997144  0.053324      0.988900  0.000100
320  0.026959  0.997824  0.050725      0.990918  0.000100
321  0.031197  0.996464  0.056662      0.984864  0.000100
322  0.026427  0.998504  0.049193      0.991927  0.000100
323  0.026026  0.998368  0.050805      0.988900  0.000100
324  0.026270  0.998096  0.065114      0.984864  0.000100
325  0.026423  0.998368  0.075190      0.980827  0.000100
326  0.026148  0.998096  0.048216      0.990918  0.000100
327  0.028216  0.997416  0.055159      0.990918  0.000100
328  0.027905  0.997552  0.053072      0.989909  0.000100
329  0.028553  0.996872  0.088055      0.976791  0.000100
330  0.029049  0.996736  0.049149      0.991927  0.000100
331  0.026655  0.998096  0.055756      0.987891  0.000100
332  0.027686  0.997552  0.053992      0.984864  0.000100
333  0.026684  0.998232  0.044859      0.991927  0.000100
334  0.023877  0.998912  0.068001      0.986882  0.000100
335  0.030005  0.996192  0.066306      0.989909  0.000100
336  0.030879  0.996600  0.072687      0.983855  0.000100
337  0.031612  0.996600  0.054482      0.988900  0.000100
338  0.028780  0.996192  0.041467      0.992936  0.000100
339  0.026500  0.997824  0.057986      0.988900  0.000100
340  0.027038  0.998232  0.063431      0.983855  0.000100
341  0.028462  0.997144  0.045257      0.992936  0.000100
342  0.030146  0.996600  0.067547      0.984864  0.000100
343  0.026678  0.998368  0.044351      0.992936  0.000100
344  0.032025  0.995511  0.052990      0.989909  0.000100
345  0.027501  0.998232  0.057713      0.988900  0.000100
346  0.025659  0.998368  0.043773      0.991927  0.000100
347  0.024892  0.998504  0.050075      0.989909  0.000100
348  0.024771  0.998504  0.051469      0.986882  0.000100
349  0.029848  0.996192  0.045076      0.991927  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 128, 9)]     0                                            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 9)       36          input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 128, 32)      288         batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 128, 9)       0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 128, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 128, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 128, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 128, 64)      576         max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 128, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 128, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 128, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 128, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 128, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 128, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 128, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 128, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 128, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 128, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 128, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 128, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 128, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 128, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 128, 256)     2304        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 128, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 128, 256)     0           batch_normalization_6[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 128, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 128, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 128, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 128, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 128, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 128, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 128, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 128, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 128, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 128, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 128, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 128, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 128, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 128, 256)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 6)            1542        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,327,754
Trainable params: 1,324,664
Non-trainable params: 3,090
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 94.73415017127991
Test Loss: 0.16391056776046753



Classification Report
                    precision    recall  f1-score   support

           Walking       1.00      0.94      0.97       335
  Walking_Upstairs       0.99      0.93      0.96       316
Walking_Downstairs       0.87      1.00      0.93       284
           Sitting       0.97      0.84      0.90       324
          Standing       0.88      0.98      0.93       342
            Laying       0.99      1.00      0.99       355

          accuracy                           0.95      1956
         macro avg       0.95      0.95      0.95      1956
      weighted avg       0.95      0.95      0.95      1956



Confusion Matrix
[[314   0  21   0   0   0]
 [  0 293  23   0   0   0]
 [  0   0 284   0   0   0]
 [  0   2   0 272  45   5]
 [  0   0   0   7 335   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               93.73              0.00                6.27     0.00   
Walking_Upstairs       0.00             92.72                7.28     0.00   
Walking_Downstairs     0.00              0.00              100.00     0.00   
Sitting                0.00              0.62                0.00    83.95   
Standing               0.00              0.00                0.00     2.05   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                13.89    1.54  
Standing               97.95    0.00  
Laying                  0.00  100.00  


Finished working on: iSPLInception at: 2021-02-10 23:24:52.182835 -> 2128.370733499527

