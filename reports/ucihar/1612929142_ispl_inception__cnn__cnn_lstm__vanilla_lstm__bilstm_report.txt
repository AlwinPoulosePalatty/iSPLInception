This is the report for the ucihar Dataset

Data Distribution: 

Train:  X -> (7352, 128, 9) Class count -> [1226, 1073, 986, 1286, 1374, 1407] 

                    frequency
Walking             16.675735
Walking_Upstairs    14.594668
Walking_Downstairs  13.411316
Sitting             17.491838
Standing            18.688791
Laying              19.137650

Validation:  X -> (991, 128, 9) Class count -> [161, 155, 136, 167, 190, 182] 

                    frequency
Walking             16.246216
Walking_Upstairs    15.640767
Walking_Downstairs  13.723512
Sitting             16.851665
Standing            19.172554
Laying              18.365288

Test:  X -> (1956, 128, 9) Class count -> [335, 316, 284, 324, 342, 355] 

                    frequency
Walking             17.126789
Walking_Upstairs    16.155418
Walking_Downstairs  14.519427
Sitting             16.564417
Standing            17.484661
Laying              18.149284

sLSTM Model : 2021-02-10 13:36:07.543557
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.499309  0.919206  5.443264      0.448032  0.000500
1    2.963711  0.949265  3.380973      0.516650  0.000500
2    1.762820  0.943825  2.173158      0.725530  0.000500
3    1.199743  0.950490  1.567423      0.841574  0.000500
4    0.864388  0.951986  1.399934      0.765893  0.000500
5    0.687420  0.952258  0.785998      0.937437  0.000500
6    0.527102  0.954298  0.978835      0.854692  0.000500
7    0.429867  0.955250  0.734813      0.922301  0.000500
8    0.392266  0.951578  0.444281      0.951564  0.000500
9    0.305557  0.951034  0.695603      0.807265  0.000500
10   0.297780  0.955250  0.385489      0.984864  0.000500
11   0.256166  0.957427  0.402039      0.934410  0.000500
12   0.239898  0.955794  0.416050      0.924319  0.000500
13   0.221022  0.954026  0.345034      0.921292  0.000500
14   0.224205  0.955114  0.344741      0.966700  0.000500
15   0.196530  0.955658  0.259263      0.964682  0.000500
16   0.185335  0.954298  0.268266      0.955600  0.000500
17   0.175248  0.959875  0.229658      0.958628  0.000500
18   0.168017  0.957699  0.176289      0.975782  0.000500
19   0.172165  0.955930  0.845231      0.772957  0.000500
20   0.178216  0.956066  0.227399      0.964682  0.000500
21   0.163540  0.957291  0.189001      0.973764  0.000500
22   0.195740  0.954162  0.204008      0.970737  0.000500
23   0.168254  0.956610  0.375308      0.913219  0.000500
24   0.153209  0.960011  0.745802      0.797175  0.000500
25   0.156335  0.958651  0.159608      0.979818  0.000500
26   0.150815  0.960147  0.406100      0.899092  0.000500
27   0.165188  0.955930  0.260777      0.956609  0.000500
28   0.150974  0.959739  0.212783      0.971746  0.000500
29   0.137082  0.960147  0.264109      0.929364  0.000500
30   0.157437  0.956610  0.479414      0.865792  0.000500
31   0.151605  0.960419  0.154093      0.983855  0.000500
32   0.136451  0.962731  0.182488      0.977800  0.000500
33   0.133643  0.959467  0.472796      0.892028  0.000500
34   0.155639  0.960283  0.163603      0.973764  0.000500
35   0.136345  0.964772  0.223633      0.965691  0.000500
36   0.126106  0.962867  0.190221      0.954591  0.000500
37   0.138241  0.960963  0.736434      0.798184  0.000500
38   0.148965  0.965044  0.178527      0.962664  0.000500
39   0.136972  0.962867  0.226666      0.960646  0.000500
40   0.124484  0.963275  8.759172      0.318870  0.000500
41   0.127367  0.962867  0.178563      0.965691  0.000500
42   0.131522  0.964499  0.136452      0.976791  0.000500
43   0.130789  0.965996  0.178933      0.964682  0.000500
44   0.121984  0.966676  0.198809      0.957619  0.000500
45   0.142033  0.963819  0.149643      0.971746  0.000500
46   0.131509  0.969124  0.225902      0.958628  0.000500
47   0.117843  0.967628  0.171626      0.961655  0.000500
48   0.118937  0.966268  0.226534      0.946519  0.000500
49   0.118322  0.966676  0.196658      0.950555  0.000500
50   0.133055  0.965452  0.140641      0.969728  0.000500
51   0.113598  0.971844  0.237299      0.930373  0.000500
52   0.123287  0.968036  0.175730      0.942482  0.000500
53   0.110712  0.972660  0.124745      0.975782  0.000500
54   0.104595  0.973749  0.130102      0.986882  0.000500
55   0.109086  0.969804  0.156989      0.954591  0.000500
56   0.108170  0.970756  0.253900      0.928355  0.000500
57   0.111990  0.974021  0.321310      0.912210  0.000500
58   0.119390  0.972116  0.139232      0.974773  0.000500
59   0.109107  0.974565  0.112627      0.983855  0.000500
60   0.106848  0.973069  0.205899      0.954591  0.000500
61   0.103453  0.975381  0.129570      0.984864  0.000500
62   0.106929  0.974565  0.682845      0.850656  0.000500
63   0.102021  0.977285  0.130227      0.966700  0.000500
64   0.092548  0.979733  0.098495      0.980827  0.000500
65   0.099964  0.975789  0.193214      0.946519  0.000500
66   0.107999  0.975245  0.263589      0.942482  0.000500
67   0.097479  0.976741  0.184157      0.948537  0.000500
68   0.084269  0.981366  0.405074      0.882946  0.000500
69   0.110744  0.976469  0.123786      0.967709  0.000500
70   0.094260  0.979189  0.120436      0.979818  0.000500
71   0.093653  0.977557  0.179163      0.961655  0.000500
72   0.092587  0.978645  0.126976      0.971746  0.000500
73   0.090140  0.980958  0.096762      0.972755  0.000500
74   0.111453  0.977557  0.168911      0.959637  0.000500
75   0.101742  0.979461  0.158162      0.962664  0.000500
76   0.093435  0.979325  0.177695      0.967709  0.000500
77   0.087011  0.981638  0.158337      0.959637  0.000500
78   0.097926  0.978781  2.604427      0.691221  0.000500
79   0.105803  0.982318  0.115113      0.987891  0.000400
80   0.092812  0.981230  0.153978      0.964682  0.000400
81   0.086834  0.983542  0.144208      0.970737  0.000400
82   0.087395  0.981774  0.807060      0.757820  0.000400
83   0.080812  0.984766  0.109248      0.983855  0.000400
84   0.079739  0.982862  0.134071      0.968718  0.000400
85   0.079606  0.983270  0.139249      0.966700  0.000400
86   0.076411  0.985582  0.095402      0.983855  0.000400
87   0.072249  0.985718  0.131924      0.983855  0.000400
88   0.083006  0.982726  0.123611      0.964682  0.000400
89   0.075796  0.982862  0.119120      0.971746  0.000400
90   0.078137  0.982046  0.472922      0.898083  0.000400
91   0.084263  0.985718  0.162081      0.951564  0.000400
92   0.080876  0.983542  0.337927      0.879919  0.000400
93   0.076646  0.983406  0.761210      0.839556  0.000400
94   0.075206  0.984086  0.163297      0.956609  0.000400
95   0.072171  0.985718  0.099989      0.975782  0.000400
96   0.077363  0.982862  0.106634      0.985873  0.000400
97   0.074089  0.983542  0.158882      0.952573  0.000400
98   0.068803  0.987486  0.195359      0.952573  0.000320
99   0.079518  0.982046  0.123033      0.961655  0.000320
100  0.072266  0.985174  0.097190      0.986882  0.000320
101  0.070835  0.985854  0.126063      0.955600  0.000320
102  0.068281  0.986398  0.089860      0.981837  0.000320
103  0.062197  0.989391  0.100603      0.973764  0.000320
104  0.073094  0.984902  0.081943      0.982846  0.000320
105  0.065603  0.987350  0.081396      0.983855  0.000320
106  0.063833  0.988167  0.110837      0.970737  0.000320
107  0.061493  0.987758  0.108091      0.971746  0.000320
108  0.064309  0.988303  0.111809      0.969728  0.000320
109  0.063312  0.987350  0.139172      0.960646  0.000320
110  0.061786  0.988711  0.140281      0.960646  0.000320
111  0.073333  0.986398  0.086138      0.984864  0.000320
112  0.076256  0.984358  0.084031      0.987891  0.000320
113  0.071508  0.984630  0.087095      0.980827  0.000320
114  0.061634  0.988303  0.215831      0.949546  0.000320
115  0.061782  0.988575  0.141915      0.962664  0.000320
116  0.058438  0.989119  0.068550      0.986882  0.000320
117  0.061369  0.986398  0.258685      0.907164  0.000320
118  0.058068  0.986534  0.077699      0.981837  0.000320
119  0.057514  0.988575  0.075129      0.985873  0.000320
120  0.056764  0.989391  0.092078      0.983855  0.000320
121  0.060281  0.988167  0.065998      0.986882  0.000320
122  0.060072  0.988303  0.188101      0.946519  0.000320
123  0.074122  0.987350  0.126456      0.966700  0.000320
124  0.066834  0.987078  0.065826      0.994955  0.000320
125  0.071661  0.985038  0.077549      0.982846  0.000320
126  0.064632  0.986670  0.108598      0.968718  0.000320
127  0.059493  0.989255  0.080559      0.982846  0.000320
128  0.061652  0.987622  0.078302      0.984864  0.000320
129  0.057352  0.988575  0.062061      0.991927  0.000320
130  0.058650  0.989119  0.081142      0.977800  0.000320
131  0.059999  0.988439  0.361218      0.875883  0.000256
132  0.061246  0.990343  0.088712      0.980827  0.000256
133  0.057647  0.990887  0.082792      0.984864  0.000256
134  0.055510  0.991159  0.073790      0.985873  0.000256
135  0.059239  0.987486  0.082947      0.984864  0.000256
136  0.051361  0.991023  0.068851      0.986882  0.000256
137  0.052382  0.990751  0.124737      0.965691  0.000256
138  0.059842  0.986534  0.092932      0.971746  0.000256
139  0.058651  0.987758  0.071300      0.985873  0.000256
140  0.051662  0.992655  0.286214      0.925328  0.000256
141  0.049714  0.992791  0.081181      0.976791  0.000256
142  0.053211  0.989119  0.058360      0.987891  0.000256
143  0.054502  0.989391  0.104916      0.971746  0.000256
144  0.057221  0.989527  0.072196      0.984864  0.000256
145  0.049570  0.990615  0.058022      0.992936  0.000256
146  0.051235  0.989255  0.095942      0.975782  0.000256
147  0.056981  0.988167  0.073257      0.981837  0.000256
148  0.054042  0.989799  0.087185      0.978809  0.000256
149  0.059038  0.987622  0.060078      0.989909  0.000256
150  0.053463  0.989391  0.065712      0.988900  0.000256
151  0.055142  0.988711  0.122031      0.963673  0.000256
152  0.053032  0.990615  0.112949      0.970737  0.000256
153  0.055595  0.988847  0.193566      0.927346  0.000256
154  0.055840  0.989935  0.062160      0.984864  0.000256
155  0.054426  0.989799  0.062059      0.988900  0.000256
156  0.049854  0.990343  0.071347      0.985873  0.000205
157  0.047893  0.991839  0.121444      0.966700  0.000205
158  0.048324  0.990479  0.066911      0.985873  0.000205
159  0.045080  0.991703  0.063813      0.986882  0.000205
160  0.042056  0.993199  0.061713      0.983855  0.000205
161  0.054272  0.990479  0.073201      0.986882  0.000205
162  0.048513  0.991567  0.080976      0.983855  0.000205
163  0.045771  0.992791  0.077788      0.979818  0.000205
164  0.046577  0.991839  0.097398      0.973764  0.000205
165  0.046107  0.992791  0.074576      0.982846  0.000205
166  0.046138  0.992927  0.077328      0.982846  0.000205
167  0.049895  0.990207  0.158321      0.952573  0.000205
168  0.049720  0.991295  0.080809      0.981837  0.000205
169  0.048934  0.990751  0.080310      0.984864  0.000205
170  0.057356  0.988303  0.055862      0.992936  0.000205
171  0.052477  0.989935  0.070751      0.989909  0.000164
172  0.048211  0.992247  0.077838      0.982846  0.000164
173  0.048917  0.991295  0.052024      0.991927  0.000164
174  0.045153  0.992519  0.054510      0.991927  0.000164
175  0.045573  0.991431  0.064048      0.986882  0.000164
176  0.043287  0.993335  0.075755      0.983855  0.000164
177  0.045909  0.991703  0.119263      0.965691  0.000164
178  0.050513  0.989799  0.081537      0.977800  0.000164
179  0.046516  0.991567  0.082295      0.976791  0.000164
180  0.041362  0.993607  0.069137      0.985873  0.000164
181  0.046867  0.991567  0.092220      0.974773  0.000164
182  0.046093  0.992519  0.055886      0.990918  0.000164
183  0.043556  0.992791  0.064902      0.985873  0.000164
184  0.042288  0.992247  0.059188      0.986882  0.000164
185  0.040460  0.992519  0.055676      0.990918  0.000164
186  0.040058  0.993607  0.090961      0.974773  0.000164
187  0.041738  0.993471  0.050978      0.991927  0.000164
188  0.045038  0.991567  0.065678      0.990918  0.000164
189  0.047139  0.990479  0.061959      0.989909  0.000164
190  0.040736  0.993879  0.067421      0.986882  0.000164
191  0.045131  0.991431  0.055733      0.991927  0.000164
192  0.045800  0.991295  0.062743      0.986882  0.000164
193  0.040813  0.993199  0.052665      0.992936  0.000164
194  0.046032  0.992655  0.065245      0.986882  0.000164
195  0.043925  0.992519  0.100340      0.971746  0.000164
196  0.042460  0.992111  0.079405      0.982846  0.000164
197  0.039542  0.993879  0.056183      0.990918  0.000131
198  0.039551  0.993743  0.070428      0.986882  0.000131
199  0.038114  0.993879  0.058773      0.988900  0.000131
200  0.038225  0.993471  0.069434      0.985873  0.000131
201  0.036847  0.994695  0.081718      0.979818  0.000131
202  0.041599  0.994151  0.063606      0.984864  0.000131
203  0.037727  0.995103  0.049038      0.991927  0.000131
204  0.041230  0.992519  0.052972      0.992936  0.000131
205  0.038737  0.994015  0.058424      0.991927  0.000131
206  0.038184  0.993063  0.055771      0.988900  0.000131
207  0.042309  0.991431  0.066730      0.985873  0.000131
208  0.042225  0.993199  0.055509      0.993946  0.000131
209  0.041530  0.992247  0.108050      0.973764  0.000131
210  0.038367  0.993607  0.067150      0.987891  0.000131
211  0.041128  0.992927  0.063105      0.988900  0.000131
212  0.037614  0.993879  0.060024      0.987891  0.000105
213  0.035609  0.995511  0.053867      0.989909  0.000105
214  0.033325  0.995647  0.058436      0.988900  0.000105
215  0.041331  0.992927  0.066482      0.983855  0.000105
216  0.037798  0.995239  0.054120      0.990918  0.000105
217  0.036358  0.994015  0.081272      0.978809  0.000105
218  0.035961  0.994695  0.052847      0.989909  0.000105
219  0.034125  0.995647  0.064921      0.985873  0.000105
220  0.034212  0.995511  0.063531      0.987891  0.000105
221  0.037351  0.994151  0.060224      0.987891  0.000105
222  0.033615  0.995647  0.064054      0.985873  0.000105
223  0.036268  0.994423  0.071513      0.983855  0.000105
224  0.040936  0.991703  0.072083      0.986882  0.000105
225  0.037761  0.993743  0.058742      0.989909  0.000100
226  0.034334  0.995919  0.061799      0.988900  0.000100
227  0.037199  0.994151  0.059522      0.987891  0.000100
228  0.039176  0.994015  0.050935      0.991927  0.000100
229  0.034352  0.995239  0.054735      0.992936  0.000100
230  0.034248  0.995239  0.063788      0.987891  0.000100
231  0.035702  0.995919  0.060159      0.985873  0.000100
232  0.035066  0.995239  0.062230      0.988900  0.000100
233  0.036190  0.994695  0.054288      0.988900  0.000100
234  0.035901  0.994151  0.075009      0.983855  0.000100
235  0.035301  0.994967  0.062080      0.983855  0.000100
236  0.034936  0.994151  0.063911      0.983855  0.000100
237  0.035426  0.994559  0.068879      0.984864  0.000100
238  0.035753  0.994151  0.090939      0.975782  0.000100
239  0.035706  0.994695  0.065043      0.985873  0.000100
240  0.035305  0.994423  0.054103      0.988900  0.000100
241  0.035208  0.995375  0.066357      0.986882  0.000100
242  0.034832  0.995783  0.068711      0.986882  0.000100
243  0.033537  0.995783  0.067738      0.987891  0.000100
244  0.034390  0.995103  0.056222      0.985873  0.000100
245  0.033424  0.994559  0.060194      0.987891  0.000100
246  0.033259  0.995103  0.392314      0.920283  0.000100
247  0.036109  0.996055  0.084925      0.978809  0.000100
248  0.035039  0.995647  0.057509      0.991927  0.000100
249  0.036969  0.994015  0.058408      0.992936  0.000100
250  0.034461  0.994695  0.075352      0.984864  0.000100
251  0.034170  0.995375  0.073968      0.982846  0.000100
252  0.033388  0.995103  0.063066      0.989909  0.000100
253  0.032769  0.996055  0.054877      0.992936  0.000100
254  0.032297  0.995919  0.061377      0.989909  0.000100
255  0.035199  0.994287  0.061677      0.988900  0.000100
256  0.033367  0.994831  0.060962      0.990918  0.000100
257  0.035109  0.995103  0.059070      0.991927  0.000100
258  0.035526  0.995375  0.099425      0.978809  0.000100
259  0.036945  0.994423  0.068606      0.983855  0.000100
260  0.033371  0.995239  0.074093      0.984864  0.000100
261  0.032779  0.995511  0.056995      0.990918  0.000100
262  0.032654  0.995511  0.072163      0.981837  0.000100
263  0.034896  0.994423  0.055417      0.991927  0.000100
264  0.030427  0.996600  0.052947      0.989909  0.000100
265  0.031123  0.996192  0.062935      0.989909  0.000100
266  0.031896  0.995239  0.058946      0.987891  0.000100
267  0.032930  0.995511  0.064980      0.985873  0.000100
268  0.034507  0.994151  0.058660      0.988900  0.000100
269  0.032986  0.994831  0.053711      0.988900  0.000100
270  0.031520  0.995783  0.056707      0.987891  0.000100
271  0.034958  0.993879  0.056378      0.988900  0.000100
272  0.033376  0.994967  0.062532      0.987891  0.000100
273  0.035996  0.994967  0.056260      0.991927  0.000100
274  0.031744  0.996328  0.063135      0.986882  0.000100
275  0.032285  0.995103  0.051200      0.989909  0.000100
276  0.032050  0.995647  0.063490      0.988900  0.000100
277  0.029722  0.997008  0.049286      0.989909  0.000100
278  0.032723  0.995919  0.051893      0.989909  0.000100
279  0.030854  0.995783  0.067880      0.984864  0.000100
280  0.031772  0.995511  0.073035      0.986882  0.000100
281  0.029139  0.996872  0.052317      0.991927  0.000100
282  0.033605  0.994423  0.052833      0.989909  0.000100
283  0.029892  0.996736  0.053406      0.989909  0.000100
284  0.030258  0.995783  0.141001      0.966700  0.000100
285  0.032926  0.994423  0.056953      0.991927  0.000100
286  0.029104  0.996600  0.070214      0.981837  0.000100
287  0.031771  0.995375  0.055268      0.985873  0.000100
288  0.029851  0.996736  0.059153      0.987891  0.000100
289  0.028245  0.997688  0.057491      0.989909  0.000100
290  0.030290  0.996328  0.057268      0.988900  0.000100
291  0.035559  0.994015  0.070720      0.983855  0.000100
292  0.031733  0.995375  0.076010      0.984864  0.000100
293  0.032403  0.995919  0.063687      0.986882  0.000100
294  0.029388  0.996736  0.056706      0.988900  0.000100
295  0.027936  0.997280  0.059967      0.987891  0.000100
296  0.030251  0.996192  0.064501      0.986882  0.000100
297  0.029794  0.996600  0.056892      0.988900  0.000100
298  0.028639  0.996464  0.053264      0.990918  0.000100
299  0.031417  0.996055  0.060768      0.987891  0.000100
300  0.027825  0.997144  0.075251      0.982846  0.000100
301  0.030458  0.995919  0.063804      0.985873  0.000100
302  0.036710  0.993607  0.059071      0.988900  0.000100
303  0.029241  0.996192  0.057366      0.983855  0.000100
304  0.028553  0.996055  0.051455      0.990918  0.000100
305  0.032327  0.995375  0.077720      0.980827  0.000100
306  0.031830  0.995103  0.052427      0.990918  0.000100
307  0.030965  0.995919  0.077544      0.981837  0.000100
308  0.030180  0.996055  0.053304      0.990918  0.000100
309  0.033120  0.995103  0.053894      0.988900  0.000100
310  0.027845  0.996872  0.050694      0.989909  0.000100
311  0.029031  0.996464  0.040283      0.994955  0.000100
312  0.028228  0.997144  0.054030      0.988900  0.000100
313  0.028511  0.996464  0.062687      0.989909  0.000100
314  0.027142  0.997552  0.061793      0.987891  0.000100
315  0.027740  0.996872  0.052004      0.988900  0.000100
316  0.030095  0.996464  0.063717      0.987891  0.000100
317  0.027150  0.997280  0.052211      0.986882  0.000100
318  0.026572  0.997960  0.055268      0.986882  0.000100
319  0.026957  0.997688  0.045670      0.990918  0.000100
320  0.025275  0.997280  0.061605      0.987891  0.000100
321  0.029085  0.996600  0.051114      0.987891  0.000100
322  0.031939  0.994559  0.245934      0.906155  0.000100
323  0.030717  0.996055  0.081214      0.980827  0.000100
324  0.029539  0.995647  0.064768      0.987891  0.000100
325  0.028712  0.997144  0.052338      0.988900  0.000100
326  0.026464  0.997008  0.058144      0.989909  0.000100
327  0.029174  0.996464  0.052917      0.988900  0.000100
328  0.026809  0.997824  0.053716      0.988900  0.000100
329  0.027156  0.997552  0.051324      0.990918  0.000100
330  0.030226  0.996872  0.067435      0.985873  0.000100
331  0.027155  0.996736  0.061474      0.986882  0.000100
332  0.027136  0.997008  0.063970      0.988900  0.000100
333  0.026726  0.997688  0.077970      0.983855  0.000100
334  0.027213  0.997144  0.069610      0.989909  0.000100
335  0.027273  0.996464  0.085659      0.983855  0.000100
336  0.033914  0.994967  0.053246      0.987891  0.000100
337  0.030459  0.997008  0.054649      0.986882  0.000100
338  0.027482  0.996736  0.049948      0.987891  0.000100
339  0.029765  0.996600  0.048909      0.992936  0.000100
340  0.025611  0.997552  0.055517      0.987891  0.000100
341  0.029557  0.995919  0.053696      0.988900  0.000100
342  0.028505  0.996464  0.049105      0.988900  0.000100
343  0.027103  0.997144  0.078729      0.981837  0.000100
344  0.029435  0.996192  0.050688      0.988900  0.000100
345  0.025981  0.997688  0.054764      0.984864  0.000100
346  0.026553  0.996736  0.048161      0.989909  0.000100
347  0.026489  0.996328  0.054038      0.987891  0.000100
348  0.024221  0.997824  0.058056      0.987891  0.000100
349  0.025160  0.997688  0.055907      0.989909  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 128, 9)]     0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 9)       36          input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 128, 32)      288         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 128, 9)       0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 128, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 128, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 128, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 128, 64)      576         max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 128, 256)     0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 128, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 128, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 128, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 128, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 128, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 128, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 128, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 128, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 128, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 128, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 128, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 128, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 128, 256)     2304        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 128, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 128, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 128, 256)     0           batch_normalization_5[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 128, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 128, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 128, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 128, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 128, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 128, 256)     0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 128, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 128, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 128, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 128, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 128, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 128, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 128, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 6)            1542        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,327,754
Trainable params: 1,324,664
Non-trainable params: 3,090
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 93.91615390777588
Test Loss: 0.23206071555614471



Classification Report
                    precision    recall  f1-score   support

           Walking       1.00      0.93      0.96       335
  Walking_Upstairs       1.00      0.92      0.96       316
Walking_Downstairs       0.86      1.00      0.92       284
           Sitting       0.93      0.84      0.88       324
          Standing       0.88      0.94      0.91       342
            Laying       0.98      1.00      0.99       355

          accuracy                           0.94      1956
         macro avg       0.94      0.94      0.94      1956
      weighted avg       0.94      0.94      0.94      1956



Confusion Matrix
[[311   0  24   0   0   0]
 [  0 292  24   0   0   0]
 [  0   0 284   0   0   0]
 [  0   1   0 273  44   6]
 [  0   0   0  20 322   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               92.84              0.00                7.16     0.00   
Walking_Upstairs       0.00             92.41                7.59     0.00   
Walking_Downstairs     0.00              0.00              100.00     0.00   
Sitting                0.00              0.31                0.00    84.26   
Standing               0.00              0.00                0.00     5.85   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                13.58    1.85  
Standing               94.15    0.00  
Laying                  0.00  100.00  


Finished working on: iSPLInception at: 2021-02-10 13:36:10.026793 -> 2628.455573797226

