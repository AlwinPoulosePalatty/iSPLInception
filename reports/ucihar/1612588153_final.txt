This is the report for the ucihar Dataset

Data Distribution: 

Train:  X -> (7352, 128, 9) Class count -> [1226, 1073, 986, 1286, 1374, 1407] 

                    frequency
Walking             16.675735
Walking_Upstairs    14.594668
Walking_Downstairs  13.411316
Sitting             17.491838
Standing            18.688791
Laying              19.137650

Validation:  X -> (991, 128, 9) Class count -> [161, 155, 136, 167, 190, 182] 

                    frequency
Walking             16.246216
Walking_Upstairs    15.640767
Walking_Downstairs  13.723512
Sitting             16.851665
Standing            19.172554
Laying              18.365288

Test:  X -> (1956, 128, 9) Class count -> [335, 316, 284, 324, 342, 355] 

                    frequency
Walking             17.126789
Walking_Upstairs    16.155418
Walking_Downstairs  14.519427
Sitting             16.564417
Standing            17.484661
Laying              18.149284

iSPLInception Model : 2021-02-06 14:47:07.557982
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.333508  0.926959  5.190848      0.479314  0.000500
1    2.796373  0.947225  3.158810      0.530777  0.000500
2    1.675433  0.945049  2.254950      0.643794  0.000500
3    1.159844  0.950354  1.439319      0.863774  0.000500
4    0.868412  0.950762  0.916948      0.972755  0.000500
5    0.659909  0.951170  0.915738      0.865792  0.000500
6    0.522935  0.952530  2.641055      0.577195  0.000500
7    0.467932  0.954026  0.674341      0.934410  0.000500
8    0.380950  0.955794  1.430311      0.740666  0.000500
9    0.340919  0.955522  0.449457      0.947528  0.000500
10   0.273997  0.958379  0.588685      0.849647  0.000500
11   0.259522  0.953482  0.383666      0.947528  0.000500
12   0.238355  0.957291  0.519234      0.917255  0.000500
13   0.216078  0.958651  0.427605      0.875883  0.000500
14   0.207020  0.957427  0.669350      0.817356  0.000500
15   0.223366  0.955522  0.317447      0.949546  0.000500
16   0.196470  0.954434  0.277598      0.942482  0.000500
17   0.202193  0.954434  0.378393      0.905146  0.000500
18   0.181074  0.960011  0.342466      0.883956  0.000500
19   0.167526  0.959331  0.444037      0.880928  0.000500
20   0.158833  0.959739  0.282227      0.959637  0.000500
21   0.163410  0.958515  0.654332      0.825429  0.000500
22   0.179811  0.959603  0.395732      0.906155  0.000500
23   0.165055  0.961779  0.231113      0.971746  0.000500
24   0.161568  0.958651  0.689802      0.715439  0.000500
25   0.150567  0.961235  0.193392      0.960646  0.000500
26   0.157498  0.958379  0.259344      0.919273  0.000500
27   0.162699  0.960283  0.168588      0.968718  0.000500
28   0.138689  0.962731  0.354027      0.909183  0.000500
29   0.156875  0.957291  0.722989      0.849647  0.000500
30   0.140857  0.960283  0.239695      0.947528  0.000500
31   0.129038  0.962867  0.299729      0.911201  0.000500
32   0.134026  0.964091  0.231191      0.947528  0.000500
33   0.131312  0.960691  0.267195      0.907164  0.000500
34   0.140791  0.962051  0.225283      0.932392  0.000500
35   0.128111  0.962731  0.250711      0.949546  0.000500
36   0.148829  0.962323  0.768682      0.855701  0.000500
37   0.156632  0.960011  0.379219      0.901110  0.000500
38   0.134037  0.963275  0.310583      0.919273  0.000500
39   0.126303  0.963003  0.290796      0.932392  0.000500
40   0.152563  0.961779  0.540144      0.797175  0.000500
41   0.146584  0.963819  0.218361      0.953582  0.000500
42   0.131927  0.963955  0.539083      0.915237  0.000500
43   0.156023  0.962187  0.162777      0.965691  0.000500
44   0.127358  0.965044  0.343365      0.893037  0.000500
45   0.114995  0.967220  0.303878      0.928355  0.000500
46   0.121093  0.962187  0.198866      0.960646  0.000500
47   0.122745  0.963003  0.156627      0.963673  0.000500
48   0.122057  0.964908  0.252072      0.923310  0.000500
49   0.123445  0.967084  0.252080      0.917255  0.000500
50   0.139992  0.960691  0.155104      0.965691  0.000500
51   0.120935  0.965180  0.249605      0.938446  0.000500
52   0.128890  0.963955  0.605320      0.841574  0.000500
53   0.132315  0.966132  0.202624      0.959637  0.000500
54   0.122154  0.968852  0.272560      0.907164  0.000500
55   0.122112  0.963411  0.149843      0.979818  0.000500
56   0.108755  0.967764  0.198596      0.953582  0.000400
57   0.109052  0.966812  0.144196      0.965691  0.000400
58   0.103473  0.967628  0.187695      0.944501  0.000400
59   0.111980  0.967220  0.158379      0.963673  0.000400
60   0.110592  0.968580  0.688730      0.810293  0.000400
61   0.109913  0.969260  0.137289      0.964682  0.000400
62   0.108159  0.969396  0.216219      0.934410  0.000400
63   0.102527  0.971028  0.206679      0.941473  0.000400
64   0.103515  0.969260  0.168981      0.951564  0.000400
65   0.109936  0.967356  0.388396      0.924319  0.000400
66   0.104393  0.970756  0.143123      0.963673  0.000400
67   0.103800  0.969668  0.143918      0.966700  0.000400
68   0.103068  0.970620  0.147282      0.964682  0.000400
69   0.097886  0.971708  0.209105      0.959637  0.000400
70   0.101875  0.969124  0.429118      0.882946  0.000400
71   0.100860  0.971980  0.193087      0.957619  0.000400
72   0.099559  0.973205  0.131912      0.973764  0.000400
73   0.106867  0.970076  0.264745      0.926337  0.000400
74   0.102598  0.971844  0.115039      0.979818  0.000400
75   0.100697  0.972933  0.148221      0.978809  0.000400
76   0.097593  0.974429  0.130462      0.973764  0.000400
77   0.092662  0.975381  0.140318      0.967709  0.000400
78   0.094562  0.972933  0.131916      0.974773  0.000400
79   0.095147  0.976197  0.458530      0.898083  0.000400
80   0.098135  0.974021  0.095198      0.981837  0.000400
81   0.086435  0.977965  0.101312      0.988900  0.000400
82   0.089384  0.975381  0.094781      0.983855  0.000400
83   0.090534  0.976605  0.123162      0.962664  0.000400
84   0.089388  0.976197  0.141622      0.982846  0.000400
85   0.089409  0.976333  0.111423      0.976791  0.000400
86   0.089965  0.977829  2.173072      0.774975  0.000400
87   0.086708  0.981366  0.445308      0.901110  0.000400
88   0.088705  0.977557  0.137180      0.979818  0.000400
89   0.085964  0.979597  0.205007      0.950555  0.000400
90   0.090257  0.979461  0.102932      0.985873  0.000400
91   0.076280  0.981910  0.184575      0.952573  0.000400
92   0.075402  0.983134  0.203450      0.922301  0.000400
93   0.087369  0.980413  0.379540      0.881937  0.000400
94   0.091313  0.977829  0.193898      0.946519  0.000400
95   0.086362  0.981638  0.129765      0.973764  0.000400
96   0.085403  0.979869  0.132609      0.961655  0.000400
97   0.078748  0.982862  0.098709      0.977800  0.000400
98   0.082635  0.980686  0.210340      0.940464  0.000400
99   0.077349  0.983950  0.115680      0.967709  0.000400
100  0.074879  0.984902  0.104533      0.976791  0.000400
101  0.070128  0.984358  0.118499      0.968718  0.000400
102  0.080169  0.980686  0.524786      0.795156  0.000400
103  0.081341  0.983134  0.104387      0.974773  0.000400
104  0.077170  0.983542  0.574512      0.775984  0.000400
105  0.079617  0.984902  0.092828      0.984864  0.000400
106  0.079041  0.982454  0.290740      0.918264  0.000400
107  0.075253  0.982998  0.283252      0.908174  0.000400
108  0.072808  0.983270  0.150283      0.961655  0.000400
109  0.071983  0.984358  0.135671      0.953582  0.000400
110  0.068595  0.985174  0.094126      0.982846  0.000400
111  0.074406  0.983678  0.160994      0.956609  0.000400
112  0.070785  0.985854  0.192647      0.948537  0.000400
113  0.070773  0.981774  0.323549      0.944501  0.000400
114  0.069333  0.986534  0.259796      0.918264  0.000400
115  0.074269  0.984630  0.117562      0.978809  0.000400
116  0.079078  0.982862  0.146140      0.955600  0.000400
117  0.069078  0.986262  0.073976      0.988900  0.000400
118  0.074558  0.984766  0.267205      0.902119  0.000400
119  0.077339  0.982998  0.117409      0.975782  0.000400
120  0.068717  0.985718  0.153499      0.956609  0.000400
121  0.071014  0.983542  0.115149      0.977800  0.000320
122  0.065948  0.986670  0.116032      0.972755  0.000320
123  0.068539  0.985446  0.095237      0.978809  0.000320
124  0.064038  0.987486  0.108712      0.966700  0.000320
125  0.066834  0.985718  0.072230      0.983855  0.000320
126  0.063903  0.987894  0.072763      0.986882  0.000320
127  0.065621  0.985174  0.116184      0.968718  0.000320
128  0.062397  0.988575  0.098481      0.984864  0.000320
129  0.061563  0.986942  0.107205      0.969728  0.000320
130  0.062942  0.987758  0.592777      0.837538  0.000320
131  0.061448  0.988030  0.085072      0.984864  0.000320
132  0.065314  0.985718  0.084439      0.980827  0.000320
133  0.064375  0.986942  0.093256      0.981837  0.000320
134  0.062591  0.985854  0.148749      0.963673  0.000320
135  0.071826  0.985990  0.074124      0.988900  0.000320
136  0.065652  0.985174  0.169595      0.943491  0.000320
137  0.064316  0.987486  0.329248      0.903128  0.000320
138  0.060922  0.988439  0.112954      0.967709  0.000320
139  0.062050  0.986942  0.073432      0.984864  0.000320
140  0.062451  0.987078  0.083201      0.984864  0.000320
141  0.063636  0.986670  0.071611      0.987891  0.000320
142  0.059628  0.988303  0.062077      0.985873  0.000320
143  0.057624  0.989527  0.085623      0.976791  0.000320
144  0.065786  0.985990  0.141384      0.967709  0.000320
145  0.064369  0.986126  0.148409      0.962664  0.000320
146  0.058599  0.988711  0.109019      0.974773  0.000320
147  0.056480  0.989663  0.090623      0.981837  0.000320
148  0.057896  0.988167  0.097775      0.983855  0.000320
149  0.058240  0.986806  0.096386      0.976791  0.000320
150  0.057131  0.988303  0.071121      0.986882  0.000320
151  0.058585  0.988711  0.103508      0.974773  0.000320
152  0.057240  0.986534  0.118442      0.972755  0.000320
153  0.062997  0.986670  0.090173      0.982846  0.000320
154  0.062224  0.987894  0.091268      0.984864  0.000320
155  0.058442  0.987622  0.083088      0.978809  0.000320
156  0.070747  0.987078  0.076250      0.985873  0.000320
157  0.066072  0.986806  0.086430      0.988900  0.000320
158  0.061424  0.988847  0.140208      0.968718  0.000256
159  0.056005  0.989663  0.110078      0.982846  0.000256
160  0.058398  0.988575  0.123103      0.964682  0.000256
161  0.054623  0.989663  0.084863      0.990918  0.000256
162  0.059248  0.985854  0.083361      0.983855  0.000256
163  0.051978  0.990751  0.071427      0.987891  0.000256
164  0.057312  0.988711  0.147494      0.956609  0.000256
165  0.057818  0.989391  0.069289      0.985873  0.000256
166  0.059229  0.987894  0.075703      0.986882  0.000256
167  0.051217  0.990479  0.068954      0.985873  0.000256
168  0.053893  0.989391  0.068976      0.989909  0.000256
169  0.052580  0.989119  0.086195      0.980827  0.000256
170  0.049475  0.990887  0.088700      0.980827  0.000256
171  0.055014  0.989663  0.069604      0.987891  0.000256
172  0.060245  0.985854  0.090653      0.976791  0.000256
173  0.055266  0.989527  0.110263      0.971746  0.000256
174  0.052772  0.991023  0.206625      0.946519  0.000256
175  0.060882  0.987214  0.086021      0.982846  0.000256
176  0.054840  0.988847  0.076795      0.984864  0.000256
177  0.053479  0.990207  0.093494      0.983855  0.000256
178  0.048995  0.991975  0.184984      0.953582  0.000256
179  0.050516  0.990343  0.084904      0.981837  0.000256
180  0.049634  0.990479  0.080596      0.980827  0.000256
181  0.049213  0.990343  0.070637      0.985873  0.000256
182  0.050492  0.989799  0.064226      0.992936  0.000256
183  0.046913  0.992247  0.083112      0.981837  0.000256
184  0.049096  0.990887  0.054777      0.990918  0.000256
185  0.056264  0.989255  0.110634      0.981837  0.000256
186  0.049134  0.991023  0.060617      0.989909  0.000256
187  0.050074  0.991159  0.077449      0.980827  0.000256
188  0.055971  0.988847  0.185603      0.951564  0.000256
189  0.050979  0.991567  0.105474      0.976791  0.000256
190  0.050366  0.991975  0.100717      0.974773  0.000256
191  0.048972  0.991567  0.084712      0.982846  0.000256
192  0.052310  0.990343  0.084366      0.976791  0.000256
193  0.045917  0.992519  0.076635      0.986882  0.000256
194  0.049203  0.990479  0.078604      0.984864  0.000256
195  0.052056  0.989935  0.100015      0.975782  0.000256
196  0.049270  0.991023  0.095291      0.975782  0.000256
197  0.046663  0.991431  0.146398      0.952573  0.000256
198  0.047514  0.993199  0.088973      0.986882  0.000256
199  0.045728  0.991567  0.066192      0.991927  0.000256
200  0.045840  0.992111  0.106469      0.972755  0.000256
201  0.046065  0.992383  0.091991      0.982846  0.000256
202  0.048670  0.990751  0.085923      0.979818  0.000256
203  0.051284  0.990071  0.078787      0.980827  0.000256
204  0.049897  0.991023  0.085620      0.975782  0.000256
205  0.050084  0.989663  0.080369      0.985873  0.000256
206  0.046230  0.992111  0.078061      0.981837  0.000256
207  0.044236  0.992519  0.070022      0.984864  0.000256
208  0.045554  0.991975  0.113875      0.966700  0.000256
209  0.049245  0.992519  0.066097      0.987891  0.000256
210  0.047012  0.991567  0.078989      0.986882  0.000256
211  0.050137  0.990887  0.086800      0.982846  0.000256
212  0.050152  0.990207  0.073128      0.985873  0.000256
213  0.044706  0.992519  0.076200      0.988900  0.000256
214  0.045678  0.991431  0.087779      0.980827  0.000256
215  0.048049  0.992111  0.246873      0.927346  0.000256
216  0.052904  0.990207  0.082428      0.984864  0.000256
217  0.052669  0.987894  0.063649      0.990918  0.000256
218  0.044286  0.993607  0.081123      0.983855  0.000205
219  0.050660  0.989527  0.096217      0.980827  0.000205
220  0.045952  0.991975  0.061457      0.990918  0.000205
221  0.045868  0.991703  0.080559      0.983855  0.000205
222  0.042300  0.993607  0.351994      0.889001  0.000205
223  0.046984  0.992383  0.159148      0.955600  0.000205
224  0.042714  0.993335  0.074577      0.987891  0.000205
225  0.041927  0.992791  0.069576      0.987891  0.000205
226  0.038478  0.995511  0.058297      0.990918  0.000205
227  0.048349  0.990343  0.077594      0.984864  0.000205
228  0.049514  0.990615  0.076175      0.979818  0.000205
229  0.043873  0.992111  0.064169      0.988900  0.000205
230  0.043286  0.992247  0.069398      0.990918  0.000205
231  0.048432  0.990343  0.074357      0.982846  0.000205
232  0.038889  0.995783  0.069560      0.984864  0.000205
233  0.042283  0.992791  0.061667      0.990918  0.000205
234  0.046104  0.991975  0.084157      0.980827  0.000205
235  0.042335  0.993879  0.070553      0.987891  0.000205
236  0.040911  0.992927  0.078901      0.985873  0.000205
237  0.044438  0.991567  0.081413      0.978809  0.000164
238  0.042309  0.993879  0.056543      0.990918  0.000164
239  0.043309  0.992791  0.068263      0.987891  0.000164
240  0.041290  0.993335  0.061477      0.989909  0.000164
241  0.043295  0.991567  0.072642      0.988900  0.000164
242  0.039862  0.992655  0.075738      0.983855  0.000164
243  0.042869  0.991703  0.063677      0.987891  0.000164
244  0.043706  0.993471  0.056614      0.989909  0.000164
245  0.037403  0.994831  0.068521      0.988900  0.000164
246  0.040634  0.993607  0.062094      0.988900  0.000164
247  0.037596  0.993199  0.055069      0.993946  0.000164
248  0.044828  0.991703  0.061743      0.984864  0.000164
249  0.038758  0.993335  0.104781      0.979818  0.000164
250  0.045933  0.993743  0.075294      0.984864  0.000164
251  0.044307  0.992655  0.128780      0.966700  0.000164
252  0.038649  0.995103  0.089326      0.977800  0.000164
253  0.038118  0.995103  0.070804      0.989909  0.000164
254  0.040855  0.994015  0.069061      0.986882  0.000164
255  0.044676  0.991431  0.075917      0.986882  0.000164
256  0.038455  0.995103  0.075536      0.986882  0.000131
257  0.040914  0.992519  0.077416      0.983855  0.000131
258  0.040119  0.994287  0.062601      0.987891  0.000131
259  0.039169  0.994151  0.100710      0.974773  0.000131
260  0.036182  0.994287  0.083509      0.982846  0.000131
261  0.035221  0.995239  0.063038      0.989909  0.000131
262  0.037303  0.994151  0.069974      0.985873  0.000131
263  0.036291  0.994831  0.062176      0.986882  0.000131
264  0.036052  0.994967  0.073817      0.984864  0.000131
265  0.036499  0.994967  0.070078      0.987891  0.000131
266  0.035994  0.995375  0.058027      0.989909  0.000131
267  0.036377  0.995103  0.054595      0.989909  0.000131
268  0.035646  0.995239  0.066293      0.984864  0.000131
269  0.035680  0.995375  0.072640      0.985873  0.000131
270  0.034954  0.994423  0.060751      0.988900  0.000131
271  0.040846  0.991975  0.081755      0.980827  0.000131
272  0.036197  0.994967  0.058957      0.988900  0.000131
273  0.036886  0.994151  0.078120      0.983855  0.000131
274  0.034386  0.994423  0.066340      0.984864  0.000131
275  0.032260  0.995511  0.060289      0.987891  0.000131
276  0.037313  0.994151  0.052791      0.989909  0.000131
277  0.033692  0.995511  0.077266      0.983855  0.000131
278  0.033572  0.995783  0.064194      0.988900  0.000131
279  0.035007  0.995103  0.068963      0.985873  0.000131
280  0.034052  0.994967  0.071456      0.988900  0.000131
281  0.036535  0.994287  0.208105      0.923310  0.000131
282  0.035698  0.994287  0.057503      0.988900  0.000131
283  0.037358  0.993607  0.084435      0.980827  0.000131
284  0.036318  0.993471  0.085006      0.974773  0.000131
285  0.034748  0.995375  0.078463      0.985873  0.000131
286  0.031761  0.996464  0.154798      0.960646  0.000105
287  0.032912  0.995783  0.072110      0.984864  0.000105
288  0.032308  0.996192  0.057418      0.990918  0.000105
289  0.030359  0.996872  0.056796      0.992936  0.000105
290  0.041342  0.994015  0.068981      0.987891  0.000105
291  0.037675  0.995103  0.074956      0.986882  0.000105
292  0.035879  0.994967  0.081055      0.988900  0.000105
293  0.033143  0.996600  0.079681      0.988900  0.000105
294  0.035464  0.993743  0.075982      0.987891  0.000105
295  0.035767  0.994831  0.077014      0.984864  0.000105
296  0.034975  0.995103  0.068409      0.987891  0.000105
297  0.031023  0.996192  0.083284      0.983855  0.000105
298  0.035985  0.994151  0.076102      0.983855  0.000105
299  0.031304  0.996464  0.063550      0.992936  0.000105
300  0.032821  0.995375  0.065315      0.988900  0.000100
301  0.030621  0.996600  0.058914      0.990918  0.000100
302  0.031468  0.995919  0.054330      0.990918  0.000100
303  0.029074  0.996736  0.095074      0.977800  0.000100
304  0.034598  0.994967  0.055544      0.990918  0.000100
305  0.035323  0.994695  0.076012      0.980827  0.000100
306  0.031411  0.995103  0.058942      0.990918  0.000100
307  0.034687  0.994287  0.060229      0.987891  0.000100
308  0.036002  0.995103  0.064293      0.989909  0.000100
309  0.031971  0.995919  0.063650      0.987891  0.000100
310  0.028733  0.997144  0.053325      0.992936  0.000100
311  0.030939  0.995919  0.070280      0.982846  0.000100
312  0.032329  0.996055  0.056737      0.990918  0.000100
313  0.030702  0.996055  0.058964      0.988900  0.000100
314  0.029510  0.996872  0.059400      0.987891  0.000100
315  0.032968  0.995103  0.073504      0.984864  0.000100
316  0.030633  0.995919  0.079750      0.980827  0.000100
317  0.031046  0.995783  0.055051      0.990918  0.000100
318  0.029336  0.996464  0.072038      0.985873  0.000100
319  0.030339  0.995647  0.070102      0.983855  0.000100
320  0.030987  0.995919  0.065570      0.985873  0.000100
321  0.032311  0.995375  0.074150      0.982846  0.000100
322  0.029709  0.995919  0.052012      0.989909  0.000100
323  0.029689  0.995919  0.053668      0.990918  0.000100
324  0.031996  0.994695  0.054053      0.991927  0.000100
325  0.029170  0.996192  0.051257      0.992936  0.000100
326  0.029356  0.996600  0.052174      0.990918  0.000100
327  0.031837  0.994695  0.060196      0.986882  0.000100
328  0.030113  0.996328  0.122648      0.967709  0.000100
329  0.034215  0.994423  0.062155      0.989909  0.000100
330  0.031732  0.995103  0.054882      0.991927  0.000100
331  0.029220  0.995783  0.065724      0.988900  0.000100
332  0.030692  0.996192  0.064456      0.990918  0.000100
333  0.029890  0.996328  0.063845      0.985873  0.000100
334  0.031007  0.995783  0.060506      0.986882  0.000100
335  0.030068  0.995919  0.059598      0.991927  0.000100
336  0.028912  0.996736  0.057996      0.990918  0.000100
337  0.036874  0.993335  0.059956      0.988900  0.000100
338  0.029345  0.996192  0.062383      0.990918  0.000100
339  0.027665  0.997144  0.056526      0.990918  0.000100
340  0.031800  0.995239  0.056153      0.994955  0.000100
341  0.030138  0.995919  0.056594      0.993946  0.000100
342  0.033583  0.994695  0.050736      0.993946  0.000100
343  0.030345  0.995919  0.049700      0.993946  0.000100
344  0.028130  0.997416  0.066379      0.987891  0.000100
345  0.029736  0.995783  0.058789      0.987891  0.000100
346  0.028496  0.996736  0.054582      0.992936  0.000100
347  0.031572  0.995375  0.052055      0.992936  0.000100
348  0.031845  0.995647  0.053019      0.993946  0.000100
349  0.031732  0.994967  0.081422      0.975782  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 128, 9)]     0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 9)       36          input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 128, 32)      288         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 128, 9)       0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 128, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 128, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 128, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 128, 64)      576         max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 128, 256)     0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 128, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 128, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 128, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 128, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 128, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 128, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 128, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 128, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 128, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 128, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 128, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 128, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 128, 256)     2304        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 128, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 128, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 128, 256)     0           batch_normalization_5[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 128, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 128, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 128, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 128, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 128, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 128, 256)     0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 128, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 128, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 128, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 128, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 128, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 128, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 128, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 6)            1542        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,327,754
Trainable params: 1,324,664
Non-trainable params: 3,090
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 95.09202241897583
Test Loss: 0.17614002525806427



Classification Report
                    precision    recall  f1-score   support

           Walking       1.00      0.88      0.93       335
  Walking_Upstairs       0.94      0.96      0.95       316
Walking_Downstairs       0.88      1.00      0.94       284
           Sitting       0.97      0.90      0.93       324
          Standing       0.92      0.97      0.95       342
            Laying       1.00      1.00      1.00       355

          accuracy                           0.95      1956
         macro avg       0.95      0.95      0.95      1956
      weighted avg       0.95      0.95      0.95      1956



Confusion Matrix
[[294  17  24   0   0   0]
 [  0 302  13   0   1   0]
 [  0   0 284   0   0   0]
 [  0   3   0 292  28   1]
 [  0   0   0   9 333   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               87.76              5.07                7.16     0.00   
Walking_Upstairs       0.00             95.57                4.11     0.00   
Walking_Downstairs     0.00              0.00              100.00     0.00   
Sitting                0.00              0.93                0.00    90.12   
Standing               0.00              0.00                0.00     2.63   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.32    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                 8.64    0.31  
Standing               97.37    0.00  
Laying                  0.00  100.00  

Finished working on: iSPLInception at: 2021-02-06 14:47:09.845225 -> 2277.284042596817

CNN Model : 2021-02-06 14:53:12.892949
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.676846  0.844124  1.292177      0.526741  0.000500
1    0.435505  0.946681  0.849375      0.754793  0.000500
2    0.381605  0.950626  0.553760      0.920283  0.000500
3    0.350140  0.951850  0.399689      0.948537  0.000500
4    0.323913  0.953890  0.375677      0.966700  0.000500
5    0.307390  0.953074  0.331243      0.944501  0.000500
6    0.287657  0.955522  0.314303      0.942482  0.000500
7    0.268565  0.956882  0.293877      0.957619  0.000500
8    0.251121  0.957155  0.306761      0.946519  0.000500
9    0.244435  0.958515  0.303661      0.963673  0.000500
10   0.230827  0.957563  0.263466      0.960646  0.000500
11   0.219823  0.961779  0.224746      0.968718  0.000500
12   0.207184  0.962459  0.251437      0.951564  0.000500
13   0.200713  0.962867  0.242243      0.962664  0.000500
14   0.193615  0.963139  0.212251      0.967709  0.000500
15   0.192785  0.960147  0.195653      0.961655  0.000500
16   0.179572  0.963547  0.251583      0.940464  0.000500
17   0.173078  0.962459  0.221842      0.960646  0.000500
18   0.170406  0.962323  0.198445      0.960646  0.000500
19   0.159767  0.966268  0.247147      0.948537  0.000500
20   0.160780  0.967220  0.201064      0.965691  0.000500
21   0.153387  0.965588  0.209783      0.948537  0.000500
22   0.150668  0.967900  0.200958      0.948537  0.000500
23   0.145258  0.967220  0.206068      0.951564  0.000500
24   0.139939  0.966404  0.175995      0.955600  0.000500
25   0.133291  0.969804  0.177194      0.958628  0.000500
26   0.133011  0.967492  0.224206      0.933401  0.000500
27   0.133658  0.970076  0.175135      0.958628  0.000500
28   0.122958  0.971572  0.205184      0.949546  0.000500
29   0.127358  0.969668  0.189749      0.941473  0.000500
30   0.118122  0.972524  0.202375      0.941473  0.000500
31   0.117723  0.972660  0.204496      0.942482  0.000500
32   0.116166  0.970484  0.182087      0.954591  0.000500
33   0.118246  0.970756  0.184285      0.944501  0.000500
34   0.117244  0.968580  0.156979      0.962664  0.000500
35   0.104475  0.972660  0.191670      0.948537  0.000500
36   0.107123  0.972660  0.167635      0.959637  0.000500
37   0.108232  0.976197  0.155001      0.954591  0.000500
38   0.101195  0.973613  0.200493      0.948537  0.000500
39   0.104552  0.975789  0.184730      0.947528  0.000500
40   0.102265  0.973749  0.187389      0.941473  0.000500
41   0.098153  0.976605  0.201818      0.949546  0.000500
42   0.095040  0.976605  0.328416      0.891019  0.000500
43   0.092201  0.979325  0.213671      0.939455  0.000500
44   0.093942  0.977557  0.164489      0.951564  0.000500
45   0.090309  0.977285  0.257689      0.908174  0.000500
46   0.088992  0.976877  0.234033      0.937437  0.000500
47   0.086974  0.979189  0.217384      0.939455  0.000500
48   0.089669  0.974701  0.220302      0.934410  0.000500
49   0.085624  0.978917  0.247981      0.925328  0.000500
50   0.085395  0.977965  0.219644      0.935419  0.000500
51   0.073705  0.983542  0.206583      0.944501  0.000500
52   0.083201  0.979733  0.202660      0.940464  0.000500
53   0.078059  0.981502  0.221225      0.935419  0.000500
54   0.077781  0.981910  0.185329      0.942482  0.000500
55   0.074767  0.983134  0.200672      0.941473  0.000500
56   0.076816  0.983678  0.200653      0.944501  0.000500
57   0.082415  0.980550  0.219023      0.933401  0.000500
58   0.068494  0.985310  0.203843      0.946519  0.000500
59   0.066717  0.985038  0.204227      0.943491  0.000500
60   0.066814  0.985990  0.208575      0.939455  0.000500
61   0.065636  0.985038  0.385021      0.868819  0.000500
62   0.078703  0.979597  0.209898      0.941473  0.000500
63   0.068394  0.984630  0.200347      0.945510  0.000500
64   0.066813  0.984902  0.190283      0.950555  0.000500
65   0.064340  0.985854  0.194008      0.947528  0.000500
66   0.066845  0.984222  0.236414      0.925328  0.000500
67   0.073212  0.981638  0.229238      0.935419  0.000500
68   0.072192  0.982590  0.183083      0.950555  0.000500
69   0.060941  0.986534  0.220458      0.938446  0.000500
70   0.059875  0.987350  0.230989      0.929364  0.000500
71   0.068163  0.984358  0.229959      0.935419  0.000500
72   0.061166  0.987350  0.196990      0.940464  0.000500
73   0.061784  0.985174  0.226904      0.934410  0.000500
74   0.062163  0.985174  0.218503      0.933401  0.000500
75   0.068717  0.983270  0.214415      0.933401  0.000500
76   0.060341  0.984902  0.192291      0.946519  0.000500
77   0.063100  0.985310  0.193195      0.945510  0.000500
78   0.062348  0.985718  0.166145      0.953582  0.000500
79   0.054125  0.989255  0.151524      0.959637  0.000500
80   0.051603  0.988303  0.185477      0.948537  0.000500
81   0.051839  0.989119  0.214687      0.938446  0.000500
82   0.070491  0.981774  0.201243      0.944501  0.000500
83   0.057859  0.987758  0.172264      0.949546  0.000500
84   0.048927  0.990615  0.198580      0.939455  0.000500
85   0.051694  0.989119  0.249043      0.935419  0.000500
86   0.051697  0.989527  0.190741      0.949546  0.000500
87   0.050604  0.989119  0.217648      0.941473  0.000500
88   0.053423  0.989255  0.205941      0.940464  0.000500
89   0.043304  0.992655  0.204874      0.943491  0.000500
90   0.043993  0.992655  0.232310      0.930373  0.000500
91   0.042867  0.993199  0.210067      0.943491  0.000500
92   0.043876  0.991567  0.244015      0.922301  0.000500
93   0.052573  0.989527  0.228779      0.934410  0.000500
94   0.049885  0.990479  0.216814      0.943491  0.000500
95   0.053353  0.988711  0.217517      0.934410  0.000500
96   0.048705  0.991159  0.207193      0.946519  0.000500
97   0.050324  0.990751  0.189381      0.944501  0.000500
98   0.050066  0.991295  0.190140      0.945510  0.000500
99   0.041916  0.993063  0.173020      0.951564  0.000500
100  0.046090  0.991431  0.243665      0.937437  0.000500
101  0.052370  0.987486  0.245468      0.936428  0.000500
102  0.044602  0.992927  0.248238      0.934410  0.000500
103  0.043944  0.992247  0.240533      0.941473  0.000500
104  0.043975  0.991431  0.246042      0.939455  0.000500
105  0.037867  0.994423  0.210589      0.951564  0.000500
106  0.037657  0.995103  0.200915      0.948537  0.000500
107  0.051139  0.991159  0.381692      0.898083  0.000500
108  0.048154  0.991023  0.416886      0.873865  0.000500
109  0.041465  0.993743  0.188722      0.940464  0.000500
110  0.045345  0.992111  0.222045      0.939455  0.000500
111  0.046801  0.991159  0.162009      0.958628  0.000500
112  0.037692  0.995375  0.239816      0.941473  0.000500
113  0.042971  0.992111  0.232325      0.936428  0.000500
114  0.044728  0.992111  0.189661      0.950555  0.000500
115  0.046060  0.991703  0.241051      0.928355  0.000500
116  0.047576  0.991023  0.219542      0.942482  0.000500
117  0.036601  0.995375  0.224379      0.941473  0.000400
118  0.035433  0.996328  0.259873      0.945510  0.000400
119  0.036333  0.994831  0.212775      0.948537  0.000400
120  0.030706  0.997688  0.227235      0.950555  0.000400
121  0.037947  0.995239  0.245532      0.938446  0.000400
122  0.029985  0.997280  0.236348      0.944501  0.000400
123  0.035640  0.994831  0.275082      0.940464  0.000400
124  0.032140  0.996328  0.224618      0.946519  0.000400
125  0.035818  0.995103  0.265148      0.943491  0.000400
126  0.033841  0.995375  0.229363      0.950555  0.000400
127  0.035721  0.994423  0.266214      0.931382  0.000400
128  0.030538  0.997144  0.214553      0.954591  0.000400
129  0.028108  0.997960  0.231473      0.951564  0.000400
130  0.029155  0.996872  0.207994      0.953582  0.000400
131  0.032476  0.995103  0.228612      0.950555  0.000400
132  0.033836  0.994559  0.263308      0.946519  0.000400
133  0.036562  0.995103  0.225235      0.949546  0.000400
134  0.037238  0.994967  0.239945      0.949546  0.000400
135  0.031912  0.996328  0.271461      0.944501  0.000400
136  0.031110  0.995647  0.262002      0.935419  0.000400
137  0.031204  0.995239  0.209014      0.954591  0.000400
138  0.029289  0.996328  0.244427      0.947528  0.000400
139  0.033600  0.995647  0.271919      0.940464  0.000400
140  0.029785  0.997008  0.236908      0.946519  0.000320
141  0.030656  0.997416  0.243603      0.948537  0.000320
142  0.030045  0.996872  0.181881      0.957619  0.000320
143  0.030662  0.996055  0.232456      0.952573  0.000320
144  0.027084  0.997552  0.223916      0.954591  0.000320
145  0.025871  0.997824  0.202604      0.956609  0.000320
146  0.031028  0.995647  0.241466      0.948537  0.000320
147  0.029492  0.996192  0.246312      0.946519  0.000320
148  0.025054  0.997960  0.230156      0.952573  0.000320
149  0.027017  0.997552  0.242482      0.948537  0.000320
150  0.027488  0.997416  0.268379      0.947528  0.000320
151  0.022996  0.998912  0.265998      0.948537  0.000320
152  0.023376  0.998504  0.274393      0.943491  0.000320
153  0.024526  0.998096  0.253680      0.945510  0.000320
154  0.025016  0.998232  0.249185      0.949546  0.000320
155  0.036349  0.995375  0.257158      0.945510  0.000320
156  0.028060  0.997824  0.258245      0.945510  0.000320
157  0.027073  0.997280  0.242146      0.943491  0.000320
158  0.027497  0.996872  0.312186      0.934410  0.000320
159  0.028311  0.996464  0.251743      0.944501  0.000320
160  0.025593  0.998232  0.228892      0.945510  0.000320
161  0.028600  0.997416  0.258284      0.950555  0.000320
162  0.023707  0.998232  0.277251      0.950555  0.000256
163  0.024761  0.997824  0.268433      0.948537  0.000256
164  0.025711  0.997960  0.261860      0.948537  0.000256
165  0.022402  0.999320  0.268390      0.949546  0.000256
166  0.022873  0.998776  0.241017      0.948537  0.000256
167  0.025721  0.997280  0.271910      0.944501  0.000256
168  0.022649  0.998776  0.249626      0.951564  0.000256
169  0.026876  0.997552  0.245024      0.949546  0.000256
170  0.024562  0.997552  0.232426      0.951564  0.000256
171  0.029132  0.995647  0.213456      0.956609  0.000256
172  0.024259  0.997552  0.248571      0.951564  0.000256
173  0.023845  0.998912  0.255296      0.950555  0.000256
174  0.022005  0.998232  0.231153      0.951564  0.000256
175  0.023295  0.998368  0.258381      0.954591  0.000256
176  0.020258  0.999320  0.284231      0.949546  0.000256
177  0.022820  0.998232  0.290312      0.942482  0.000256
178  0.021023  0.998640  0.261807      0.946519  0.000256
179  0.020551  0.998776  0.279946      0.949546  0.000256

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_7 (Batch (None, 128, 9)            36        
_________________________________________________________________
conv1d_26 (Conv1D)           (None, 128, 32)           896       
_________________________________________________________________
batch_normalization_8 (Batch (None, 128, 32)           128       
_________________________________________________________________
activation_6 (Activation)    (None, 128, 32)           0         
_________________________________________________________________
conv1d_27 (Conv1D)           (None, 128, 64)           6208      
_________________________________________________________________
batch_normalization_9 (Batch (None, 128, 64)           256       
_________________________________________________________________
activation_7 (Activation)    (None, 128, 64)           0         
_________________________________________________________________
conv1d_28 (Conv1D)           (None, 128, 32)           6176      
_________________________________________________________________
batch_normalization_10 (Batc (None, 128, 32)           128       
_________________________________________________________________
activation_8 (Activation)    (None, 128, 32)           0         
_________________________________________________________________
flatten (Flatten)            (None, 4096)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               409700    
_________________________________________________________________
activation_9 (Activation)    (None, 100)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 606       
_________________________________________________________________
batch_normalization_11 (Batc (None, 6)                 24        
_________________________________________________________________
activation_10 (Activation)   (None, 6)                 0         
=================================================================
Total params: 424,158
Trainable params: 423,872
Non-trainable params: 286
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Filters: [32, 64, 32]
Fc hidden nodes: 100
Learning rate: 0.0005
Regularization rate: 9.3e-05


Test Accuracy: 90.64416885375977
Test Loss: 0.42310479283332825



Classification Report
                    precision    recall  f1-score   support

           Walking       0.98      0.99      0.98       335
  Walking_Upstairs       0.98      0.92      0.95       316
Walking_Downstairs       0.91      0.98      0.94       284
           Sitting       0.81      0.71      0.76       324
          Standing       0.77      0.85      0.81       342
            Laying       0.99      1.00      0.99       355

          accuracy                           0.91      1956
         macro avg       0.91      0.91      0.91      1956
      weighted avg       0.91      0.91      0.91      1956



Confusion Matrix
[[331   1   3   0   0   0]
 [  1 290  25   0   0   0]
 [  6   0 277   1   0   0]
 [  0   5   0 229  86   4]
 [  0   0   0  51 291   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               98.81              0.30                0.90     0.00   
Walking_Upstairs       0.32             91.77                7.91     0.00   
Walking_Downstairs     2.11              0.00               97.54     0.35   
Sitting                0.00              1.54                0.00    70.68   
Standing               0.00              0.00                0.00    14.91   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                26.54    1.23  
Standing               85.09    0.00  
Laying                  0.00  100.00  


Finished working on: CNN at: 2021-02-06 14:53:14.634180 -> 364.78895449638367

CNN_LSTM Model : 2021-02-06 14:54:05.641740
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.676846  0.844124  1.292177      0.526741  0.000500
1    0.435505  0.946681  0.849375      0.754793  0.000500
2    0.381605  0.950626  0.553760      0.920283  0.000500
3    0.350140  0.951850  0.399689      0.948537  0.000500
4    0.323913  0.953890  0.375677      0.966700  0.000500
5    0.307390  0.953074  0.331243      0.944501  0.000500
6    0.287657  0.955522  0.314303      0.942482  0.000500
7    0.268565  0.956882  0.293877      0.957619  0.000500
8    0.251121  0.957155  0.306761      0.946519  0.000500
9    0.244435  0.958515  0.303661      0.963673  0.000500
10   0.230827  0.957563  0.263466      0.960646  0.000500
11   0.219823  0.961779  0.224746      0.968718  0.000500
12   0.207184  0.962459  0.251437      0.951564  0.000500
13   0.200713  0.962867  0.242243      0.962664  0.000500
14   0.193615  0.963139  0.212251      0.967709  0.000500
15   0.192785  0.960147  0.195653      0.961655  0.000500
16   0.179572  0.963547  0.251583      0.940464  0.000500
17   0.173078  0.962459  0.221842      0.960646  0.000500
18   0.170406  0.962323  0.198445      0.960646  0.000500
19   0.159767  0.966268  0.247147      0.948537  0.000500
20   0.160780  0.967220  0.201064      0.965691  0.000500
21   0.153387  0.965588  0.209783      0.948537  0.000500
22   0.150668  0.967900  0.200958      0.948537  0.000500
23   0.145258  0.967220  0.206068      0.951564  0.000500
24   0.139939  0.966404  0.175995      0.955600  0.000500
25   0.133291  0.969804  0.177194      0.958628  0.000500
26   0.133011  0.967492  0.224206      0.933401  0.000500
27   0.133658  0.970076  0.175135      0.958628  0.000500
28   0.122958  0.971572  0.205184      0.949546  0.000500
29   0.127358  0.969668  0.189749      0.941473  0.000500
30   0.118122  0.972524  0.202375      0.941473  0.000500
31   0.117723  0.972660  0.204496      0.942482  0.000500
32   0.116166  0.970484  0.182087      0.954591  0.000500
33   0.118246  0.970756  0.184285      0.944501  0.000500
34   0.117244  0.968580  0.156979      0.962664  0.000500
35   0.104475  0.972660  0.191670      0.948537  0.000500
36   0.107123  0.972660  0.167635      0.959637  0.000500
37   0.108232  0.976197  0.155001      0.954591  0.000500
38   0.101195  0.973613  0.200493      0.948537  0.000500
39   0.104552  0.975789  0.184730      0.947528  0.000500
40   0.102265  0.973749  0.187389      0.941473  0.000500
41   0.098153  0.976605  0.201818      0.949546  0.000500
42   0.095040  0.976605  0.328416      0.891019  0.000500
43   0.092201  0.979325  0.213671      0.939455  0.000500
44   0.093942  0.977557  0.164489      0.951564  0.000500
45   0.090309  0.977285  0.257689      0.908174  0.000500
46   0.088992  0.976877  0.234033      0.937437  0.000500
47   0.086974  0.979189  0.217384      0.939455  0.000500
48   0.089669  0.974701  0.220302      0.934410  0.000500
49   0.085624  0.978917  0.247981      0.925328  0.000500
50   0.085395  0.977965  0.219644      0.935419  0.000500
51   0.073705  0.983542  0.206583      0.944501  0.000500
52   0.083201  0.979733  0.202660      0.940464  0.000500
53   0.078059  0.981502  0.221225      0.935419  0.000500
54   0.077781  0.981910  0.185329      0.942482  0.000500
55   0.074767  0.983134  0.200672      0.941473  0.000500
56   0.076816  0.983678  0.200653      0.944501  0.000500
57   0.082415  0.980550  0.219023      0.933401  0.000500
58   0.068494  0.985310  0.203843      0.946519  0.000500
59   0.066717  0.985038  0.204227      0.943491  0.000500
60   0.066814  0.985990  0.208575      0.939455  0.000500
61   0.065636  0.985038  0.385021      0.868819  0.000500
62   0.078703  0.979597  0.209898      0.941473  0.000500
63   0.068394  0.984630  0.200347      0.945510  0.000500
64   0.066813  0.984902  0.190283      0.950555  0.000500
65   0.064340  0.985854  0.194008      0.947528  0.000500
66   0.066845  0.984222  0.236414      0.925328  0.000500
67   0.073212  0.981638  0.229238      0.935419  0.000500
68   0.072192  0.982590  0.183083      0.950555  0.000500
69   0.060941  0.986534  0.220458      0.938446  0.000500
70   0.059875  0.987350  0.230989      0.929364  0.000500
71   0.068163  0.984358  0.229959      0.935419  0.000500
72   0.061166  0.987350  0.196990      0.940464  0.000500
73   0.061784  0.985174  0.226904      0.934410  0.000500
74   0.062163  0.985174  0.218503      0.933401  0.000500
75   0.068717  0.983270  0.214415      0.933401  0.000500
76   0.060341  0.984902  0.192291      0.946519  0.000500
77   0.063100  0.985310  0.193195      0.945510  0.000500
78   0.062348  0.985718  0.166145      0.953582  0.000500
79   0.054125  0.989255  0.151524      0.959637  0.000500
80   0.051603  0.988303  0.185477      0.948537  0.000500
81   0.051839  0.989119  0.214687      0.938446  0.000500
82   0.070491  0.981774  0.201243      0.944501  0.000500
83   0.057859  0.987758  0.172264      0.949546  0.000500
84   0.048927  0.990615  0.198580      0.939455  0.000500
85   0.051694  0.989119  0.249043      0.935419  0.000500
86   0.051697  0.989527  0.190741      0.949546  0.000500
87   0.050604  0.989119  0.217648      0.941473  0.000500
88   0.053423  0.989255  0.205941      0.940464  0.000500
89   0.043304  0.992655  0.204874      0.943491  0.000500
90   0.043993  0.992655  0.232310      0.930373  0.000500
91   0.042867  0.993199  0.210067      0.943491  0.000500
92   0.043876  0.991567  0.244015      0.922301  0.000500
93   0.052573  0.989527  0.228779      0.934410  0.000500
94   0.049885  0.990479  0.216814      0.943491  0.000500
95   0.053353  0.988711  0.217517      0.934410  0.000500
96   0.048705  0.991159  0.207193      0.946519  0.000500
97   0.050324  0.990751  0.189381      0.944501  0.000500
98   0.050066  0.991295  0.190140      0.945510  0.000500
99   0.041916  0.993063  0.173020      0.951564  0.000500
100  0.046090  0.991431  0.243665      0.937437  0.000500
101  0.052370  0.987486  0.245468      0.936428  0.000500
102  0.044602  0.992927  0.248238      0.934410  0.000500
103  0.043944  0.992247  0.240533      0.941473  0.000500
104  0.043975  0.991431  0.246042      0.939455  0.000500
105  0.037867  0.994423  0.210589      0.951564  0.000500
106  0.037657  0.995103  0.200915      0.948537  0.000500
107  0.051139  0.991159  0.381692      0.898083  0.000500
108  0.048154  0.991023  0.416886      0.873865  0.000500
109  0.041465  0.993743  0.188722      0.940464  0.000500
110  0.045345  0.992111  0.222045      0.939455  0.000500
111  0.046801  0.991159  0.162009      0.958628  0.000500
112  0.037692  0.995375  0.239816      0.941473  0.000500
113  0.042971  0.992111  0.232325      0.936428  0.000500
114  0.044728  0.992111  0.189661      0.950555  0.000500
115  0.046060  0.991703  0.241051      0.928355  0.000500
116  0.047576  0.991023  0.219542      0.942482  0.000500
117  0.036601  0.995375  0.224379      0.941473  0.000400
118  0.035433  0.996328  0.259873      0.945510  0.000400
119  0.036333  0.994831  0.212775      0.948537  0.000400
120  0.030706  0.997688  0.227235      0.950555  0.000400
121  0.037947  0.995239  0.245532      0.938446  0.000400
122  0.029985  0.997280  0.236348      0.944501  0.000400
123  0.035640  0.994831  0.275082      0.940464  0.000400
124  0.032140  0.996328  0.224618      0.946519  0.000400
125  0.035818  0.995103  0.265148      0.943491  0.000400
126  0.033841  0.995375  0.229363      0.950555  0.000400
127  0.035721  0.994423  0.266214      0.931382  0.000400
128  0.030538  0.997144  0.214553      0.954591  0.000400
129  0.028108  0.997960  0.231473      0.951564  0.000400
130  0.029155  0.996872  0.207994      0.953582  0.000400
131  0.032476  0.995103  0.228612      0.950555  0.000400
132  0.033836  0.994559  0.263308      0.946519  0.000400
133  0.036562  0.995103  0.225235      0.949546  0.000400
134  0.037238  0.994967  0.239945      0.949546  0.000400
135  0.031912  0.996328  0.271461      0.944501  0.000400
136  0.031110  0.995647  0.262002      0.935419  0.000400
137  0.031204  0.995239  0.209014      0.954591  0.000400
138  0.029289  0.996328  0.244427      0.947528  0.000400
139  0.033600  0.995647  0.271919      0.940464  0.000400
140  0.029785  0.997008  0.236908      0.946519  0.000320
141  0.030656  0.997416  0.243603      0.948537  0.000320
142  0.030045  0.996872  0.181881      0.957619  0.000320
143  0.030662  0.996055  0.232456      0.952573  0.000320
144  0.027084  0.997552  0.223916      0.954591  0.000320
145  0.025871  0.997824  0.202604      0.956609  0.000320
146  0.031028  0.995647  0.241466      0.948537  0.000320
147  0.029492  0.996192  0.246312      0.946519  0.000320
148  0.025054  0.997960  0.230156      0.952573  0.000320
149  0.027017  0.997552  0.242482      0.948537  0.000320
150  0.027488  0.997416  0.268379      0.947528  0.000320
151  0.022996  0.998912  0.265998      0.948537  0.000320
152  0.023376  0.998504  0.274393      0.943491  0.000320
153  0.024526  0.998096  0.253680      0.945510  0.000320
154  0.025016  0.998232  0.249185      0.949546  0.000320
155  0.036349  0.995375  0.257158      0.945510  0.000320
156  0.028060  0.997824  0.258245      0.945510  0.000320
157  0.027073  0.997280  0.242146      0.943491  0.000320
158  0.027497  0.996872  0.312186      0.934410  0.000320
159  0.028311  0.996464  0.251743      0.944501  0.000320
160  0.025593  0.998232  0.228892      0.945510  0.000320
161  0.028600  0.997416  0.258284      0.950555  0.000320
162  0.023707  0.998232  0.277251      0.950555  0.000256
163  0.024761  0.997824  0.268433      0.948537  0.000256
164  0.025711  0.997960  0.261860      0.948537  0.000256
165  0.022402  0.999320  0.268390      0.949546  0.000256
166  0.022873  0.998776  0.241017      0.948537  0.000256
167  0.025721  0.997280  0.271910      0.944501  0.000256
168  0.022649  0.998776  0.249626      0.951564  0.000256
169  0.026876  0.997552  0.245024      0.949546  0.000256
170  0.024562  0.997552  0.232426      0.951564  0.000256
171  0.029132  0.995647  0.213456      0.956609  0.000256
172  0.024259  0.997552  0.248571      0.951564  0.000256
173  0.023845  0.998912  0.255296      0.950555  0.000256
174  0.022005  0.998232  0.231153      0.951564  0.000256
175  0.023295  0.998368  0.258381      0.954591  0.000256
176  0.020258  0.999320  0.284231      0.949546  0.000256
177  0.022820  0.998232  0.290312      0.942482  0.000256
178  0.021023  0.998640  0.261807      0.946519  0.000256
179  0.020551  0.998776  0.279946      0.949546  0.000256

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
reshape (Reshape)            (None, 4, 32, 9)          0         
_________________________________________________________________
batch_normalization_12 (Batc (None, 4, 32, 9)          36        
_________________________________________________________________
time_distributed (TimeDistri (None, 4, 32, 32)         896       
_________________________________________________________________
time_distributed_1 (TimeDist (None, 4, 28, 64)         10304     
_________________________________________________________________
time_distributed_2 (TimeDist (None, 4, 14, 64)         0         
_________________________________________________________________
time_distributed_3 (TimeDist (None, 4, 10, 64)         20544     
_________________________________________________________________
time_distributed_4 (TimeDist (None, 4, 8, 32)          6176      
_________________________________________________________________
time_distributed_5 (TimeDist (None, 4, 4, 32)          0         
_________________________________________________________________
time_distributed_6 (TimeDist (None, 4, 128)            0         
_________________________________________________________________
lstm (LSTM)                  (None, 4, 512)            1312768   
_________________________________________________________________
lstm_1 (LSTM)                (None, 512)               2099200   
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 100)               51300     
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 606       
=================================================================
Total params: 3,501,830
Trainable params: 3,501,812
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 512
N steps: 4
Length: 32
N signals: 9
Learning rate: 0.0005
Cnn depth: 3
Lstm depth: 2
Regularization rate: 9.3e-05


Test Accuracy: 90.64416885375977
Test Loss: 0.42310479283332825



Classification Report
                    precision    recall  f1-score   support

           Walking       0.95      0.98      0.96       335
  Walking_Upstairs       0.98      0.90      0.94       316
Walking_Downstairs       0.93      0.98      0.96       284
           Sitting       0.81      0.81      0.81       324
          Standing       0.83      0.82      0.83       342
            Laying       1.00      1.00      1.00       355

          accuracy                           0.92      1956
         macro avg       0.92      0.92      0.92      1956
      weighted avg       0.92      0.92      0.92      1956



Confusion Matrix
[[329   2   4   0   0   0]
 [ 15 284  17   0   0   0]
 [  4   1 279   0   0   0]
 [  0   3   0 263  57   1]
 [  0   0   0  61 281   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               98.21              0.60                1.19     0.00   
Walking_Upstairs       4.75             89.87                5.38     0.00   
Walking_Downstairs     1.41              0.35               98.24     0.00   
Sitting                0.00              0.93                0.00    81.17   
Standing               0.00              0.00                0.00    17.84   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                17.59    0.31  
Standing               82.16    0.00  
Laying                  0.00  100.00  


Finished working on: CNN_LSTM at: 2021-02-06 14:54:07.997358 -> 53.36317753791809

vLSTM Model : 2021-02-06 14:59:12.775823
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.990016  0.618063  2.064240      0.439960  0.000500
1    0.386484  0.856094  1.598900      0.584258  0.000500
2    0.209108  0.921926  0.269802      0.864783  0.000500
3    0.172948  0.933079  0.280821      0.905146  0.000500
4    0.163753  0.937296  0.147378      0.948537  0.000500
5    0.150836  0.941921  0.164597      0.946519  0.000500
6    0.146753  0.942465  0.106510      0.964682  0.000500
7    0.145367  0.941240  0.110003      0.940464  0.000500
8    0.133448  0.947089  0.123728      0.959637  0.000500
9    0.126302  0.948721  0.136971      0.959637  0.000500
10   0.185389  0.928727  0.246878      0.922301  0.000500
11   0.159282  0.936208  0.119104      0.954591  0.000500
12   0.128437  0.945729  0.093880      0.966700  0.000500
13   0.153734  0.940424  0.152406      0.912210  0.000500
14   0.137576  0.943825  0.087652      0.974773  0.000500
15   0.121539  0.949674  0.096375      0.966700  0.000500
16   0.125003  0.949401  0.114105      0.935419  0.000500
17   0.122725  0.946817  0.102872      0.969728  0.000500
18   0.121281  0.950218  0.097628      0.965691  0.000500
19   0.122106  0.951170  0.144474      0.946519  0.000500
20   0.114105  0.953074  0.117480      0.957619  0.000500
21   0.118663  0.951442  0.109033      0.967709  0.000500
22   0.115002  0.952530  0.124285      0.953582  0.000500
23   0.115193  0.951442  0.093624      0.980827  0.000500
24   0.113359  0.954706  0.212761      0.930373  0.000500
25   0.117432  0.950354  0.112936      0.967709  0.000500
26   0.116365  0.950218  0.141783      0.964682  0.000500
27   0.112820  0.951442  0.138826      0.958628  0.000500
28   0.116791  0.951442  0.130468      0.940464  0.000500
29   0.118475  0.949946  0.103797      0.965691  0.000500
30   0.113611  0.952802  0.080099      0.982846  0.000500
31   0.115075  0.951714  0.085442      0.976791  0.000500
32   0.111619  0.953074  0.082507      0.980827  0.000500
33   0.121655  0.952938  0.082662      0.971746  0.000500
34   0.111272  0.953482  0.095665      0.983855  0.000500
35   0.115391  0.952666  0.083120      0.980827  0.000500
36   0.111736  0.953618  0.094858      0.972755  0.000500
37   0.108450  0.953482  0.109050      0.956609  0.000500
38   0.113208  0.953754  0.095192      0.954591  0.000500
39   0.115317  0.954298  0.089254      0.984864  0.000500
40   0.111991  0.952938  0.095471      0.970737  0.000500
41   0.110026  0.954978  0.088474      0.972755  0.000500
42   0.122717  0.949538  0.105901      0.969728  0.000500
43   0.112415  0.952802  0.101383      0.962664  0.000500
44   0.111763  0.954162  0.099464      0.980827  0.000500
45   0.104091  0.957563  0.125759      0.962664  0.000500
46   0.114491  0.953074  0.109756      0.974773  0.000500
47   0.100202  0.959331  0.131845      0.961655  0.000500
48   0.105162  0.955658  0.104670      0.971746  0.000500
49   0.105477  0.954978  0.101555      0.983855  0.000500
50   0.108981  0.954570  0.105979      0.965691  0.000500
51   0.117226  0.951850  0.141627      0.947528  0.000500
52   0.114772  0.952394  0.135924      0.943491  0.000500
53   0.105668  0.955522  0.108054      0.961655  0.000500
54   0.103071  0.958515  0.105836      0.975782  0.000500
55   0.102008  0.958379  0.109825      0.945510  0.000500
56   0.112119  0.951986  0.120388      0.945510  0.000500
57   0.102167  0.956066  0.110945      0.954591  0.000500
58   0.100677  0.959059  0.212857      0.958628  0.000400
59   0.126348  0.949946  0.088888      0.963673  0.000400
60   0.102917  0.956610  0.082146      0.962664  0.000400
61   0.107123  0.956610  0.076991      0.976791  0.000400
62   0.099741  0.959195  0.074450      0.974773  0.000400
63   0.101772  0.957835  0.080821      0.975782  0.000400
64   0.099765  0.958243  0.088744      0.966700  0.000400
65   0.104338  0.956746  0.080529      0.970737  0.000400
66   0.099003  0.960283  0.065913      0.979818  0.000400
67   0.105427  0.956066  0.097158      0.971746  0.000400
68   0.099949  0.956066  0.082241      0.971746  0.000400
69   0.097579  0.957291  0.106722      0.959637  0.000400
70   0.104420  0.954978  0.082554      0.974773  0.000400
71   0.103336  0.956746  0.085621      0.968718  0.000400
72   0.098153  0.958651  0.092808      0.963673  0.000400
73   0.102499  0.959467  0.071308      0.981837  0.000400
74   0.112256  0.952666  0.076061      0.987891  0.000400
75   0.115938  0.954298  0.110374      0.972755  0.000400
76   0.107651  0.955794  0.111914      0.978809  0.000400
77   0.103430  0.956610  0.089586      0.948537  0.000400
78   0.101432  0.955522  0.078492      0.973764  0.000400
79   0.102233  0.959059  0.082929      0.984864  0.000400
80   0.100206  0.957018  0.089988      0.975782  0.000320
81   0.096058  0.960011  0.098406      0.979818  0.000320
82   0.098368  0.957291  0.089387      0.966700  0.000320
83   0.094965  0.958515  0.084043      0.978809  0.000320
84   0.096188  0.958243  0.093968      0.973764  0.000320
85   0.095419  0.960691  0.096851      0.967709  0.000320
86   0.105603  0.956066  0.100534      0.966700  0.000320
87   0.098644  0.959467  0.106875      0.962664  0.000320
88   0.101015  0.956746  0.093606      0.964682  0.000320
89   0.094464  0.960691  0.105710      0.956609  0.000320
90   0.094559  0.961507  0.097430      0.964682  0.000320
91   0.091318  0.960555  0.078878      0.979818  0.000320
92   0.093937  0.962459  0.071459      0.977800  0.000320
93   0.095505  0.961099  0.055511      0.987891  0.000320
94   0.089804  0.964908  0.068783      0.974773  0.000320
95   0.093921  0.962323  0.074839      0.976791  0.000320
96   0.092688  0.961507  0.074505      0.978809  0.000320
97   0.091215  0.964227  0.077396      0.971746  0.000320
98   0.084974  0.964635  0.121250      0.963673  0.000320
99   0.090880  0.962051  0.087609      0.972755  0.000320
100  0.086297  0.964772  0.144808      0.956609  0.000320
101  0.090906  0.965044  0.085329      0.970737  0.000320
102  0.084794  0.967356  0.092211      0.973764  0.000320
103  0.089272  0.962051  0.087436      0.972755  0.000320
104  0.085242  0.964499  0.093603      0.968718  0.000320
105  0.091402  0.962595  0.076451      0.972755  0.000320
106  0.095818  0.960555  0.065101      0.975782  0.000320
107  0.088821  0.964635  0.077910      0.968718  0.000320
108  0.082669  0.965316  0.071131      0.973764  0.000320
109  0.087433  0.960827  0.062866      0.978809  0.000320
110  0.081092  0.965452  0.082516      0.973764  0.000320
111  0.076497  0.967900  0.075775      0.978809  0.000320
112  0.091049  0.966540  0.068480      0.977800  0.000320
113  0.078435  0.966132  0.084093      0.971746  0.000320
114  0.088602  0.965316  0.083063      0.969728  0.000320
115  0.084088  0.964635  0.068620      0.977800  0.000320
116  0.083143  0.967492  0.081249      0.970737  0.000320
117  0.090619  0.966676  0.071223      0.973764  0.000320
118  0.077263  0.970620  0.061181      0.975782  0.000320
119  0.078448  0.967220  0.086807      0.972755  0.000320
120  0.078408  0.967084  0.082151      0.971746  0.000320
121  0.077609  0.966540  0.084164      0.972755  0.000320
122  0.074497  0.969124  0.076832      0.973764  0.000256
123  0.071836  0.969940  0.063927      0.977800  0.000256
124  0.072602  0.969532  0.097464      0.971746  0.000256
125  0.076634  0.969124  0.058419      0.977800  0.000256
126  0.072276  0.971436  0.065166      0.981837  0.000256
127  0.072241  0.970756  0.093586      0.971746  0.000256
128  0.075117  0.970484  0.092624      0.968718  0.000256
129  0.074718  0.970484  0.073643      0.978809  0.000256
130  0.070819  0.970892  0.100126      0.970737  0.000256
131  0.066928  0.971708  0.074552      0.978809  0.000256
132  0.071054  0.969940  0.077796      0.973764  0.000256
133  0.063840  0.972660  0.071483      0.977800  0.000256
134  0.071703  0.971028  0.067933      0.974773  0.000256
135  0.068244  0.968716  0.108853      0.966700  0.000256
136  0.070342  0.969804  0.109789      0.968718  0.000256
137  0.069164  0.973069  0.081329      0.977800  0.000256
138  0.073028  0.970756  0.063837      0.982846  0.000256
139  0.066770  0.972252  0.073775      0.975782  0.000256
140  0.067873  0.973341  0.139062      0.956609  0.000256
141  0.064156  0.972524  0.095347      0.976791  0.000256
142  0.069015  0.969804  0.086991      0.971746  0.000256
143  0.061242  0.974565  0.129447      0.967709  0.000256
144  0.076205  0.966404  0.114196      0.970737  0.000256
145  0.068477  0.971572  0.108721      0.964682  0.000256
146  0.063360  0.972796  0.163354      0.964682  0.000256
147  0.075254  0.968988  0.116991      0.970737  0.000256
148  0.065427  0.972660  0.129773      0.968718  0.000256
149  0.061211  0.974701  0.130590      0.957619  0.000256
150  0.068175  0.970892  0.134449      0.964682  0.000256
151  0.062529  0.974565  0.138295      0.968718  0.000256
152  0.061170  0.972252  0.152097      0.967709  0.000256
153  0.061198  0.974837  0.105940      0.971746  0.000256
154  0.062592  0.973885  0.126553      0.973764  0.000205
155  0.063486  0.973613  0.119669      0.971746  0.000205
156  0.055073  0.977421  0.120672      0.972755  0.000205
157  0.057957  0.974565  0.107623      0.972755  0.000205
158  0.060222  0.976197  0.123627      0.970737  0.000205
159  0.052110  0.977965  0.125706      0.963673  0.000205
160  0.054153  0.977693  0.106503      0.977800  0.000205
161  0.051045  0.978645  0.125088      0.969728  0.000205
162  0.054906  0.977965  0.130623      0.968718  0.000205
163  0.050836  0.978509  0.127190      0.969728  0.000205
164  0.054820  0.976333  0.133795      0.975782  0.000205
165  0.050563  0.977013  0.138885      0.961655  0.000205
166  0.053215  0.979733  0.103666      0.970737  0.000205
167  0.051571  0.977557  0.108432      0.967709  0.000205
168  0.047219  0.980550  0.092811      0.973764  0.000205
169  0.050924  0.978373  0.129376      0.969728  0.000205
170  0.052277  0.978509  0.098182      0.970737  0.000205
171  0.059190  0.976197  0.106637      0.969728  0.000205
172  0.048986  0.980005  0.117392      0.967709  0.000205
173  0.049198  0.978917  0.117447      0.971746  0.000205
174  0.051153  0.978645  0.121257      0.967709  0.000205
175  0.055909  0.977693  0.122883      0.974773  0.000205
176  0.050182  0.979869  0.133582      0.968718  0.000205
177  0.062179  0.973885  0.129156      0.958628  0.000205
178  0.048440  0.980822  0.119292      0.978809  0.000205
179  0.047100  0.979189  0.171002      0.966700  0.000164
180  0.046400  0.981230  0.139020      0.967709  0.000164
181  0.045045  0.982726  0.108339      0.974773  0.000164
182  0.042214  0.982862  0.134330      0.962664  0.000164
183  0.043141  0.981094  0.121151      0.967709  0.000164
184  0.042027  0.983814  0.128343      0.965691  0.000164
185  0.047051  0.981910  0.143713      0.963673  0.000164
186  0.043971  0.983406  0.108824      0.974773  0.000164
187  0.042823  0.983270  0.110420      0.969728  0.000164
188  0.043068  0.983134  0.112378      0.970737  0.000164
189  0.043559  0.983134  0.130045      0.970737  0.000164
190  0.039070  0.984494  0.119195      0.971746  0.000164
191  0.041786  0.982862  0.143605      0.961655  0.000164
192  0.034940  0.985718  0.141026      0.959637  0.000164
193  0.036856  0.983814  0.128361      0.966700  0.000164

Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_13 (Batc (None, 128, 9)            36        
_________________________________________________________________
lstm_2 (LSTM)                (None, 128)               70656     
_________________________________________________________________
dropout_1 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_5 (Dense)              (None, 100)               12900     
_________________________________________________________________
dense_6 (Dense)              (None, 6)                 606       
=================================================================
Total params: 84,198
Trainable params: 84,180
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05


Test Accuracy: 91.20654463768005
Test Loss: 0.3240664601325989



Classification Report
                    precision    recall  f1-score   support

           Walking       0.98      0.93      0.95       335
  Walking_Upstairs       0.94      0.95      0.94       316
Walking_Downstairs       0.90      0.96      0.93       284
           Sitting       0.84      0.78      0.81       324
          Standing       0.82      0.86      0.84       342
            Laying       0.99      1.00      0.99       355

          accuracy                           0.91      1956
         macro avg       0.91      0.91      0.91      1956
      weighted avg       0.91      0.91      0.91      1956



Confusion Matrix
[[311   6  18   0   0   0]
 [  3 299  12   1   1   0]
 [  4   8 272   0   0   0]
 [  0   4   0 252  63   5]
 [  0   0   0  47 295   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               92.84              1.79                5.37     0.00   
Walking_Upstairs       0.95             94.62                3.80     0.32   
Walking_Downstairs     1.41              2.82               95.77     0.00   
Sitting                0.00              1.23                0.00    77.78   
Standing               0.00              0.00                0.00    13.74   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.32    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                19.44    1.54  
Standing               86.26    0.00  
Laying                  0.00  100.00  


Finished working on: vLSTM at: 2021-02-06 14:59:14.684869 -> 306.68751192092896


BiLSTM Model : 2021-02-06 15:32:57.069844
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.818891  0.717764  2.659133      0.472250  0.000500
1    0.242069  0.913901  1.577326      0.634712  0.000500
2    0.167869  0.937296  0.284591      0.923310  0.000500
3    0.154805  0.942873  0.142463      0.963673  0.000500
4    0.149838  0.948449  0.159717      0.937437  0.000500
..        ...       ...       ...           ...       ...
290  0.010199  0.997144  0.060821      0.985873  0.000164
291  0.013700  0.996328  0.065437      0.981837  0.000164
292  0.011735  0.996736  0.060183      0.985873  0.000164
293  0.012103  0.996872  0.068191      0.981837  0.000164
294  0.011979  0.996600  0.064348      0.985873  0.000164

[295 rows x 5 columns]

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization (BatchNo (None, 128, 9)            36        
_________________________________________________________________
bidirectional (Bidirectional (None, 256)               141312    
_________________________________________________________________
dense (Dense)                (None, 100)               25700     
_________________________________________________________________
dense_1 (Dense)              (None, 6)                 606       
=================================================================
Total params: 167,654
Trainable params: 167,636
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 4
Merge mode: concat


Test Accuracy: 93.5071587562561
Test Loss: 0.2963520288467407



Classification Report
                    precision    recall  f1-score   support

           Walking       0.99      0.93      0.96       335
  Walking_Upstairs       0.87      0.99      0.93       316
Walking_Downstairs       0.96      0.93      0.94       284
           Sitting       0.92      0.84      0.88       324
          Standing       0.88      0.93      0.90       342
            Laying       1.00      1.00      1.00       355

          accuracy                           0.94      1956
         macro avg       0.94      0.93      0.93      1956
      weighted avg       0.94      0.94      0.94      1956



Confusion Matrix
[[310  16   9   0   0   0]
 [  1 312   3   0   0   0]
 [  1  20 263   0   0   0]
 [  0   9   0 272  43   0]
 [  0   0   0  24 318   0]
 [  0   1   0   0   0 354]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               92.54              4.78                2.69     0.00   
Walking_Upstairs       0.32             98.73                0.95     0.00   
Walking_Downstairs     0.35              7.04               92.61     0.00   
Sitting                0.00              2.78                0.00    83.95   
Standing               0.00              0.00                0.00     7.02   
Laying                 0.00              0.28                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                13.27    0.00  
Standing               92.98    0.00  
Laying                  0.00   99.72  


Finished working on: BiLSTM at: 2021-02-06 15:32:59.624947 -> 726.9333193302155

Accuracy comparison 
                       0
iSPLInception  95.092022
CNN            90.644169
CNN-LSTM       90.64416885375977
Test Loss: 0.4231047928333282590.644169
vLSTM          91.206545
sLSTM          91.206545
BiLSTM         93.507159

Loss comparison 
                      0
iSPLInception  0.176140
CNN            0.423105
CNN-LSTM       90.644169
vLSTM          91.206545
sLSTM          91.206545
BiLSTM         0.296352

