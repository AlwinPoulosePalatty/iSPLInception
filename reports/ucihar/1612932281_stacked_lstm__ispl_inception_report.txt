This is the report for the ucihar Dataset

Data Distribution: 

Train:  X -> (7352, 128, 9) Class count -> [1226, 1073, 986, 1286, 1374, 1407] 

                    frequency
Walking             16.675735
Walking_Upstairs    14.594668
Walking_Downstairs  13.411316
Sitting             17.491838
Standing            18.688791
Laying              19.137650

Validation:  X -> (991, 128, 9) Class count -> [161, 155, 136, 167, 190, 182] 

                    frequency
Walking             16.246216
Walking_Upstairs    15.640767
Walking_Downstairs  13.723512
Sitting             16.851665
Standing            19.172554
Laying              18.365288

Test:  X -> (1956, 128, 9) Class count -> [335, 316, 284, 324, 342, 355] 

                    frequency
Walking             17.126789
Walking_Upstairs    16.155418
Walking_Downstairs  14.519427
Sitting             16.564417
Standing            17.484661
Laying              18.149284

sLSTM Model : 2021-02-10 13:49:33.710489
Model History 
         loss  accuracy  val_loss  val_accuracy      lr
0    0.691733  0.737486  2.450732      0.416751  0.0005
1    0.227371  0.924102  0.964488      0.768920  0.0005
2    0.203553  0.931719  0.332914      0.901110  0.0005
3    0.197402  0.933079  0.126010      0.965691  0.0005
4    0.148856  0.944369  0.140742      0.941473  0.0005
5    0.145953  0.946273  0.118264      0.957619  0.0005
6    0.160518  0.941513  0.132639      0.961655  0.0005
7    0.136752  0.949401  0.120051      0.968718  0.0005
8    0.130754  0.952258  0.107638      0.974773  0.0005
9    0.127189  0.951850  0.107680      0.970737  0.0005
10   0.130381  0.952666  0.116822      0.950555  0.0005
11   0.133479  0.948041  0.108373      0.967709  0.0005
12   0.130459  0.949538  0.109654      0.972755  0.0005
13   0.132282  0.949674  0.105063      0.974773  0.0005
14   0.135067  0.950762  0.117254      0.939455  0.0005
15   0.135090  0.947361  0.110392      0.962664  0.0005
16   0.128235  0.953346  0.096216      0.976791  0.0005
17   0.122052  0.952122  0.101244      0.967709  0.0005
18   0.140469  0.946545  0.130680      0.959637  0.0005
19   0.132186  0.951170  0.104505      0.975782  0.0005
20   0.128085  0.949538  0.093429      0.980827  0.0005
21   0.130114  0.950898  0.110998      0.979818  0.0005
22   0.122691  0.953482  0.103646      0.975782  0.0005
23   0.124807  0.951850  0.115344      0.949546  0.0005
24   0.124593  0.952394  0.105849      0.969728  0.0005
25   0.130718  0.953618  0.097368      0.962664  0.0005
26   0.121017  0.953890  0.094599      0.973764  0.0005
27   0.119441  0.953618  0.097223      0.975782  0.0005
28   0.125356  0.951034  0.094010      0.975782  0.0005
29   0.120145  0.954434  0.106222      0.950555  0.0005
30   0.144439  0.945321  0.130832      0.940464  0.0005
31   0.119505  0.953346  0.094390      0.971746  0.0005
32   0.123452  0.952802  0.105549      0.980827  0.0005
33   0.118688  0.954842  0.145944      0.927346  0.0005
34   0.124344  0.952938  0.089957      0.981837  0.0005
35   0.118318  0.953890  0.095390      0.976791  0.0005
36   0.113698  0.956066  0.086326      0.979818  0.0005
37   0.110133  0.957291  0.141212      0.953582  0.0005
38   0.117928  0.954706  0.091826      0.977800  0.0005
39   0.114557  0.953754  0.099223      0.971746  0.0005
40   0.114538  0.956882  0.108167      0.980827  0.0005
41   0.121447  0.951442  0.105437      0.956609  0.0005
42   0.117282  0.954706  0.098148      0.963673  0.0005
43   0.109414  0.956746  0.099247      0.976791  0.0005
44   0.116950  0.956066  0.088731      0.964682  0.0005
45   0.115962  0.956338  0.088616      0.969728  0.0005
46   0.113458  0.953890  0.099067      0.957619  0.0005
47   0.109785  0.957018  0.083476      0.983855  0.0005
48   0.120267  0.953074  0.091324      0.970737  0.0005
49   0.116925  0.955522  0.095833      0.974773  0.0005
50   0.104953  0.958923  0.085982      0.974773  0.0005
51   0.106048  0.958787  0.088807      0.975782  0.0005
52   0.107103  0.958379  0.094692      0.965691  0.0005
53   0.112808  0.956746  0.098513      0.954591  0.0005
54   0.107467  0.957291  0.095757      0.960646  0.0005
55   0.108178  0.957971  0.074816      0.976791  0.0005
56   0.111194  0.958379  0.077238      0.979818  0.0005
57   0.103437  0.960011  0.101885      0.949546  0.0005
58   0.106297  0.958651  0.094815      0.978809  0.0005
59   0.104503  0.956338  0.100962      0.958628  0.0005
60   0.104928  0.957563  0.111514      0.948537  0.0005
61   0.107352  0.960419  0.097843      0.969728  0.0005
62   0.104844  0.960011  0.090545      0.963673  0.0005
63   0.108338  0.957699  0.077186      0.978809  0.0005
64   0.114511  0.954706  0.204059      0.894046  0.0005
65   0.118655  0.952666  0.106117      0.960646  0.0005
66   0.097459  0.963683  0.083602      0.973764  0.0005
67   0.178735  0.945321  0.111181      0.958628  0.0005
68   0.108736  0.959331  0.096640      0.970737  0.0005
69   0.110202  0.956338  0.088130      0.976791  0.0005
70   0.109963  0.957427  0.097330      0.964682  0.0005
71   0.111598  0.956066  0.100723      0.964682  0.0005
72   0.105294  0.960011  0.086649      0.979818  0.0005
73   0.103173  0.959059  0.099667      0.959637  0.0005
74   0.095755  0.963275  0.100682      0.965691  0.0005
75   0.107146  0.959739  0.112060      0.947528  0.0005
76   0.107101  0.957971  0.118917      0.972755  0.0005
77   0.100650  0.960147  0.087768      0.973764  0.0005
78   0.103420  0.958923  0.088820      0.972755  0.0005
79   0.095844  0.961371  0.092200      0.969728  0.0005
80   0.101883  0.959739  0.090420      0.972755  0.0005
81   0.100351  0.960555  0.098734      0.953582  0.0005
82   0.092857  0.963547  0.082024      0.976791  0.0005
83   0.095055  0.964499  0.090437      0.970737  0.0005
84   0.097473  0.963411  0.089990      0.970737  0.0005
85   0.094614  0.963139  0.084563      0.966700  0.0005
86   0.098560  0.961099  0.094110      0.962664  0.0005
87   0.091141  0.965860  0.087624      0.969728  0.0005
88   0.090004  0.966540  0.097272      0.967709  0.0005
89   0.094253  0.964772  0.087202      0.972755  0.0005
90   0.089812  0.964227  0.136156      0.946519  0.0005
91   0.093159  0.963275  0.085119      0.974773  0.0005
92   0.090653  0.963819  0.101540      0.969728  0.0005
93   0.119864  0.956066  0.128055      0.951564  0.0005
94   0.118853  0.957018  0.116262      0.958628  0.0005
95   0.115620  0.954570  0.108860      0.970737  0.0005
96   0.101352  0.963275  0.082201      0.970737  0.0005
97   0.098150  0.963547  0.095351      0.969728  0.0005
98   0.098400  0.962731  0.122343      0.947528  0.0005
99   0.115710  0.952530  0.122030      0.947528  0.0005
100  0.114953  0.954434  0.099692      0.963673  0.0005
101  0.107483  0.957291  0.108948      0.961655  0.0004
102  0.100270  0.960283  0.079141      0.974773  0.0004
103  0.097981  0.963003  0.089662      0.965691  0.0004
104  0.119334  0.954570  0.087517      0.972755  0.0004
105  0.107479  0.958651  0.086947      0.973764  0.0004

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization (BatchNo (None, 128, 9)            36        
_________________________________________________________________
preprocess (Dense)           (None, 128, 100)          1000      
_________________________________________________________________
lstm (LSTM)                  (None, 128, 128)          117248    
_________________________________________________________________
dropout (Dropout)            (None, 128, 128)          0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense (Dense)                (None, 100)               12900     
_________________________________________________________________
output (Dense)               (None, 6)                 606       
=================================================================
Total params: 263,374
Trainable params: 263,356
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 4


Test Accuracy: 92.12678670883179
Test Loss: 0.30004608631134033



Classification Report
                    precision    recall  f1-score   support

           Walking       0.97      0.99      0.98       335
  Walking_Upstairs       0.99      0.93      0.96       316
Walking_Downstairs       0.96      1.00      0.98       284
           Sitting       0.83      0.76      0.79       324
          Standing       0.81      0.85      0.83       342
            Laying       0.98      1.00      0.99       355

          accuracy                           0.92      1956
         macro avg       0.92      0.92      0.92      1956
      weighted avg       0.92      0.92      0.92      1956



Confusion Matrix
[[333   0   2   0   0   0]
 [ 11 295  10   0   0   0]
 [  0   1 283   0   0   0]
 [  0   3   0 245  70   6]
 [  0   0   0  51 291   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               99.40              0.00                0.60     0.00   
Walking_Upstairs       3.48             93.35                3.16     0.00   
Walking_Downstairs     0.00              0.35               99.65     0.00   
Sitting                0.00              0.93                0.00    75.62   
Standing               0.00              0.00                0.00    14.91   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.00    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                21.60    1.85  
Standing               85.09    0.00  
Laying                  0.00  100.00  


Finished working on: sLSTM at: 2021-02-10 13:49:36.384364 -> 295.1211824417114

iSPLInception Model : 2021-02-10 14:25:09.618004
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.333508  0.926959  5.190848      0.479314  0.000500
1    2.796373  0.947225  3.158810      0.530777  0.000500
2    1.675433  0.945049  2.254950      0.643794  0.000500
3    1.159844  0.950354  1.439319      0.863774  0.000500
4    0.868412  0.950762  0.916948      0.972755  0.000500
5    0.659909  0.951170  0.915738      0.865792  0.000500
6    0.522935  0.952530  2.641055      0.577195  0.000500
7    0.467932  0.954026  0.674341      0.934410  0.000500
8    0.380950  0.955794  1.430311      0.740666  0.000500
9    0.340919  0.955522  0.449457      0.947528  0.000500
10   0.273997  0.958379  0.588685      0.849647  0.000500
11   0.259522  0.953482  0.383666      0.947528  0.000500
12   0.238355  0.957291  0.519234      0.917255  0.000500
13   0.216078  0.958651  0.427605      0.875883  0.000500
14   0.207020  0.957427  0.669350      0.817356  0.000500
15   0.223366  0.955522  0.317447      0.949546  0.000500
16   0.196470  0.954434  0.277598      0.942482  0.000500
17   0.202193  0.954434  0.378393      0.905146  0.000500
18   0.181074  0.960011  0.342466      0.883956  0.000500
19   0.167526  0.959331  0.444037      0.880928  0.000500
20   0.158833  0.959739  0.282227      0.959637  0.000500
21   0.163410  0.958515  0.654332      0.825429  0.000500
22   0.179811  0.959603  0.395732      0.906155  0.000500
23   0.165055  0.961779  0.231113      0.971746  0.000500
24   0.161568  0.958651  0.689802      0.715439  0.000500
25   0.150567  0.961235  0.193392      0.960646  0.000500
26   0.157498  0.958379  0.259344      0.919273  0.000500
27   0.162699  0.960283  0.168588      0.968718  0.000500
28   0.138689  0.962731  0.354027      0.909183  0.000500
29   0.156875  0.957291  0.722989      0.849647  0.000500
30   0.140857  0.960283  0.239695      0.947528  0.000500
31   0.129038  0.962867  0.299729      0.911201  0.000500
32   0.134026  0.964091  0.231191      0.947528  0.000500
33   0.131312  0.960691  0.267195      0.907164  0.000500
34   0.140791  0.962051  0.225283      0.932392  0.000500
35   0.128111  0.962731  0.250711      0.949546  0.000500
36   0.148829  0.962323  0.768682      0.855701  0.000500
37   0.156632  0.960011  0.379219      0.901110  0.000500
38   0.134037  0.963275  0.310583      0.919273  0.000500
39   0.126303  0.963003  0.290796      0.932392  0.000500
40   0.152563  0.961779  0.540144      0.797175  0.000500
41   0.146584  0.963819  0.218361      0.953582  0.000500
42   0.131927  0.963955  0.539083      0.915237  0.000500
43   0.156023  0.962187  0.162777      0.965691  0.000500
44   0.127358  0.965044  0.343365      0.893037  0.000500
45   0.114995  0.967220  0.303878      0.928355  0.000500
46   0.121093  0.962187  0.198866      0.960646  0.000500
47   0.122745  0.963003  0.156627      0.963673  0.000500
48   0.122057  0.964908  0.252072      0.923310  0.000500
49   0.123445  0.967084  0.252080      0.917255  0.000500
50   0.139992  0.960691  0.155104      0.965691  0.000500
51   0.120935  0.965180  0.249605      0.938446  0.000500
52   0.128890  0.963955  0.605320      0.841574  0.000500
53   0.132315  0.966132  0.202624      0.959637  0.000500
54   0.122154  0.968852  0.272560      0.907164  0.000500
55   0.122112  0.963411  0.149843      0.979818  0.000500
56   0.108755  0.967764  0.198596      0.953582  0.000400
57   0.109052  0.966812  0.144196      0.965691  0.000400
58   0.103473  0.967628  0.187695      0.944501  0.000400
59   0.111980  0.967220  0.158379      0.963673  0.000400
60   0.110592  0.968580  0.688730      0.810293  0.000400
61   0.109913  0.969260  0.137289      0.964682  0.000400
62   0.108159  0.969396  0.216219      0.934410  0.000400
63   0.102527  0.971028  0.206679      0.941473  0.000400
64   0.103515  0.969260  0.168981      0.951564  0.000400
65   0.109936  0.967356  0.388396      0.924319  0.000400
66   0.104393  0.970756  0.143123      0.963673  0.000400
67   0.103800  0.969668  0.143918      0.966700  0.000400
68   0.103068  0.970620  0.147282      0.964682  0.000400
69   0.097886  0.971708  0.209105      0.959637  0.000400
70   0.101875  0.969124  0.429118      0.882946  0.000400
71   0.100860  0.971980  0.193087      0.957619  0.000400
72   0.099559  0.973205  0.131912      0.973764  0.000400
73   0.106867  0.970076  0.264745      0.926337  0.000400
74   0.102598  0.971844  0.115039      0.979818  0.000400
75   0.100697  0.972933  0.148221      0.978809  0.000400
76   0.097593  0.974429  0.130462      0.973764  0.000400
77   0.092662  0.975381  0.140318      0.967709  0.000400
78   0.094562  0.972933  0.131916      0.974773  0.000400
79   0.095147  0.976197  0.458530      0.898083  0.000400
80   0.098135  0.974021  0.095198      0.981837  0.000400
81   0.086435  0.977965  0.101312      0.988900  0.000400
82   0.089384  0.975381  0.094781      0.983855  0.000400
83   0.090534  0.976605  0.123162      0.962664  0.000400
84   0.089388  0.976197  0.141622      0.982846  0.000400
85   0.089409  0.976333  0.111423      0.976791  0.000400
86   0.089965  0.977829  2.173072      0.774975  0.000400
87   0.086708  0.981366  0.445308      0.901110  0.000400
88   0.088705  0.977557  0.137180      0.979818  0.000400
89   0.085964  0.979597  0.205007      0.950555  0.000400
90   0.090257  0.979461  0.102932      0.985873  0.000400
91   0.076280  0.981910  0.184575      0.952573  0.000400
92   0.075402  0.983134  0.203450      0.922301  0.000400
93   0.087369  0.980413  0.379540      0.881937  0.000400
94   0.091313  0.977829  0.193898      0.946519  0.000400
95   0.086362  0.981638  0.129765      0.973764  0.000400
96   0.085403  0.979869  0.132609      0.961655  0.000400
97   0.078748  0.982862  0.098709      0.977800  0.000400
98   0.082635  0.980686  0.210340      0.940464  0.000400
99   0.077349  0.983950  0.115680      0.967709  0.000400
100  0.074879  0.984902  0.104533      0.976791  0.000400
101  0.070128  0.984358  0.118499      0.968718  0.000400
102  0.080169  0.980686  0.524786      0.795156  0.000400
103  0.081341  0.983134  0.104387      0.974773  0.000400
104  0.077170  0.983542  0.574512      0.775984  0.000400
105  0.079617  0.984902  0.092828      0.984864  0.000400
106  0.079041  0.982454  0.290740      0.918264  0.000400
107  0.075253  0.982998  0.283252      0.908174  0.000400
108  0.072808  0.983270  0.150283      0.961655  0.000400
109  0.071983  0.984358  0.135671      0.953582  0.000400
110  0.068595  0.985174  0.094126      0.982846  0.000400
111  0.074406  0.983678  0.160994      0.956609  0.000400
112  0.070785  0.985854  0.192647      0.948537  0.000400
113  0.070773  0.981774  0.323549      0.944501  0.000400
114  0.069333  0.986534  0.259796      0.918264  0.000400
115  0.074269  0.984630  0.117562      0.978809  0.000400
116  0.079078  0.982862  0.146140      0.955600  0.000400
117  0.069078  0.986262  0.073976      0.988900  0.000400
118  0.074558  0.984766  0.267205      0.902119  0.000400
119  0.077339  0.982998  0.117409      0.975782  0.000400
120  0.068717  0.985718  0.153499      0.956609  0.000400
121  0.071014  0.983542  0.115149      0.977800  0.000320
122  0.065948  0.986670  0.116032      0.972755  0.000320
123  0.068539  0.985446  0.095237      0.978809  0.000320
124  0.064038  0.987486  0.108712      0.966700  0.000320
125  0.066834  0.985718  0.072230      0.983855  0.000320
126  0.063903  0.987894  0.072763      0.986882  0.000320
127  0.065621  0.985174  0.116184      0.968718  0.000320
128  0.062397  0.988575  0.098481      0.984864  0.000320
129  0.061563  0.986942  0.107205      0.969728  0.000320
130  0.062942  0.987758  0.592777      0.837538  0.000320
131  0.061448  0.988030  0.085072      0.984864  0.000320
132  0.065314  0.985718  0.084439      0.980827  0.000320
133  0.064375  0.986942  0.093256      0.981837  0.000320
134  0.062591  0.985854  0.148749      0.963673  0.000320
135  0.071826  0.985990  0.074124      0.988900  0.000320
136  0.065652  0.985174  0.169595      0.943491  0.000320
137  0.064316  0.987486  0.329248      0.903128  0.000320
138  0.060922  0.988439  0.112954      0.967709  0.000320
139  0.062050  0.986942  0.073432      0.984864  0.000320
140  0.062451  0.987078  0.083201      0.984864  0.000320
141  0.063636  0.986670  0.071611      0.987891  0.000320
142  0.059628  0.988303  0.062077      0.985873  0.000320
143  0.057624  0.989527  0.085623      0.976791  0.000320
144  0.065786  0.985990  0.141384      0.967709  0.000320
145  0.064369  0.986126  0.148409      0.962664  0.000320
146  0.058599  0.988711  0.109019      0.974773  0.000320
147  0.056480  0.989663  0.090623      0.981837  0.000320
148  0.057896  0.988167  0.097775      0.983855  0.000320
149  0.058240  0.986806  0.096386      0.976791  0.000320
150  0.057131  0.988303  0.071121      0.986882  0.000320
151  0.058585  0.988711  0.103508      0.974773  0.000320
152  0.057240  0.986534  0.118442      0.972755  0.000320
153  0.062997  0.986670  0.090173      0.982846  0.000320
154  0.062224  0.987894  0.091268      0.984864  0.000320
155  0.058442  0.987622  0.083088      0.978809  0.000320
156  0.070747  0.987078  0.076250      0.985873  0.000320
157  0.066072  0.986806  0.086430      0.988900  0.000320
158  0.061424  0.988847  0.140208      0.968718  0.000256
159  0.056005  0.989663  0.110078      0.982846  0.000256
160  0.058398  0.988575  0.123103      0.964682  0.000256
161  0.054623  0.989663  0.084863      0.990918  0.000256
162  0.059248  0.985854  0.083361      0.983855  0.000256
163  0.051978  0.990751  0.071427      0.987891  0.000256
164  0.057312  0.988711  0.147494      0.956609  0.000256
165  0.057818  0.989391  0.069289      0.985873  0.000256
166  0.059229  0.987894  0.075703      0.986882  0.000256
167  0.051217  0.990479  0.068954      0.985873  0.000256
168  0.053893  0.989391  0.068976      0.989909  0.000256
169  0.052580  0.989119  0.086195      0.980827  0.000256
170  0.049475  0.990887  0.088700      0.980827  0.000256
171  0.055014  0.989663  0.069604      0.987891  0.000256
172  0.060245  0.985854  0.090653      0.976791  0.000256
173  0.055266  0.989527  0.110263      0.971746  0.000256
174  0.052772  0.991023  0.206625      0.946519  0.000256
175  0.060882  0.987214  0.086021      0.982846  0.000256
176  0.054840  0.988847  0.076795      0.984864  0.000256
177  0.053479  0.990207  0.093494      0.983855  0.000256
178  0.048995  0.991975  0.184984      0.953582  0.000256
179  0.050516  0.990343  0.084904      0.981837  0.000256
180  0.049634  0.990479  0.080596      0.980827  0.000256
181  0.049213  0.990343  0.070637      0.985873  0.000256
182  0.050492  0.989799  0.064226      0.992936  0.000256
183  0.046913  0.992247  0.083112      0.981837  0.000256
184  0.049096  0.990887  0.054777      0.990918  0.000256
185  0.056264  0.989255  0.110634      0.981837  0.000256
186  0.049134  0.991023  0.060617      0.989909  0.000256
187  0.050074  0.991159  0.077449      0.980827  0.000256
188  0.055971  0.988847  0.185603      0.951564  0.000256
189  0.050979  0.991567  0.105474      0.976791  0.000256
190  0.050366  0.991975  0.100717      0.974773  0.000256
191  0.048972  0.991567  0.084712      0.982846  0.000256
192  0.052310  0.990343  0.084366      0.976791  0.000256
193  0.045917  0.992519  0.076635      0.986882  0.000256
194  0.049203  0.990479  0.078604      0.984864  0.000256
195  0.052056  0.989935  0.100015      0.975782  0.000256
196  0.049270  0.991023  0.095291      0.975782  0.000256
197  0.046663  0.991431  0.146398      0.952573  0.000256
198  0.047514  0.993199  0.088973      0.986882  0.000256
199  0.045728  0.991567  0.066192      0.991927  0.000256
200  0.045840  0.992111  0.106469      0.972755  0.000256
201  0.046065  0.992383  0.091991      0.982846  0.000256
202  0.048670  0.990751  0.085923      0.979818  0.000256
203  0.051284  0.990071  0.078787      0.980827  0.000256
204  0.049897  0.991023  0.085620      0.975782  0.000256
205  0.050084  0.989663  0.080369      0.985873  0.000256
206  0.046230  0.992111  0.078061      0.981837  0.000256
207  0.044236  0.992519  0.070022      0.984864  0.000256
208  0.045554  0.991975  0.113875      0.966700  0.000256
209  0.049245  0.992519  0.066097      0.987891  0.000256
210  0.047012  0.991567  0.078989      0.986882  0.000256
211  0.050137  0.990887  0.086800      0.982846  0.000256
212  0.050152  0.990207  0.073128      0.985873  0.000256
213  0.044706  0.992519  0.076200      0.988900  0.000256
214  0.045678  0.991431  0.087779      0.980827  0.000256
215  0.048049  0.992111  0.246873      0.927346  0.000256
216  0.052904  0.990207  0.082428      0.984864  0.000256
217  0.052669  0.987894  0.063649      0.990918  0.000256
218  0.044286  0.993607  0.081123      0.983855  0.000205
219  0.050660  0.989527  0.096217      0.980827  0.000205
220  0.045952  0.991975  0.061457      0.990918  0.000205
221  0.045868  0.991703  0.080559      0.983855  0.000205
222  0.042300  0.993607  0.351994      0.889001  0.000205
223  0.046984  0.992383  0.159148      0.955600  0.000205
224  0.042714  0.993335  0.074577      0.987891  0.000205
225  0.041927  0.992791  0.069576      0.987891  0.000205
226  0.038478  0.995511  0.058297      0.990918  0.000205
227  0.048349  0.990343  0.077594      0.984864  0.000205
228  0.049514  0.990615  0.076175      0.979818  0.000205
229  0.043873  0.992111  0.064169      0.988900  0.000205
230  0.043286  0.992247  0.069398      0.990918  0.000205
231  0.048432  0.990343  0.074357      0.982846  0.000205
232  0.038889  0.995783  0.069560      0.984864  0.000205
233  0.042283  0.992791  0.061667      0.990918  0.000205
234  0.046104  0.991975  0.084157      0.980827  0.000205
235  0.042335  0.993879  0.070553      0.987891  0.000205
236  0.040911  0.992927  0.078901      0.985873  0.000205
237  0.044438  0.991567  0.081413      0.978809  0.000164
238  0.042309  0.993879  0.056543      0.990918  0.000164
239  0.043309  0.992791  0.068263      0.987891  0.000164
240  0.041290  0.993335  0.061477      0.989909  0.000164
241  0.043295  0.991567  0.072642      0.988900  0.000164
242  0.039862  0.992655  0.075738      0.983855  0.000164
243  0.042869  0.991703  0.063677      0.987891  0.000164
244  0.043706  0.993471  0.056614      0.989909  0.000164
245  0.037403  0.994831  0.068521      0.988900  0.000164
246  0.040634  0.993607  0.062094      0.988900  0.000164
247  0.037596  0.993199  0.055069      0.993946  0.000164
248  0.044828  0.991703  0.061743      0.984864  0.000164
249  0.038758  0.993335  0.104781      0.979818  0.000164
250  0.045933  0.993743  0.075294      0.984864  0.000164
251  0.044307  0.992655  0.128780      0.966700  0.000164
252  0.038649  0.995103  0.089326      0.977800  0.000164
253  0.038118  0.995103  0.070804      0.989909  0.000164
254  0.040855  0.994015  0.069061      0.986882  0.000164
255  0.044676  0.991431  0.075917      0.986882  0.000164
256  0.038455  0.995103  0.075536      0.986882  0.000131
257  0.040914  0.992519  0.077416      0.983855  0.000131
258  0.040119  0.994287  0.062601      0.987891  0.000131
259  0.039169  0.994151  0.100710      0.974773  0.000131
260  0.036182  0.994287  0.083509      0.982846  0.000131
261  0.035221  0.995239  0.063038      0.989909  0.000131
262  0.037303  0.994151  0.069974      0.985873  0.000131
263  0.036291  0.994831  0.062176      0.986882  0.000131
264  0.036052  0.994967  0.073817      0.984864  0.000131
265  0.036499  0.994967  0.070078      0.987891  0.000131
266  0.035994  0.995375  0.058027      0.989909  0.000131
267  0.036377  0.995103  0.054595      0.989909  0.000131
268  0.035646  0.995239  0.066293      0.984864  0.000131
269  0.035680  0.995375  0.072640      0.985873  0.000131
270  0.034954  0.994423  0.060751      0.988900  0.000131
271  0.040846  0.991975  0.081755      0.980827  0.000131
272  0.036197  0.994967  0.058957      0.988900  0.000131
273  0.036886  0.994151  0.078120      0.983855  0.000131
274  0.034386  0.994423  0.066340      0.984864  0.000131
275  0.032260  0.995511  0.060289      0.987891  0.000131
276  0.037313  0.994151  0.052791      0.989909  0.000131
277  0.033692  0.995511  0.077266      0.983855  0.000131
278  0.033572  0.995783  0.064194      0.988900  0.000131
279  0.035007  0.995103  0.068963      0.985873  0.000131
280  0.034052  0.994967  0.071456      0.988900  0.000131
281  0.036535  0.994287  0.208105      0.923310  0.000131
282  0.035698  0.994287  0.057503      0.988900  0.000131
283  0.037358  0.993607  0.084435      0.980827  0.000131
284  0.036318  0.993471  0.085006      0.974773  0.000131
285  0.034748  0.995375  0.078463      0.985873  0.000131
286  0.031761  0.996464  0.154798      0.960646  0.000105
287  0.032912  0.995783  0.072110      0.984864  0.000105
288  0.032308  0.996192  0.057418      0.990918  0.000105
289  0.030359  0.996872  0.056796      0.992936  0.000105
290  0.041342  0.994015  0.068981      0.987891  0.000105
291  0.037675  0.995103  0.074956      0.986882  0.000105
292  0.035879  0.994967  0.081055      0.988900  0.000105
293  0.033143  0.996600  0.079681      0.988900  0.000105
294  0.035464  0.993743  0.075982      0.987891  0.000105
295  0.035767  0.994831  0.077014      0.984864  0.000105
296  0.034975  0.995103  0.068409      0.987891  0.000105
297  0.031023  0.996192  0.083284      0.983855  0.000105
298  0.035985  0.994151  0.076102      0.983855  0.000105
299  0.031304  0.996464  0.063550      0.992936  0.000105
300  0.032821  0.995375  0.065315      0.988900  0.000100
301  0.030621  0.996600  0.058914      0.990918  0.000100
302  0.031468  0.995919  0.054330      0.990918  0.000100
303  0.029074  0.996736  0.095074      0.977800  0.000100
304  0.034598  0.994967  0.055544      0.990918  0.000100
305  0.035323  0.994695  0.076012      0.980827  0.000100
306  0.031411  0.995103  0.058942      0.990918  0.000100
307  0.034687  0.994287  0.060229      0.987891  0.000100
308  0.036002  0.995103  0.064293      0.989909  0.000100
309  0.031971  0.995919  0.063650      0.987891  0.000100
310  0.028733  0.997144  0.053325      0.992936  0.000100
311  0.030939  0.995919  0.070280      0.982846  0.000100
312  0.032329  0.996055  0.056737      0.990918  0.000100
313  0.030702  0.996055  0.058964      0.988900  0.000100
314  0.029510  0.996872  0.059400      0.987891  0.000100
315  0.032968  0.995103  0.073504      0.984864  0.000100
316  0.030633  0.995919  0.079750      0.980827  0.000100
317  0.031046  0.995783  0.055051      0.990918  0.000100
318  0.029336  0.996464  0.072038      0.985873  0.000100
319  0.030339  0.995647  0.070102      0.983855  0.000100
320  0.030987  0.995919  0.065570      0.985873  0.000100
321  0.032311  0.995375  0.074150      0.982846  0.000100
322  0.029709  0.995919  0.052012      0.989909  0.000100
323  0.029689  0.995919  0.053668      0.990918  0.000100
324  0.031996  0.994695  0.054053      0.991927  0.000100
325  0.029170  0.996192  0.051257      0.992936  0.000100
326  0.029356  0.996600  0.052174      0.990918  0.000100
327  0.031837  0.994695  0.060196      0.986882  0.000100
328  0.030113  0.996328  0.122648      0.967709  0.000100
329  0.034215  0.994423  0.062155      0.989909  0.000100
330  0.031732  0.995103  0.054882      0.991927  0.000100
331  0.029220  0.995783  0.065724      0.988900  0.000100
332  0.030692  0.996192  0.064456      0.990918  0.000100
333  0.029890  0.996328  0.063845      0.985873  0.000100
334  0.031007  0.995783  0.060506      0.986882  0.000100
335  0.030068  0.995919  0.059598      0.991927  0.000100
336  0.028912  0.996736  0.057996      0.990918  0.000100
337  0.036874  0.993335  0.059956      0.988900  0.000100
338  0.029345  0.996192  0.062383      0.990918  0.000100
339  0.027665  0.997144  0.056526      0.990918  0.000100
340  0.031800  0.995239  0.056153      0.994955  0.000100
341  0.030138  0.995919  0.056594      0.993946  0.000100
342  0.033583  0.994695  0.050736      0.993946  0.000100
343  0.030345  0.995919  0.049700      0.993946  0.000100
344  0.028130  0.997416  0.066379      0.987891  0.000100
345  0.029736  0.995783  0.058789      0.987891  0.000100
346  0.028496  0.996736  0.054582      0.992936  0.000100
347  0.031572  0.995375  0.052055      0.992936  0.000100
348  0.031845  0.995647  0.053019      0.993946  0.000100
349  0.031732  0.994967  0.081422      0.975782  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 128, 9)]     0                                            
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 128, 9)       36          input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 128, 32)      288         batch_normalization_1[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 128, 9)       0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 128, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 128, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 128, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 128, 64)      576         max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 128, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 128, 256)     0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 128, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 128, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 128, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 128, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 128, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 128, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 128, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 128, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 128, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 128, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 128, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 128, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 128, 256)     2304        batch_normalization_1[0][0]      
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 128, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 128, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 128, 256)     0           batch_normalization_5[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 128, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 128, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 128, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 128, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 128, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 128, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 128, 256)     0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 128, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 128, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 128, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 128, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 128, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 128, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 128, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 128, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 128, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 6)            1542        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,327,754
Trainable params: 1,324,664
Non-trainable params: 3,090
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 95.09202241897583
Test Loss: 0.17614002525806427



Classification Report
                    precision    recall  f1-score   support

           Walking       1.00      0.88      0.93       335
  Walking_Upstairs       0.94      0.96      0.95       316
Walking_Downstairs       0.88      1.00      0.94       284
           Sitting       0.97      0.90      0.93       324
          Standing       0.92      0.97      0.95       342
            Laying       1.00      1.00      1.00       355

          accuracy                           0.95      1956
         macro avg       0.95      0.95      0.95      1956
      weighted avg       0.95      0.95      0.95      1956



Confusion Matrix
[[294  17  24   0   0   0]
 [  0 302  13   0   1   0]
 [  0   0 284   0   0   0]
 [  0   3   0 292  28   1]
 [  0   0   0   9 333   0]
 [  0   0   0   0   0 355]]


Normalised Confusion Matrix: True
                    Walking  Walking_Upstairs  Walking_Downstairs  Sitting  \
Walking               87.76              5.07                7.16     0.00   
Walking_Upstairs       0.00             95.57                4.11     0.00   
Walking_Downstairs     0.00              0.00              100.00     0.00   
Sitting                0.00              0.93                0.00    90.12   
Standing               0.00              0.00                0.00     2.63   
Laying                 0.00              0.00                0.00     0.00   

                    Standing  Laying  
Walking                 0.00    0.00  
Walking_Upstairs        0.32    0.00  
Walking_Downstairs      0.00    0.00  
Sitting                 8.64    0.31  
Standing               97.37    0.00  
Laying                  0.00  100.00  


Finished working on: iSPLInception at: 2021-02-10 14:25:12.111688 -> 2135.727323770523

