This is the report for the daphnet Dataset

Data Distribution: 

Train:  X -> (7945, 192, 9) Class count -> [7251, 694] 

           frequency
No freeze  91.264946
Freeze      8.735053

Validation:  X -> (1602, 192, 9) Class count -> [1345, 257] 

           frequency
No freeze  83.957550
Freeze     16.042446

Test:  X -> (2332, 192, 9) Class count -> [2165, 167] 

           frequency
No freeze  92.838760
Freeze      7.161235

sLSTM Model : 2021-02-10 18:29:51.814084
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.307455  0.904594  0.505650      0.839576  0.000500
1    0.240870  0.912398  0.740210      0.838327  0.000500
2    0.242518  0.914160  0.566080      0.837703  0.000500
3    0.232616  0.914915  0.371864      0.834582  0.000500
4    0.224164  0.907615  0.382812      0.838327  0.000500
5    0.211464  0.915544  0.436702      0.837703  0.000500
6    0.204091  0.918313  0.388063      0.837079  0.000500
7    0.244009  0.915670  0.496525      0.839576  0.000500
8    0.293775  0.912398  0.491375      0.839576  0.000500
9    0.284317  0.914286  0.481466      0.839576  0.000500
10   0.290011  0.909755  0.461935      0.839576  0.000500
11   0.283634  0.912524  0.408875      0.838951  0.000500
12   0.265429  0.912524  0.449341      0.839576  0.000500
13   0.239117  0.913027  0.506511      0.839576  0.000500
14   0.235579  0.912649  0.606475      0.839576  0.000500
15   0.236481  0.914286  0.678617      0.839576  0.000500
16   0.258579  0.910761  0.554511      0.839576  0.000500
17   0.227845  0.915293  0.499825      0.837703  0.000400
18   0.247909  0.906860  0.483015      0.839576  0.000400
19   0.229805  0.912649  0.471611      0.839576  0.000400
20   0.226256  0.913405  0.423610      0.838327  0.000400
21   0.216653  0.916929  0.473414      0.835830  0.000400
22   0.206709  0.917936  0.464720      0.818976  0.000400
23   0.225918  0.914789  0.450541      0.833333  0.000400
24   0.205873  0.921586  0.531601      0.788390  0.000400
25   0.207911  0.912901  0.433537      0.837703  0.000400
26   0.204912  0.922089  0.454564      0.827091  0.000400
27   0.192793  0.921838  0.457010      0.827715  0.000320
28   0.190686  0.924103  0.457608      0.818976  0.000320
29   0.191151  0.923977  0.430933      0.774032  0.000320
30   0.190097  0.922089  0.472952      0.807740  0.000320
31   0.180307  0.925739  0.398077      0.814607  0.000320
32   0.182569  0.925110  0.381912      0.817728  0.000320
33   0.185454  0.923600  0.389557      0.822722  0.000320
34   0.172013  0.928509  0.441441      0.837703  0.000320
35   0.179109  0.927627  0.416479      0.834582  0.000320
36   0.171625  0.930396  0.351420      0.838327  0.000320
37   0.175922  0.925614  0.501105      0.828340  0.000320
38   0.173326  0.929138  0.461272      0.837703  0.000320
39   0.188888  0.921712  0.356892      0.788390  0.000320
40   0.193701  0.919698  0.349422      0.825843  0.000320
41   0.188750  0.920201  0.367195      0.806492  0.000320
42   0.182080  0.926369  0.357983      0.787141  0.000320
43   0.172860  0.927879  0.398224      0.832709  0.000320
44   0.168920  0.930522  0.456104      0.843945  0.000320
45   0.187818  0.923977  0.403866      0.848315  0.000320
46   0.173943  0.929390  0.341888      0.847066  0.000320
47   0.169036  0.930396  0.347296      0.857678  0.000320
48   0.175006  0.922970  0.328785      0.844569  0.000320
49   0.168929  0.927879  0.331809      0.817728  0.000320
50   0.167066  0.930019  0.376280      0.809613  0.000320
51   0.160493  0.931026  0.393463      0.816479  0.000320
52   0.159458  0.934550  0.351059      0.810237  0.000320
53   0.160032  0.930271  0.358807      0.847066  0.000320
54   0.158115  0.935053  0.396540      0.838951  0.000320
55   0.162554  0.933040  0.349662      0.853308  0.000320
56   0.160929  0.931907  0.350583      0.841448  0.000320
57   0.153234  0.935935  0.367748      0.838327  0.000320
58   0.155479  0.935683  0.361421      0.822722  0.000320
59   0.148430  0.941598  0.369036      0.834582  0.000320
60   0.149509  0.940340  0.381354      0.847690  0.000320
61   0.198597  0.928886  0.410994      0.848939  0.000320
62   0.182356  0.930145  0.412505      0.848939  0.000320
63   0.164717  0.934550  0.421448      0.848939  0.000320
64   0.156933  0.937697  0.410204      0.852060  0.000320
65   0.151156  0.939081  0.405778      0.847690  0.000320
66   0.159162  0.937823  0.474381      0.846442  0.000320
67   0.159055  0.937319  0.388047      0.848939  0.000320
68   0.150762  0.942228  0.390342      0.853308  0.000320
69   0.148909  0.942857  0.397190      0.848939  0.000320
70   0.148032  0.942228  0.415246      0.848315  0.000256
71   0.151478  0.941347  0.389792      0.851436  0.000256
72   0.143225  0.945374  0.398798      0.847690  0.000256
73   0.142296  0.942605  0.380721      0.848939  0.000256
74   0.143377  0.943235  0.430651      0.849563  0.000256
75   0.146052  0.940214  0.416910      0.846442  0.000256
76   0.140471  0.944997  0.385739      0.848939  0.000256
77   0.142007  0.944242  0.370708      0.855805  0.000256
78   0.144044  0.944242  0.398617      0.852060  0.000256
79   0.140309  0.946507  0.414415      0.848315  0.000256
80   0.141282  0.946130  0.427657      0.847690  0.000256
81   0.139719  0.945123  0.419760      0.843945  0.000256
82   0.140528  0.947892  0.482523      0.843321  0.000256
83   0.138078  0.947262  0.475761      0.844569  0.000256
84   0.141423  0.944493  0.438551      0.847690  0.000256
85   0.141004  0.944871  0.441759      0.848315  0.000256
86   0.146955  0.943612  0.441288      0.848939  0.000256
87   0.143279  0.944116  0.470799      0.843945  0.000256
88   0.153392  0.942857  0.451961      0.848939  0.000256
89   0.146443  0.945878  0.468835      0.843945  0.000256
90   0.144481  0.947640  0.357202      0.849563  0.000256
91   0.134415  0.948521  0.372196      0.847690  0.000256
92   0.136232  0.947640  0.381622      0.846442  0.000256
93   0.131349  0.947388  0.416523      0.847066  0.000256
94   0.132129  0.948395  0.332611      0.847066  0.000256
95   0.145264  0.943612  0.410248      0.838951  0.000256
96   0.133839  0.948395  0.429962      0.843945  0.000256
97   0.131888  0.948899  0.416990      0.845194  0.000256
98   0.130153  0.952675  0.465171      0.842072  0.000256
99   0.129275  0.950409  0.349641      0.843321  0.000256
100  0.131032  0.949906  0.400618      0.831461  0.000256
101  0.128473  0.951794  0.408711      0.838951  0.000256
102  0.128864  0.950157  0.385206      0.828340  0.000256
103  0.128759  0.950157  0.401633      0.842697  0.000256
104  0.129446  0.947640  0.407341      0.838951  0.000256
105  0.129555  0.951164  0.425629      0.846442  0.000256
106  0.130040  0.949150  0.358372      0.835830  0.000256
107  0.126512  0.951038  0.390645      0.842697  0.000256
108  0.123504  0.953933  0.385346      0.843945  0.000256
109  0.124048  0.955192  0.422452      0.839576  0.000256
110  0.120452  0.957080  0.404390      0.843321  0.000256
111  0.130615  0.949780  0.410249      0.850811  0.000256
112  0.123301  0.954185  0.374120      0.849563  0.000256
113  0.124608  0.951794  0.407181      0.847690  0.000256
114  0.120599  0.954185  0.463594      0.842072  0.000256
115  0.119186  0.953556  0.411328      0.845194  0.000256
116  0.120695  0.956199  0.372081      0.846442  0.000256
117  0.121594  0.951542  0.374007      0.846442  0.000256
118  0.118456  0.955821  0.378528      0.846442  0.000256
119  0.118316  0.957080  0.412027      0.845818  0.000256
120  0.121613  0.953556  0.373281      0.843945  0.000256
121  0.117125  0.954437  0.389043      0.840824  0.000256
122  0.115785  0.955066  0.387774      0.844569  0.000256
123  0.116293  0.956325  0.435130      0.844569  0.000256
124  0.118450  0.956576  0.354648      0.848315  0.000256
125  0.117100  0.957206  0.407826      0.845194  0.000256
126  0.115036  0.956576  0.371605      0.847066  0.000256
127  0.116522  0.954059  0.445059      0.843321  0.000256
128  0.117884  0.955318  0.367279      0.847066  0.000256
129  0.118132  0.954814  0.416150      0.840824  0.000256
130  0.115519  0.954688  0.398286      0.842697  0.000256
131  0.107461  0.956954  0.390710      0.839576  0.000256
132  0.112198  0.957583  0.450717      0.843945  0.000256
133  0.111956  0.958590  0.436392      0.841448  0.000256
134  0.109464  0.960101  0.416631      0.846442  0.000256
135  0.118051  0.952801  0.414076      0.842072  0.000256
136  0.109219  0.958464  0.428458      0.842072  0.000256
137  0.111820  0.957583  0.366493      0.843945  0.000256
138  0.111167  0.958339  0.448226      0.846442  0.000256
139  0.106593  0.960856  0.471173      0.842072  0.000256
140  0.106291  0.960604  0.439161      0.843945  0.000256
141  0.109097  0.958716  0.392840      0.849563  0.000256
142  0.106767  0.960982  0.393782      0.847690  0.000256
143  0.109931  0.958842  0.344778      0.842697  0.000256
144  0.108289  0.958716  0.422416      0.847690  0.000256
145  0.105916  0.960101  0.455777      0.846442  0.000256
146  0.106817  0.959849  0.446842      0.841448  0.000256
147  0.104285  0.959597  0.406393      0.843321  0.000256
148  0.106670  0.959723  0.472856      0.848315  0.000256

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization (BatchNo (None, 192, 9)            36        
_________________________________________________________________
preprocess (Dense)           (None, 192, 100)          1000      
_________________________________________________________________
lstm (LSTM)                  (None, 192, 128)          117248    
_________________________________________________________________
dropout (Dropout)            (None, 192, 128)          0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense (Dense)                (None, 100)               12900     
_________________________________________________________________
output (Dense)               (None, 2)                 202       
=================================================================
Total params: 262,970
Trainable params: 262,952
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 4


Test Accuracy: 87.65008449554443
Test Loss: 0.29688966274261475



Classification Report
              precision    recall  f1-score   support

   No freeze       0.94      0.93      0.93      2165
      Freeze       0.18      0.20      0.19       167

    accuracy                           0.88      2332
   macro avg       0.56      0.56      0.56      2332
weighted avg       0.88      0.88      0.88      2332



Confusion Matrix
[[2011  154]
 [ 134   33]]


Normalised Confusion Matrix: True
           No freeze  Freeze
No freeze      92.89    7.11
Freeze         80.24   19.76


Finished working on: sLSTM at: 2021-02-10 18:29:54.690456 -> 591.7951726913452

BiLSTM Model : 2021-02-10 18:42:30.333124
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.296793  0.911768  0.551276      0.839576  0.000500
1    0.238848  0.913279  0.498803      0.839576  0.000500
2    0.221276  0.914537  0.545399      0.838951  0.000500
3    0.228632  0.912398  0.364739      0.839576  0.000500
4    0.214950  0.914915  0.395207      0.838327  0.000500
5    0.227607  0.912272  0.396876      0.839576  0.000500
6    0.206474  0.913908  0.351953      0.840824  0.000500
7    0.196377  0.917306  0.332543      0.840824  0.000500
8    0.195402  0.921712  0.393426      0.839576  0.000500
9    0.202242  0.916551  0.333955      0.842697  0.000500
10   0.221035  0.913405  0.358473      0.839576  0.000500
11   0.216380  0.917181  0.336220      0.838951  0.000500
12   0.208819  0.916551  0.389092      0.838951  0.000500
13   0.204920  0.916929  0.399866      0.839576  0.000500
14   0.189620  0.921208  0.373879      0.779026  0.000500
15   0.194459  0.918439  0.350895      0.825843  0.000500
16   0.201912  0.918313  0.374641      0.839576  0.000500
17   0.194677  0.918188  0.371707      0.839576  0.000500
18   0.193648  0.921208  0.505188      0.840200  0.000500
19   0.190807  0.920076  0.395844      0.839576  0.000500
20   0.179949  0.924481  0.453192      0.840824  0.000500
21   0.174709  0.929515  0.517549      0.842072  0.000500
22   0.170196  0.929767  0.465998      0.842697  0.000500
23   0.175605  0.923977  0.552291      0.839576  0.000500
24   0.171877  0.928383  0.405624      0.854557  0.000500
25   0.167053  0.928131  0.514053      0.843945  0.000500
26   0.162695  0.933417  0.459263      0.815855  0.000500
27   0.168146  0.929641  0.418768      0.840824  0.000500
28   0.170015  0.927376  0.640614      0.852060  0.000500
29   0.168955  0.928131  0.673534      0.840824  0.000500
30   0.162613  0.927753  0.563255      0.840824  0.000500
31   0.154748  0.933040  0.576147      0.842072  0.000500
32   0.150886  0.935935  0.497434      0.845818  0.000500
33   0.149868  0.931655  0.578491      0.848315  0.000500
34   0.151440  0.935179  0.477478      0.852060  0.000500
35   0.144664  0.938955  0.586674      0.842697  0.000500
36   0.142544  0.937823  0.485970      0.846442  0.000500
37   0.143097  0.940969  0.516779      0.848939  0.000500
38   0.138962  0.941221  0.439916      0.845818  0.000500
39   0.140807  0.940843  0.473576      0.843321  0.000500
40   0.135204  0.944368  0.392399      0.850811  0.000500
41   0.131729  0.944368  0.560640      0.843321  0.000500
42   0.134237  0.943486  0.515324      0.845194  0.000500
43   0.133807  0.943612  0.521559      0.846442  0.000500
44   0.134099  0.948143  0.417566      0.850187  0.000500
45   0.129541  0.947262  0.524236      0.840824  0.000500
46   0.134013  0.945123  0.473536      0.842072  0.000500
47   0.128208  0.949150  0.484009      0.851436  0.000500
48   0.125115  0.949906  0.448850      0.848315  0.000500
49   0.123795  0.953052  0.486070      0.855181  0.000500
50   0.123753  0.950031  0.576066      0.848939  0.000500
51   0.128032  0.950913  0.613172      0.845818  0.000500
52   0.123657  0.950787  0.544917      0.845818  0.000500
53   0.120238  0.950787  0.566474      0.844569  0.000500
54   0.122681  0.951290  0.558864      0.840200  0.000500
55   0.129758  0.946885  0.538529      0.842072  0.000500
56   0.126394  0.948395  0.601512      0.841448  0.000500
57   0.122516  0.949150  0.470527      0.845818  0.000500
58   0.123612  0.950787  0.456939      0.848315  0.000500
59   0.119159  0.952675  0.516371      0.848315  0.000500
60   0.116476  0.952926  0.601542      0.842072  0.000500
61   0.117323  0.952549  0.533012      0.844569  0.000500
62   0.107760  0.958464  0.567385      0.849563  0.000500
63   0.116733  0.954185  0.506552      0.848315  0.000500
64   0.114954  0.953304  0.579772      0.850187  0.000500
65   0.106336  0.957835  0.586514      0.850811  0.000500
66   0.104561  0.958968  0.662476      0.849563  0.000500
67   0.110244  0.956576  0.623004      0.847690  0.000500
68   0.111308  0.954814  0.641756      0.850187  0.000500
69   0.103289  0.959849  0.466764      0.860175  0.000500
70   0.106579  0.956702  0.560966      0.850187  0.000500
71   0.111269  0.955570  0.584003      0.849563  0.000500
72   0.097774  0.961611  0.614424      0.852684  0.000500
73   0.098127  0.959471  0.570752      0.852060  0.000500
74   0.101398  0.961233  0.571597      0.852684  0.000500
75   0.096817  0.959723  0.706767      0.842072  0.000500
76   0.090948  0.962996  0.571025      0.853308  0.000500
77   0.097184  0.961989  0.685882      0.845818  0.000500
78   0.095328  0.962492  0.684304      0.842072  0.000500
79   0.091197  0.964254  0.642619      0.851436  0.000500
80   0.100115  0.961611  0.700336      0.845194  0.000500
81   0.095718  0.962870  0.633792      0.849563  0.000500
82   0.122910  0.952045  0.861084      0.845194  0.000500
83   0.112607  0.954185  0.754507      0.843945  0.000500
84   0.120394  0.951416  0.672604      0.844569  0.000500
85   0.126909  0.948018  0.607242      0.845818  0.000500
86   0.112192  0.954940  0.630879      0.848939  0.000500
87   0.109903  0.954814  0.669980      0.846442  0.000400
88   0.104113  0.959220  0.720558      0.846442  0.000400
89   0.098250  0.962115  0.757118      0.847690  0.000400
90   0.096245  0.960604  0.819188      0.848315  0.000400
91   0.096359  0.961233  0.875300      0.844569  0.000400
92   0.090063  0.965261  0.712723      0.848939  0.000400
93   0.091183  0.964884  0.837318      0.843945  0.000400
94   0.087101  0.966394  0.714411      0.846442  0.000400
95   0.089264  0.965135  0.821473      0.843945  0.000400
96   0.092911  0.965765  0.694812      0.847690  0.000400
97   0.094948  0.964002  0.718876      0.846442  0.000400
98   0.089240  0.963877  0.749171      0.846442  0.000400
99   0.084983  0.967904  0.762237      0.847690  0.000400
100  0.083712  0.968408  0.703393      0.847690  0.000400
101  0.083728  0.969289  0.761094      0.848315  0.000400
102  0.075763  0.971051  0.735080      0.848315  0.000400
103  0.079648  0.970673  0.764320      0.847066  0.000400
104  0.077762  0.970925  0.734690      0.848315  0.000400
105  0.076617  0.970799  0.757428      0.850187  0.000400
106  0.075939  0.970673  0.706309      0.850187  0.000400
107  0.073031  0.971177  0.848639      0.847690  0.000400
108  0.066549  0.975582  0.766638      0.849563  0.000400
109  0.069545  0.976211  0.724972      0.847066  0.000400
110  0.070606  0.973946  0.793877      0.847066  0.000400
111  0.070468  0.975456  0.716122      0.851436  0.000400
112  0.070418  0.974449  0.828751      0.849563  0.000400
113  0.068185  0.975708  0.614872      0.852684  0.000400
114  0.070336  0.973694  0.692337      0.850187  0.000400
115  0.065165  0.977218  0.865210      0.846442  0.000400
116  0.066151  0.977093  0.786337      0.850187  0.000400
117  0.074317  0.970422  0.743904      0.849563  0.000400
118  0.067683  0.976463  0.736136      0.850187  0.000400
119  0.066821  0.976967  0.676039      0.848939  0.000400
120  0.061302  0.978351  0.806667      0.847066  0.000400
121  0.061407  0.979106  0.798437      0.847066  0.000400
122  0.059422  0.980113  0.769125      0.847066  0.000400
123  0.063804  0.978855  0.889382      0.847066  0.000400
124  0.056941  0.980868  0.789867      0.847066  0.000400
125  0.054636  0.979736  0.855092      0.852060  0.000400
126  0.057182  0.982001  0.792823      0.846442  0.000400
127  0.056691  0.980239  0.916885      0.847690  0.000400
128  0.053934  0.981875  0.943038      0.848315  0.000400
129  0.058069  0.979736  0.845618      0.848939  0.000400
130  0.055671  0.981372  0.861439      0.849563  0.000400
131  0.054427  0.981875  0.740443      0.850811  0.000400
132  0.054242  0.981246  0.891571      0.846442  0.000400
133  0.056750  0.981120  0.978515      0.847690  0.000400
134  0.055107  0.981875  0.781949      0.852684  0.000400
135  0.049170  0.983386  0.875060      0.847066  0.000400
136  0.053279  0.983134  0.867724      0.845818  0.000400
137  0.060896  0.977596  0.674258      0.853308  0.000400
138  0.059084  0.978351  0.739236      0.851436  0.000400
139  0.055241  0.980617  0.819674      0.848939  0.000400
140  0.050560  0.984267  0.825939      0.849563  0.000400
141  0.049325  0.983889  1.014074      0.844569  0.000400
142  0.058348  0.978980  0.946568      0.843321  0.000400
143  0.050056  0.983512  0.846328      0.848315  0.000400
144  0.050836  0.981875  0.728530      0.847690  0.000400
145  0.056252  0.980994  0.863429      0.848315  0.000400
146  0.045898  0.983638  0.940469      0.843945  0.000320
147  0.044675  0.986281  0.888779      0.846442  0.000320
148  0.051261  0.981624  1.008825      0.844569  0.000320
149  0.047429  0.984519  1.022375      0.845818  0.000320
150  0.039865  0.988420  1.012641      0.845818  0.000320
151  0.046376  0.986784  0.804910      0.848315  0.000320
152  0.046493  0.985274  0.957094      0.847690  0.000320
153  0.043050  0.986407  0.928733      0.848315  0.000320
154  0.044919  0.986407  0.906240      0.847066  0.000320
155  0.049043  0.984015  0.791406      0.847690  0.000320
156  0.048887  0.983638  0.749242      0.850811  0.000320
157  0.042691  0.986155  0.867565      0.847066  0.000320
158  0.041858  0.987288  0.927497      0.845818  0.000320
159  0.044171  0.985148  0.879440      0.847066  0.000320
160  0.047795  0.984267  0.848203      0.845818  0.000320
161  0.037896  0.988798  0.837874      0.848315  0.000256
162  0.039643  0.987665  0.845248      0.848315  0.000256
163  0.039599  0.987791  0.890708      0.847066  0.000256
164  0.039346  0.987413  0.965607      0.845194  0.000256
165  0.036052  0.988924  0.957627      0.842697  0.000256
166  0.035558  0.988672  0.894823      0.845818  0.000256
167  0.036974  0.987791  0.878063      0.846442  0.000256
168  0.035486  0.989301  0.995864      0.844569  0.000256
169  0.039343  0.987791  0.996181      0.846442  0.000256
170  0.037942  0.986658  0.871971      0.846442  0.000256
171  0.036982  0.988420  0.983430      0.843945  0.000256
172  0.033771  0.990057  0.996148      0.844569  0.000256
173  0.034665  0.989427  1.047033      0.844569  0.000256
174  0.036079  0.988924  0.969436      0.845818  0.000256
175  0.037294  0.988295  0.921113      0.846442  0.000256
176  0.033998  0.990434  0.934147      0.844569  0.000256
177  0.032739  0.990686  0.978329      0.845194  0.000256
178  0.036034  0.989931  0.977208      0.843321  0.000256
179  0.033551  0.990057  1.013957      0.843321  0.000256
180  0.033151  0.990812  1.069936      0.843945  0.000256
181  0.031382  0.991189  1.035119      0.845194  0.000256
182  0.034437  0.989931  1.024020      0.847066  0.000256
183  0.032463  0.990560  1.104163      0.845194  0.000256
184  0.035036  0.989679  1.141605      0.846442  0.000256
185  0.037019  0.988672  1.036580      0.846442  0.000256
186  0.036256  0.990938  0.887360      0.848939  0.000256
187  0.040794  0.986910  0.990026      0.847066  0.000256
188  0.033170  0.989679  0.971778      0.846442  0.000256
189  0.035000  0.989805  0.889001      0.847066  0.000256
190  0.031750  0.991567  1.023937      0.846442  0.000256
191  0.030457  0.991064  1.050605      0.847066  0.000256
192  0.030768  0.990183  1.064900      0.843321  0.000256
193  0.032942  0.990183  0.924017      0.847690  0.000256
194  0.035494  0.988672  1.007370      0.844569  0.000256
195  0.032170  0.990183  1.099872      0.844569  0.000256
196  0.039783  0.987413  1.048175      0.844569  0.000256
197  0.034745  0.989805  1.141063      0.846442  0.000256
198  0.032250  0.991441  1.101525      0.845194  0.000256
199  0.030704  0.990686  1.067798      0.845194  0.000256
200  0.033220  0.989553  1.045008      0.850187  0.000256
201  0.049399  0.983260  0.942805      0.848939  0.000256
202  0.041527  0.988043  1.091092      0.845194  0.000205
203  0.033863  0.989050  0.968784      0.848939  0.000205
204  0.037115  0.990183  1.188951      0.843321  0.000205
205  0.036650  0.987665  1.015840      0.847066  0.000205
206  0.031652  0.990308  1.156760      0.844569  0.000205
207  0.036108  0.988546  1.112429      0.845818  0.000205

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_1 (Batch (None, 192, 9)            36        
_________________________________________________________________
bidirectional (Bidirectional (None, 256)               141312    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               25700     
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 202       
=================================================================
Total params: 167,250
Trainable params: 167,232
Non-trainable params: 18
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 1
Merge mode: concat


Test Accuracy: 92.40995049476624
Test Loss: 0.24789339303970337



Classification Report
              precision    recall  f1-score   support

   No freeze       0.93      1.00      0.96      2165
      Freeze       0.00      0.00      0.00       167

    accuracy                           0.92      2332
   macro avg       0.46      0.50      0.48      2332
weighted avg       0.86      0.92      0.89      2332



Confusion Matrix
[[2155   10]
 [ 167    0]]


Normalised Confusion Matrix: True
           No freeze  Freeze
No freeze      99.54    0.46
Freeze        100.00    0.00


Finished working on: BiLSTM at: 2021-02-10 18:42:32.976983 -> 758.285528421402

iSPLInception Model : 2021-02-10 19:24:43.970215
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    5.205230  0.908496  3.280460      0.839576  0.000500
1    2.145238  0.930774  2.077955      0.838327  0.000500
2    1.349723  0.938326  1.362744      0.839576  0.000500
3    0.972310  0.938704  1.044248      0.839576  0.000500
4    0.709293  0.946381  0.915135      0.839576  0.000500
5    0.542746  0.950409  0.849031      0.839576  0.000500
6    0.432523  0.945123  0.565816      0.855181  0.000500
7    0.356957  0.950409  0.533882      0.839576  0.000500
8    0.295099  0.952549  0.570666      0.846442  0.000500
9    0.256709  0.951794  0.514978      0.839576  0.000500
10   0.231227  0.953052  0.396469      0.855181  0.000500
11   0.206353  0.952423  0.424525      0.882647  0.000500
12   0.221979  0.948899  0.781296      0.840200  0.000500
13   0.201270  0.952423  0.412986      0.845194  0.000500
14   0.187737  0.955695  0.449805      0.845818  0.000500
15   0.191026  0.953682  0.411952      0.845194  0.000500
16   0.170564  0.956828  0.463651      0.850811  0.000500
17   0.168985  0.955444  0.348363      0.870162  0.000500
18   0.162376  0.958464  0.507445      0.839576  0.000500
19   0.166902  0.956828  0.721746      0.850187  0.000500
20   0.152975  0.958339  0.376962      0.858302  0.000500
21   0.157437  0.959975  0.512772      0.842072  0.000500
22   0.150759  0.963121  0.399949      0.858926  0.000500
23   0.161368  0.960856  0.667075      0.842072  0.000500
24   0.143284  0.964002  0.466990      0.862047  0.000500
25   0.136813  0.965891  0.502147      0.860799  0.000500
26   0.134749  0.966646  0.373951      0.873283  0.000500
27   0.133042  0.966268  0.315253      0.895755  0.000500
28   0.140484  0.966646  0.500661      0.849563  0.000500
29   0.133360  0.969037  0.573764      0.841448  0.000500
30   0.129615  0.967653  0.526924      0.845818  0.000500
31   0.129180  0.967779  0.519135      0.854557  0.000500
32   0.123588  0.971177  0.406495      0.832085  0.000500
33   0.121338  0.970673  0.331461      0.876405  0.000500
34   0.127841  0.972184  0.358060      0.879526  0.000500
35   0.116679  0.973820  0.552180      0.855181  0.000500
36   0.115152  0.973694  0.582752      0.855805  0.000500
37   0.115116  0.973442  0.775318      0.840200  0.000500
38   0.125921  0.970547  0.627885      0.843945  0.000500
39   0.121349  0.972058  0.600038      0.854557  0.000500
40   0.124367  0.974953  0.542582      0.864544  0.000500
41   0.116248  0.972184  0.763783      0.840200  0.000500
42   0.114865  0.974575  1.292583      0.843945  0.000500
43   0.112028  0.976841  1.520264      0.839576  0.000500
44   0.113211  0.973946  0.774421      0.847066  0.000500
45   0.130547  0.972687  0.564729      0.863296  0.000500
46   0.142544  0.974575  0.664834      0.854557  0.000500
47   0.117433  0.978729  0.656221      0.852060  0.000500
48   0.115690  0.976463  0.552576      0.855805  0.000500
49   0.112121  0.977974  0.420468      0.868290  0.000500
50   0.119377  0.975205  0.622610      0.857678  0.000500
51   0.107784  0.979610  0.571588      0.865793  0.000500
52   0.111579  0.978980  0.439709      0.855805  0.000500
53   0.099522  0.979232  0.416722      0.864544  0.000500
54   0.119364  0.973065  0.899699      0.850187  0.000500
55   0.103531  0.980491  0.674351      0.846442  0.000500
56   0.104410  0.977974  0.498378      0.855805  0.000500
57   0.103752  0.979987  0.407425      0.866417  0.000500
58   0.100746  0.980365  0.771088      0.851436  0.000500
59   0.105361  0.978225  0.895251      0.847690  0.000500
60   0.106328  0.979736  0.577220      0.848939  0.000500
61   0.098745  0.981624  0.810053      0.843321  0.000500
62   0.100450  0.979232  0.910856      0.840824  0.000500
63   0.108507  0.976337  0.945412      0.848315  0.000500
64   0.096706  0.980113  0.507964      0.865169  0.000500
65   0.094720  0.982253  1.078686      0.841448  0.000500
66   0.099136  0.979232  1.334656      0.838951  0.000500
67   0.106514  0.974701  0.992705      0.846442  0.000500
68   0.094856  0.981120  1.017605      0.850811  0.000500
69   0.110653  0.978855  0.396501      0.865169  0.000500
70   0.106201  0.979736  0.713486      0.848939  0.000500
71   0.097011  0.981372  0.470078      0.851436  0.000500
72   0.089240  0.984393  0.702562      0.855181  0.000500
73   0.096878  0.982253  0.988321      0.843945  0.000500
74   0.091047  0.982756  0.483138      0.855805  0.000500
75   0.084426  0.985022  1.349862      0.840824  0.000500
76   0.092340  0.982127  0.485097      0.854557  0.000500
77   0.084153  0.986658  0.946585      0.843945  0.000500
78   0.091120  0.981750  1.238377      0.838327  0.000500
79   0.093005  0.982001  0.631013      0.848939  0.000500
80   0.084379  0.984393  1.101938      0.847690  0.000500
81   0.088236  0.984519  0.495311      0.867041  0.000500
82   0.094396  0.980743  0.865180      0.853933  0.000500
83   0.103990  0.979987  0.542806      0.858926  0.000500
84   0.083679  0.986155  1.022503      0.842072  0.000500
85   0.100545  0.980365  0.778108      0.852684  0.000500
86   0.084595  0.985148  0.657320      0.850187  0.000500
87   0.109315  0.976211  0.449642      0.857054  0.000500
88   0.103567  0.980617  0.519993      0.842697  0.000500
89   0.087550  0.983889  0.396972      0.877653  0.000500
90   0.087020  0.984519  0.709886      0.850811  0.000500
91   0.089546  0.982631  0.736025      0.860175  0.000500
92   0.083040  0.985777  0.916260      0.845194  0.000500
93   0.079245  0.986532  0.969748      0.840824  0.000500
94   0.081399  0.986532  1.452881      0.840824  0.000500
95   0.084481  0.984393  0.661712      0.857678  0.000500
96   0.088335  0.983512  0.724406      0.852060  0.000500
97   0.083717  0.985148  0.611639      0.857054  0.000500
98   0.085704  0.984015  0.975673      0.846442  0.000500
99   0.083995  0.986029  0.900226      0.853933  0.000500
100  0.088494  0.983008  0.536509      0.870786  0.000500
101  0.101968  0.982505  0.533793      0.865169  0.000500
102  0.089013  0.983134  0.870775      0.849563  0.000500
103  0.084021  0.984770  1.281770      0.843321  0.000500
104  0.079577  0.986029  0.482499      0.860799  0.000400
105  0.071500  0.988295  0.831509      0.854557  0.000400
106  0.078111  0.985903  0.732737      0.862047  0.000400
107  0.075026  0.988043  0.744796      0.856429  0.000400
108  0.068585  0.989931  0.816152      0.852060  0.000400
109  0.083864  0.983512  1.060322      0.843321  0.000400
110  0.075302  0.985903  0.680846      0.862047  0.000400
111  0.079340  0.987665  0.801886      0.853933  0.000400
112  0.076321  0.985148  0.970753      0.841448  0.000400
113  0.073705  0.987413  0.810758      0.862047  0.000400
114  0.072456  0.988798  0.913484      0.850811  0.000400
115  0.075716  0.986784  0.537327      0.878277  0.000400
116  0.073794  0.987917  0.497968      0.865793  0.000400
117  0.073920  0.986029  0.592814      0.868290  0.000400
118  0.081482  0.983889  1.015342      0.838327  0.000400
119  0.073441  0.988295  0.667629      0.849563  0.000320
120  0.068371  0.988672  1.008261      0.847066  0.000320
121  0.076256  0.987413  0.834158      0.847066  0.000320
122  0.073636  0.987413  0.665227      0.853308  0.000320
123  0.065183  0.991441  0.826873      0.845194  0.000320
124  0.062425  0.991693  0.668964      0.860175  0.000320
125  0.064961  0.990183  0.752138      0.856429  0.000320
126  0.063246  0.990686  0.696932      0.853933  0.000320
127  0.058141  0.991064  0.813790      0.848939  0.000320
128  0.075875  0.986532  0.593200      0.863920  0.000320
129  0.065678  0.989553  0.838982      0.847690  0.000320
130  0.064671  0.989427  0.596837      0.861423  0.000320
131  0.079327  0.983260  0.869437      0.853933  0.000320
132  0.085650  0.982505  1.046844      0.843945  0.000320
133  0.067924  0.989805  1.056871      0.848939  0.000320
134  0.071587  0.988546  0.682217      0.848315  0.000320
135  0.061088  0.991441  0.886958      0.852060  0.000320
136  0.060365  0.991064  0.908232      0.853933  0.000320
137  0.062534  0.991567  0.927316      0.851436  0.000320
138  0.057378  0.992322  0.757598      0.857678  0.000256
139  0.058943  0.990434  1.187881      0.854557  0.000256
140  0.056148  0.992952  0.649788      0.866417  0.000256
141  0.052684  0.992952  0.862345      0.853933  0.000256
142  0.048776  0.993958  0.694433      0.852060  0.000256
143  0.071367  0.988420  0.562898      0.862672  0.000256
144  0.074745  0.987413  0.887702      0.852060  0.000256
145  0.062110  0.991567  0.704203      0.865169  0.000256
146  0.058770  0.991567  0.691228      0.862047  0.000256
147  0.059454  0.990560  0.699274      0.861423  0.000256
148  0.054517  0.993707  1.278656      0.843321  0.000256
149  0.053404  0.992952  0.765507      0.858302  0.000256
150  0.053065  0.993833  0.788265      0.857054  0.000256
151  0.050407  0.993455  1.093021      0.849563  0.000256
152  0.051593  0.993203  0.974633      0.852684  0.000256
153  0.057479  0.993077  0.901798      0.855805  0.000205
154  0.049540  0.994714  0.752254      0.853308  0.000205
155  0.049410  0.993707  0.871620      0.850187  0.000205
156  0.053460  0.992826  0.914642      0.847066  0.000205
157  0.047959  0.994462  0.935480      0.846442  0.000205
158  0.050445  0.993707  1.191705      0.843945  0.000205
159  0.053496  0.992070  0.649959      0.856429  0.000205
160  0.046924  0.994588  0.743417      0.854557  0.000205
161  0.042671  0.995721  0.996558      0.844569  0.000205
162  0.044667  0.994210  0.931239      0.847690  0.000205
163  0.045823  0.994084  1.057532      0.845194  0.000205
164  0.046065  0.995469  0.758768      0.850187  0.000205
165  0.043368  0.995846  1.418895      0.843321  0.000205
166  0.048590  0.993203  1.196008      0.843321  0.000205
167  0.053922  0.991945  0.969820      0.843321  0.000205
168  0.047333  0.994965  0.881742      0.852060  0.000205
169  0.044227  0.995469  0.862652      0.848315  0.000205
170  0.045009  0.994210  1.093788      0.848939  0.000205
171  0.042289  0.994840  0.981714      0.852684  0.000205
172  0.046193  0.994714  0.949580      0.855805  0.000205
173  0.043996  0.994462  1.109466      0.850187  0.000205
174  0.043092  0.995721  0.778991      0.857678  0.000205
175  0.042848  0.994588  0.954908      0.850187  0.000205
176  0.042563  0.995091  0.777243      0.857054  0.000205
177  0.039549  0.995972  0.933329      0.849563  0.000205
178  0.049168  0.992448  0.777423      0.865793  0.000205
179  0.045360  0.994714  0.833886      0.855805  0.000205
180  0.044117  0.994084  0.790849      0.855181  0.000205
181  0.050111  0.991819  1.174314      0.845194  0.000205
182  0.045912  0.994084  1.155477      0.846442  0.000205
183  0.048584  0.993077  1.059709      0.841448  0.000205
184  0.046586  0.993958  1.452604      0.840824  0.000205
185  0.045930  0.993958  0.685278      0.858302  0.000205
186  0.043825  0.994965  0.885413      0.850187  0.000205
187  0.046042  0.993581  1.012057      0.848939  0.000205
188  0.041216  0.995721  0.772470      0.856429  0.000164
189  0.039820  0.996224  1.100468      0.847066  0.000164
190  0.040083  0.995846  1.103341      0.848939  0.000164
191  0.040514  0.995972  0.726115      0.860175  0.000164
192  0.038867  0.995972  0.787615      0.852684  0.000164
193  0.040618  0.994965  0.823027      0.859551  0.000164
194  0.040063  0.994462  1.122361      0.849563  0.000164
195  0.042920  0.995469  1.163805      0.843321  0.000164
196  0.049676  0.994462  0.750543      0.852684  0.000164
197  0.041988  0.995469  0.684571      0.860175  0.000164
198  0.044212  0.993203  0.950899      0.842697  0.000164
199  0.040697  0.994840  0.808231      0.846442  0.000164
200  0.044674  0.993833  0.788289      0.850811  0.000164
201  0.040294  0.995721  1.019616      0.843321  0.000164
202  0.038512  0.996224  1.248056      0.838951  0.000164
203  0.039504  0.995343  0.875472      0.853933  0.000164
204  0.040455  0.994336  0.993667      0.858926  0.000164
205  0.042456  0.994462  0.970797      0.844569  0.000164
206  0.037875  0.996476  0.903656      0.854557  0.000164
207  0.036697  0.996350  0.630322      0.862672  0.000164
208  0.038153  0.995469  0.800243      0.864544  0.000164
209  0.042308  0.994462  1.272261      0.841448  0.000164
210  0.040515  0.995091  0.775214      0.857678  0.000164
211  0.035504  0.996728  1.004594      0.846442  0.000164
212  0.043159  0.994084  0.996324      0.848939  0.000164
213  0.041532  0.994840  0.755218      0.855805  0.000164
214  0.036745  0.996476  0.750030      0.857054  0.000164
215  0.040084  0.994965  0.964686      0.853308  0.000164
216  0.038677  0.995469  0.935024      0.850187  0.000164
217  0.043048  0.994965  0.767182      0.857678  0.000164
218  0.040836  0.994714  1.210954      0.844569  0.000164
219  0.035481  0.997734  0.916247      0.849563  0.000164
220  0.036256  0.996350  0.797114      0.853308  0.000164
221  0.045557  0.992448  0.676102      0.863920  0.000164
222  0.042673  0.994714  0.860648      0.858302  0.000131
223  0.036351  0.996476  0.913203      0.850811  0.000131
224  0.044634  0.993581  0.967247      0.849563  0.000131
225  0.039833  0.994840  1.147600      0.843945  0.000131
226  0.034525  0.997105  1.080581      0.845194  0.000131
227  0.037036  0.996602  0.973564      0.850187  0.000131
228  0.035466  0.997609  0.868486      0.849563  0.000131
229  0.036204  0.995469  1.144154      0.845194  0.000131
230  0.036002  0.996476  0.872241      0.853933  0.000131
231  0.032329  0.996853  0.998915      0.845818  0.000131
232  0.034026  0.996602  1.040622      0.846442  0.000131
233  0.036188  0.996224  1.301865      0.844569  0.000131
234  0.034142  0.996853  0.988973      0.847066  0.000131
235  0.044030  0.994714  0.899034      0.847066  0.000131
236  0.036944  0.997105  1.018400      0.846442  0.000131
237  0.034612  0.996979  1.038670      0.847066  0.000131
238  0.034428  0.996350  1.053866      0.850187  0.000131
239  0.036917  0.995972  0.931035      0.849563  0.000131
240  0.035795  0.995595  0.834772      0.851436  0.000131
241  0.040532  0.994965  0.934878      0.846442  0.000131
242  0.037922  0.995972  0.944740      0.848939  0.000105
243  0.041412  0.993581  0.738207      0.847690  0.000105
244  0.034077  0.997734  0.953738      0.843321  0.000105
245  0.032377  0.997231  0.904570      0.845818  0.000105
246  0.034080  0.995721  1.022783      0.846442  0.000105
247  0.034794  0.996350  0.999637      0.846442  0.000105
248  0.033784  0.996979  0.956509      0.847690  0.000105
249  0.037478  0.995343  0.814437      0.852684  0.000105
250  0.033232  0.995972  0.816302      0.847066  0.000105
251  0.032915  0.996979  1.011528      0.848939  0.000105
252  0.036968  0.995469  1.135252      0.845818  0.000100
253  0.034103  0.996853  0.689455      0.855805  0.000100
254  0.032132  0.996979  0.852501      0.849563  0.000100
255  0.031951  0.997357  0.985484      0.848939  0.000100
256  0.030965  0.997734  0.861477      0.852684  0.000100
257  0.031879  0.996602  1.060814      0.842697  0.000100
258  0.032783  0.995972  0.864791      0.852684  0.000100
259  0.034495  0.995972  0.742384      0.855805  0.000100
260  0.030100  0.997483  0.888765      0.850187  0.000100
261  0.033104  0.996350  0.965822      0.848315  0.000100
262  0.030491  0.997357  0.862078      0.852060  0.000100
263  0.032047  0.996476  0.829610      0.852060  0.000100
264  0.029866  0.997483  0.896199      0.849563  0.000100
265  0.028459  0.997734  1.110622      0.845818  0.000100
266  0.028888  0.997860  0.919913      0.854557  0.000100
267  0.027866  0.997860  0.997660      0.849563  0.000100
268  0.033297  0.996350  1.421033      0.843945  0.000100
269  0.029360  0.997105  0.960821      0.848315  0.000100
270  0.029792  0.997231  1.221498      0.845194  0.000100
271  0.028856  0.998238  0.847651      0.850187  0.000100
272  0.030254  0.996602  1.214910      0.844569  0.000100
273  0.029234  0.997231  0.826995      0.850187  0.000100
274  0.032267  0.996476  1.189284      0.844569  0.000100
275  0.029958  0.997231  0.819766      0.849563  0.000100
276  0.028249  0.997860  0.900374      0.843945  0.000100
277  0.028712  0.997357  0.883932      0.844569  0.000100
278  0.028470  0.997357  1.047004      0.847066  0.000100
279  0.028491  0.997860  0.905929      0.846442  0.000100
280  0.028385  0.996979  0.984431      0.844569  0.000100
281  0.031016  0.995972  0.950487      0.853933  0.000100
282  0.028787  0.997231  1.205907      0.847690  0.000100
283  0.033701  0.995972  0.880283      0.850187  0.000100
284  0.029513  0.996853  0.985307      0.845194  0.000100
285  0.029095  0.997231  1.111157      0.844569  0.000100
286  0.030296  0.997357  0.891867      0.847066  0.000100
287  0.028778  0.997357  0.725158      0.853933  0.000100
288  0.030164  0.996728  0.932325      0.848315  0.000100
289  0.027319  0.997986  0.997821      0.848939  0.000100
290  0.028322  0.997357  0.924882      0.845818  0.000100
291  0.028078  0.997483  0.981161      0.849563  0.000100
292  0.025749  0.998490  0.811193      0.852060  0.000100
293  0.026310  0.997986  1.083756      0.846442  0.000100
294  0.024600  0.998364  0.920670      0.852060  0.000100
295  0.028969  0.996853  0.964187      0.852060  0.000100
296  0.028237  0.996476  1.073164      0.848315  0.000100
297  0.028288  0.997483  1.126153      0.847690  0.000100
298  0.028634  0.996979  0.865786      0.846442  0.000100
299  0.026809  0.997860  0.811620      0.854557  0.000100
300  0.030254  0.996853  0.969700      0.852684  0.000100
301  0.028826  0.996979  1.118009      0.845194  0.000100
302  0.027425  0.997483  0.910138      0.850811  0.000100
303  0.026983  0.997609  1.304367      0.840824  0.000100
304  0.029867  0.996350  1.400043      0.843945  0.000100
305  0.033549  0.994210  1.023808      0.842072  0.000100
306  0.029330  0.997105  0.941494      0.847066  0.000100
307  0.028994  0.997483  0.961591      0.846442  0.000100
308  0.028343  0.997483  1.014393      0.847066  0.000100
309  0.036523  0.995343  0.854710      0.845818  0.000100
310  0.030747  0.996728  0.647386      0.849563  0.000100
311  0.029979  0.997105  0.786789      0.843321  0.000100
312  0.029011  0.997483  0.669981      0.852684  0.000100
313  0.028278  0.997231  1.049917      0.840200  0.000100
314  0.028468  0.996979  0.956426      0.843945  0.000100
315  0.027760  0.997483  0.955346      0.843321  0.000100
316  0.028709  0.996979  1.068119      0.840824  0.000100
317  0.026618  0.997483  0.765370      0.852060  0.000100
318  0.026859  0.997986  0.899091      0.851436  0.000100
319  0.025665  0.997860  1.161630      0.845818  0.000100
320  0.025777  0.997734  0.932601      0.845194  0.000100
321  0.025942  0.997986  0.695734      0.855805  0.000100
322  0.025468  0.997860  1.069411      0.847066  0.000100
323  0.026301  0.997734  0.830792      0.851436  0.000100
324  0.023869  0.998364  0.785871      0.855805  0.000100
325  0.025659  0.997986  0.928388      0.848939  0.000100
326  0.026601  0.997609  0.854239      0.851436  0.000100
327  0.026838  0.997357  1.022267      0.850811  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 192, 9)]     0                                            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 192, 9)       36          input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 192, 32)      288         batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 192, 9)       0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 192, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 192, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 192, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 192, 64)      576         max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 192, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 192, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 192, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 192, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 192, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 192, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 192, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 192, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 192, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 192, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 192, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 192, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 192, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 192, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 192, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 192, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 192, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 192, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 192, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 192, 256)     2304        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 192, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 192, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 192, 256)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 192, 256)     0           batch_normalization_6[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 192, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 192, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 192, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 192, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 192, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 192, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 192, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 192, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 192, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 192, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 192, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 192, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 192, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 192, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 192, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 192, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 192, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 192, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 192, 256)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 2)            514         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,326,726
Trainable params: 1,323,636
Non-trainable params: 3,090
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 93.48198771476746
Test Loss: 0.26109156012535095



Classification Report
              precision    recall  f1-score   support

   No freeze       0.95      0.98      0.97      2165
      Freeze       0.57      0.37      0.45       167

    accuracy                           0.93      2332
   macro avg       0.76      0.67      0.71      2332
weighted avg       0.92      0.93      0.93      2332



Confusion Matrix
[[2119   46]
 [ 106   61]]


Normalised Confusion Matrix: True
           No freeze  Freeze
No freeze      97.88    2.12
Freeze         63.47   36.53


Finished working on: iSPLInception at: 2021-02-10 19:24:46.339365 -> 2533.3623826503754

