This is the report for the pamap2 Dataset

Data Distribution: 

Train:  X -> (10656, 256, 36) Class count -> [1135, 1056, 1117, 1359, 383, 926, 1031, 700, 629, 1015, 1305] 

                   frequency
lying              10.651277
sitting             9.909910
standing           10.482357
walking            12.753378
running             3.594219
cycling             8.689939
nordic walking      9.675301
ascending stairs    6.569069
descending stairs   5.902778
vacuum cleaning     9.525150
ironing            12.246621

Validation:  X -> (2044, 256, 36) Class count -> [184, 209, 173, 239, 189, 190, 202, 112, 99, 190, 257] 

                   frequency
lying               9.001957
sitting            10.225049
standing            8.463797
walking            11.692760
running             9.246575
cycling             9.295500
nordic walking      9.882584
ascending stairs    5.479452
descending stairs   4.843444
vacuum cleaning     9.295500
ironing            12.573385

Test:  X -> (1935, 256, 36) Class count -> [182, 178, 188, 198, 173, 159, 207, 104, 88, 164, 294] 

                   frequency
lying               9.405684
sitting             9.198967
standing            9.715762
walking            10.232558
running             8.940569
cycling             8.217054
nordic walking     10.697675
ascending stairs    5.374677
descending stairs   4.547803
vacuum cleaning     8.475451
ironing            15.193799

iSPLInception Model : 2021-02-15 15:48:52.475780
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.627650  0.881569  5.331985      0.657045  0.000500
1    3.078233  0.940034  3.187686      0.698141  0.000500
2    1.754250  0.950450  2.253589      0.656067  0.000500
3    1.115056  0.961993  1.811391      0.710372  0.000500
4    0.802305  0.962744  1.413040      0.792564  0.000500
5    0.610847  0.966967  1.387048      0.716732  0.000500
6    0.499058  0.969501  1.513350      0.678571  0.000500
7    0.442987  0.968375  0.940627      0.810176  0.000500
8    0.363979  0.975976  1.242578      0.710861  0.000500
9    0.343810  0.972316  1.000990      0.782779  0.000500
10   0.327859  0.973536  0.850392      0.815558  0.000500
11   0.307866  0.976070  1.051248      0.745597  0.000500
12   0.286576  0.977290  0.910550      0.814090  0.000500
13   0.244075  0.980011  1.052075      0.757339  0.000500
14   0.249562  0.978228  1.158063      0.740704  0.000500
15   0.238448  0.980105  0.693042      0.870842  0.000500
16   0.234064  0.980011  1.054997      0.726517  0.000500
17   0.205082  0.982357  0.898464      0.761742  0.000500
18   0.221894  0.977384  0.746586      0.844912  0.000500
19   0.215156  0.980856  0.653264      0.886986  0.000500
20   0.197741  0.982639  0.846091      0.795988  0.000500
21   0.195151  0.982733  0.747131      0.865460  0.000500
22   0.176749  0.985454  0.895984      0.802348  0.000500
23   0.201919  0.979542  0.760601      0.862035  0.000500
24   0.176360  0.984140  0.837530      0.830724  0.000500
25   0.179818  0.983202  0.797839      0.803327  0.000500
26   0.181097  0.982170  0.840125      0.808708  0.000500
27   0.175533  0.983953  1.055758      0.712818  0.000500
28   0.177186  0.986017  0.818191      0.872798  0.000500
29   0.176203  0.983859  0.639443      0.878669  0.000500
30   0.164554  0.986768  0.910726      0.803816  0.000500
31   0.160028  0.984234  0.772858      0.830235  0.000500
32   0.175149  0.984140  0.907035      0.776908  0.000500
33   0.159136  0.984797  0.787749      0.840998  0.000500
34   0.162179  0.983765  0.842732      0.845890  0.000500
35   0.166410  0.985173  0.866764      0.837084  0.000500
36   0.144168  0.986862  0.618059      0.876223  0.000500
37   0.135762  0.987519  0.621422      0.881115  0.000500
38   0.148992  0.985360  0.820320      0.837573  0.000500
39   0.150926  0.985736  0.634060      0.874266  0.000500
40   0.140871  0.986393  0.757516      0.840998  0.000500
41   0.129969  0.988833  1.053832      0.706947  0.000500
42   0.129577  0.987143  2.736628      0.562622  0.000500
43   0.152448  0.984703  0.858943      0.801370  0.000500
44   0.132592  0.988270  0.620066      0.877202  0.000500
45   0.150132  0.983859  0.693079      0.866438  0.000500
46   0.126579  0.989489  0.605304      0.869374  0.000500
47   0.138511  0.985454  0.741598      0.877691  0.000500
48   0.142434  0.987613  0.735552      0.806751  0.000500
49   0.143237  0.984985  0.771120      0.833659  0.000500
50   0.122104  0.989677  0.773276      0.802348  0.000500
51   0.130745  0.986956  0.798105      0.813112  0.000500
52   0.148128  0.984234  0.789921      0.805284  0.000500
53   0.135156  0.987519  0.706597      0.860078  0.000500
54   0.136415  0.986862  0.605030      0.879648  0.000500
55   0.119870  0.989396  0.743972      0.819472  0.000500
56   0.117415  0.989583  0.645606      0.870352  0.000500
57   0.112240  0.989677  0.859434      0.801370  0.000500
58   0.122500  0.988645  0.768553      0.824364  0.000500
59   0.119333  0.988176  0.739776      0.836106  0.000500
60   0.127215  0.988551  0.773864      0.822407  0.000500
61   0.125340  0.988176  0.588674      0.876712  0.000500
62   0.121589  0.987988  0.716339      0.829746  0.000500
63   0.115669  0.990240  0.572498      0.857143  0.000500
64   0.125826  0.984891  0.806633      0.831213  0.000500
65   0.126416  0.987894  0.520708      0.885519  0.000500
66   0.117728  0.988645  0.712782      0.826810  0.000500
67   0.105808  0.990334  0.845013      0.838063  0.000500
68   0.103490  0.991742  0.678046      0.854697  0.000500
69   0.098705  0.990240  0.690576      0.820450  0.000500
70   0.102624  0.989489  0.682444      0.857632  0.000500
71   0.104943  0.989865  0.748808      0.834149  0.000500
72   0.112446  0.988926  0.894459      0.802838  0.000500
73   0.110981  0.989677  0.768554      0.843933  0.000500
74   0.104597  0.990709  0.729579      0.855675  0.000500
75   0.108235  0.988270  0.747713      0.869863  0.000500
76   0.106932  0.988926  0.713296      0.854207  0.000500
77   0.098545  0.990991  0.836285      0.832192  0.000500
78   0.098679  0.989677  0.711440      0.836106  0.000500
79   0.103389  0.990053  0.889011      0.818493  0.000500
80   0.107232  0.987988  0.738941      0.842955  0.000500
81   0.124503  0.985548  0.713593      0.850783  0.000500
82   0.118129  0.987894  0.694060      0.848337  0.000500
83   0.102931  0.992023  0.667443      0.825832  0.000500
84   0.090971  0.992399  0.694402      0.865460  0.000500
85   0.109504  0.987425  0.614145      0.885029  0.000500
86   0.101249  0.991085  1.014646      0.807241  0.000500
87   0.094290  0.990897  0.732493      0.847358  0.000500
88   0.102858  0.990240  0.666320      0.875245  0.000500
89   0.107520  0.989583  0.974387      0.771526  0.000500
90   0.105851  0.988645  0.879443      0.799902  0.000500
91   0.093828  0.991929  0.706131      0.822407  0.000500
92   0.104574  0.988176  0.782547      0.802838  0.000500
93   0.113980  0.987425  0.950399      0.787182  0.000500
94   0.103610  0.990522  0.805886      0.815558  0.000500
95   0.086907  0.993900  0.698312      0.865949  0.000400
96   0.095478  0.988833  0.709776      0.857143  0.000400
97   0.095108  0.991179  0.808105      0.831703  0.000400
98   0.090760  0.991648  0.713339      0.862035  0.000400
99   0.076793  0.994463  0.844830      0.791585  0.000400
100  0.077313  0.993619  0.716907      0.814579  0.000400
101  0.073226  0.994463  0.831312      0.763699  0.000400
102  0.072385  0.993525  0.743130      0.845401  0.000400
103  0.071335  0.994088  0.600284      0.870842  0.000400
104  0.076099  0.993525  0.832770      0.833170  0.000400
105  0.083209  0.991836  0.715859      0.848337  0.000400
106  0.111828  0.987050  0.696882      0.869374  0.000400
107  0.086194  0.993056  0.649790      0.854697  0.000400
108  0.091482  0.990897  0.680642      0.856654  0.000400
109  0.080468  0.992586  0.674482      0.858121  0.000400
110  0.089745  0.989771  0.597807      0.872798  0.000400
111  0.085455  0.991554  0.614122      0.857632  0.000400
112  0.081662  0.992586  0.840843      0.789628  0.000400
113  0.097908  0.990428  0.699211      0.863014  0.000400
114  0.078817  0.993712  0.614230      0.865949  0.000320
115  0.076207  0.993431  0.648134      0.864971  0.000320
116  0.069673  0.994651  0.693983      0.851761  0.000320
117  0.063567  0.994932  0.705594      0.848826  0.000320
118  0.063319  0.994932  0.675880      0.857143  0.000320
119  0.072045  0.993056  0.889465      0.800391  0.000320
120  0.067430  0.994182  0.904879      0.820939  0.000320
121  0.066807  0.993619  0.808123      0.806751  0.000320
122  0.071259  0.992962  1.334095      0.706458  0.000320
123  0.089247  0.990334  0.615496      0.864971  0.000320
124  0.073315  0.993337  0.693544      0.837084  0.000320
125  0.069264  0.993806  1.044318      0.776419  0.000320
126  0.067952  0.994557  0.622352      0.833170  0.000320
127  0.061989  0.995214  0.773408      0.802838  0.000320
128  0.061534  0.995214  0.799471      0.818493  0.000320
129  0.060378  0.995120  0.680131      0.838552  0.000320
130  0.076489  0.991366  0.920361      0.840509  0.000320
131  0.071502  0.993337  0.647563      0.849804  0.000320
132  0.064872  0.993806  0.640340      0.834638  0.000320
133  0.059354  0.995871  0.817384      0.808708  0.000320
134  0.060726  0.995495  0.658415      0.874755  0.000320
135  0.071005  0.992680  0.744373      0.835127  0.000320
136  0.075767  0.991085  0.850372      0.822896  0.000320
137  0.077111  0.991366  0.591811      0.864481  0.000320
138  0.065352  0.993900  0.522126      0.880137  0.000320
139  0.064425  0.994745  0.619852      0.866438  0.000320
140  0.060818  0.995683  0.649612      0.852251  0.000320
141  0.061851  0.994182  0.619482      0.861057  0.000320
142  0.057430  0.994932  0.601012      0.857143  0.000320
143  0.060785  0.993994  0.943944      0.839530  0.000320
144  0.063763  0.994276  0.597188      0.857143  0.000320
145  0.057905  0.995120  0.586561      0.853718  0.000320
146  0.054101  0.995871  0.653427      0.840998  0.000320
147  0.062544  0.993900  0.688213      0.843933  0.000320
148  0.054607  0.995871  0.572402      0.877202  0.000320
149  0.057993  0.994088  0.685792      0.846869  0.000320
150  0.060935  0.994651  0.699670      0.819961  0.000320
151  0.066591  0.992868  0.746678      0.810176  0.000320
152  0.064542  0.994276  0.721189      0.853229  0.000320
153  0.060838  0.994088  0.878034      0.787182  0.000320
154  0.059802  0.994745  0.588445      0.854207  0.000320
155  0.053922  0.995120  0.634106      0.850294  0.000320
156  0.057410  0.993994  0.688811      0.852740  0.000320
157  0.057967  0.994651  0.766358      0.812133  0.000320
158  0.068664  0.993056  0.673625      0.837573  0.000320
159  0.057505  0.995495  0.592680      0.872309  0.000320
160  0.063853  0.993994  0.723817      0.818982  0.000320
161  0.059942  0.994557  0.767915      0.804305  0.000320
162  0.066285  0.993806  0.719140      0.829256  0.000320
163  0.068671  0.991929  0.768285      0.797456  0.000320
164  0.062374  0.993431  0.789617      0.822407  0.000320
165  0.060257  0.995402  0.636620      0.830724  0.000320
166  0.052707  0.995777  0.762791      0.839041  0.000256
167  0.053062  0.995871  0.659706      0.852740  0.000256
168  0.050568  0.995683  0.706203      0.850783  0.000256
169  0.052267  0.994839  0.685777      0.851272  0.000256
170  0.047770  0.996246  0.725119      0.843933  0.000256
171  0.045442  0.996622  0.674423      0.847847  0.000256
172  0.048889  0.995120  0.801572      0.805773  0.000256
173  0.044194  0.996903  0.745604      0.817025  0.000256
174  0.043747  0.997654  0.704076      0.849804  0.000256
175  0.045379  0.996059  0.802661      0.811155  0.000256
176  0.047636  0.995402  0.730651      0.818982  0.000256
177  0.046186  0.996434  0.592046      0.846869  0.000256
178  0.046166  0.995871  0.812135      0.774462  0.000256
179  0.053752  0.994839  0.700665      0.807730  0.000256
180  0.048613  0.995402  0.704871      0.831703  0.000256
181  0.049930  0.995026  0.582943      0.879158  0.000256
182  0.047344  0.996715  0.757598      0.804795  0.000256
183  0.053823  0.994463  0.795036      0.807730  0.000256
184  0.050708  0.995026  0.940882      0.782290  0.000256
185  0.056664  0.993619  0.583672      0.872798  0.000205
186  0.053776  0.995026  0.547042      0.874755  0.000205
187  0.046582  0.996715  0.544106      0.861057  0.000205
188  0.043928  0.996903  0.667991      0.841977  0.000205
189  0.046258  0.996809  0.773873      0.800391  0.000205
190  0.042107  0.996903  0.725747      0.818982  0.000205
191  0.039875  0.997372  0.649195      0.828278  0.000205
192  0.041836  0.997091  0.836691      0.805773  0.000205
193  0.041041  0.996715  0.644318      0.853718  0.000205
194  0.043084  0.995495  0.629710      0.859589  0.000205
195  0.042193  0.996152  0.788836      0.790117  0.000205
196  0.045680  0.996246  0.752440      0.793542  0.000205
197  0.041397  0.997185  0.662602      0.818982  0.000205
198  0.044106  0.995871  0.739767      0.796477  0.000205
199  0.047842  0.995308  0.665182      0.822407  0.000205
200  0.043176  0.996434  0.705071      0.823386  0.000205
201  0.042539  0.995871  0.711882      0.810176  0.000205
202  0.039619  0.997185  0.655843      0.815558  0.000164
203  0.040371  0.996809  0.691544      0.819472  0.000164
204  0.038596  0.997091  0.629004      0.836595  0.000164
205  0.038769  0.997185  0.697901      0.841487  0.000164
206  0.038373  0.997091  0.636043      0.857143  0.000164
207  0.037300  0.997466  0.726163      0.838552  0.000164
208  0.035927  0.997748  0.650623      0.860078  0.000164
209  0.035950  0.997185  0.648627      0.860568  0.000164
210  0.035550  0.997372  0.603720      0.861057  0.000164
211  0.035618  0.997466  0.570003      0.878180  0.000164
212  0.034908  0.997279  0.613867      0.866928  0.000164
213  0.034233  0.997560  0.807969      0.797945  0.000164
214  0.036891  0.997091  0.617388      0.863992  0.000164
215  0.036222  0.997372  0.569856      0.862524  0.000164
216  0.037606  0.996434  0.716010      0.861546  0.000164
217  0.043311  0.995214  0.692023      0.835616  0.000164
218  0.037522  0.997185  0.633985      0.843444  0.000164
219  0.036784  0.996809  0.689221      0.833659  0.000164
220  0.038121  0.996622  0.694085      0.833659  0.000164
221  0.035869  0.997372  0.648544      0.841977  0.000164
222  0.033791  0.997560  0.650706      0.822407  0.000164
223  0.041663  0.995683  0.597801      0.874755  0.000164
224  0.038800  0.996715  0.674527      0.846869  0.000164
225  0.042353  0.996152  0.558834      0.853229  0.000164
226  0.044063  0.995026  0.815094      0.809198  0.000164
227  0.043681  0.995683  0.650789      0.810665  0.000164
228  0.041032  0.996340  0.587232      0.863014  0.000164
229  0.036851  0.997091  0.659761      0.810176  0.000164
230  0.037056  0.997560  0.698902      0.813601  0.000164
231  0.038693  0.996340  0.799479      0.813112  0.000164
232  0.036170  0.997185  0.692078      0.820450  0.000164
233  0.034010  0.997654  0.659126      0.848337  0.000131
234  0.034466  0.996809  0.663136      0.842955  0.000131
235  0.034572  0.996903  0.627469      0.845401  0.000131
236  0.033932  0.997560  0.676657      0.842466  0.000131
237  0.032372  0.997748  0.708231      0.833170  0.000131
238  0.033008  0.997842  0.667947      0.827789  0.000131
239  0.032422  0.997372  0.655980      0.851272  0.000131
240  0.031724  0.997748  0.629408      0.860078  0.000131
241  0.031704  0.997935  0.682480      0.822896  0.000131
242  0.032204  0.997748  0.752053      0.808219  0.000131
243  0.034021  0.997091  0.698160      0.845890  0.000131
244  0.031612  0.997560  0.644101      0.834149  0.000131
245  0.030517  0.998123  0.754817      0.812133  0.000131
246  0.031359  0.997654  0.667440      0.822896  0.000131
247  0.032104  0.997372  0.719501      0.809687  0.000131
248  0.031579  0.997279  0.721902      0.827789  0.000131
249  0.032679  0.997372  0.751457      0.815558  0.000131
250  0.032726  0.997372  0.648047      0.860568  0.000131
251  0.030051  0.998029  0.693389      0.819961  0.000131
252  0.031719  0.997748  0.769483      0.809687  0.000131
253  0.030269  0.997748  0.749228      0.823875  0.000131
254  0.031458  0.997279  0.692363      0.843444  0.000131
255  0.030576  0.997748  0.658709      0.831703  0.000131
256  0.031187  0.997466  0.650408      0.854207  0.000131
257  0.031417  0.997935  0.642767      0.855675  0.000131
258  0.032425  0.997185  0.727978      0.819961  0.000131
259  0.033555  0.996903  0.748956      0.812622  0.000131
260  0.034896  0.996903  0.656062      0.849315  0.000131
261  0.031983  0.997185  0.699038      0.842955  0.000131
262  0.030951  0.997466  0.736166      0.818982  0.000105
263  0.030089  0.997935  0.820645      0.809687  0.000105
264  0.030105  0.997372  0.747366      0.814090  0.000105
265  0.029892  0.997654  0.726036      0.809687  0.000105
266  0.029956  0.997748  0.748929      0.813112  0.000105
267  0.034262  0.996622  0.777428      0.813601  0.000105
268  0.030756  0.997654  0.749215      0.811644  0.000105
269  0.028919  0.998311  0.744450      0.811644  0.000105
270  0.028883  0.998217  0.672816      0.827299  0.000105
271  0.032088  0.996622  0.772078      0.828278  0.000105
272  0.032435  0.996434  0.674100      0.820939  0.000105
273  0.031092  0.997654  0.713615      0.811644  0.000105
274  0.029975  0.997935  0.633303      0.820939  0.000105
275  0.029037  0.997935  0.671389      0.813112  0.000105
276  0.027834  0.998311  0.682671      0.823386  0.000105
277  0.029827  0.997466  0.700721      0.831213  0.000105
278  0.029477  0.997842  0.720653      0.824364  0.000105
279  0.028657  0.997842  0.694377      0.820450  0.000105
280  0.028830  0.997842  0.782785      0.805773  0.000105
281  0.029864  0.997560  0.747037      0.814579  0.000105
282  0.028089  0.997748  0.746963      0.815558  0.000105
283  0.027259  0.998405  0.734346      0.816047  0.000105
284  0.028290  0.998123  0.740396      0.809198  0.000105
285  0.031567  0.996528  0.894206      0.804305  0.000105
286  0.029858  0.997560  0.698439      0.820450  0.000105
287  0.029483  0.997748  0.696511      0.823875  0.000105
288  0.028099  0.998029  0.735665      0.810176  0.000105
289  0.027230  0.997935  0.756364      0.817515  0.000105
290  0.027423  0.997935  0.709983      0.825342  0.000105
291  0.028290  0.997935  0.714375      0.812622  0.000105
292  0.029060  0.997466  0.720742      0.818004  0.000105
293  0.029825  0.997654  0.680533      0.822896  0.000105
294  0.027894  0.997842  0.708148      0.819472  0.000100
295  0.027792  0.997560  0.708846      0.815068  0.000100
296  0.027011  0.998405  0.696034      0.818004  0.000100
297  0.028272  0.997935  0.745213      0.811155  0.000100
298  0.030220  0.997185  0.599504      0.868885  0.000100
299  0.029945  0.997372  0.734928      0.808708  0.000100
300  0.028921  0.997654  0.703106      0.827299  0.000100
301  0.027254  0.998217  0.752628      0.822407  0.000100
302  0.029870  0.997560  0.711314      0.820450  0.000100
303  0.027343  0.998311  0.706042      0.826321  0.000100
304  0.027780  0.997842  0.714074      0.816536  0.000100
305  0.026797  0.998029  0.696279      0.812622  0.000100
306  0.026983  0.998123  0.696881      0.823386  0.000100
307  0.025828  0.998217  0.678362      0.828767  0.000100
308  0.025914  0.998405  0.714111      0.821429  0.000100
309  0.026089  0.998405  0.750997      0.812133  0.000100
310  0.027236  0.997842  0.697976      0.819472  0.000100
311  0.026438  0.998311  0.637486      0.836106  0.000100
312  0.027300  0.997842  0.635476      0.818493  0.000100
313  0.028372  0.997654  0.695527      0.811155  0.000100
314  0.029391  0.997466  0.646010      0.824364  0.000100
315  0.028306  0.997748  0.626420      0.823386  0.000100
316  0.028442  0.997372  0.682052      0.811644  0.000100
317  0.027966  0.997748  0.659428      0.818004  0.000100
318  0.031787  0.996809  0.689738      0.811155  0.000100
319  0.030453  0.997372  0.780122      0.809687  0.000100
320  0.031207  0.997091  0.735598      0.808708  0.000100
321  0.030915  0.997185  0.607745      0.863014  0.000100
322  0.029635  0.997654  0.723847      0.817515  0.000100
323  0.027161  0.998217  0.730813      0.822896  0.000100
324  0.027345  0.998029  0.739678      0.818493  0.000100
325  0.029875  0.996528  0.802321      0.807730  0.000100
326  0.032643  0.996715  0.751207      0.824853  0.000100
327  0.033992  0.996528  0.645689      0.844912  0.000100
328  0.031043  0.997372  0.621561      0.851272  0.000100
329  0.027857  0.997935  0.649654      0.837573  0.000100
330  0.029734  0.996903  0.654939      0.834149  0.000100
331  0.029676  0.997279  0.647390      0.852251  0.000100
332  0.031329  0.996715  0.783513      0.784736  0.000100
333  0.029826  0.997466  0.629668      0.856164  0.000100
334  0.028159  0.997842  0.701687      0.847847  0.000100
335  0.031994  0.996809  0.764078      0.823875  0.000100
336  0.031339  0.997372  0.614831      0.852740  0.000100
337  0.028526  0.997935  0.651854      0.840020  0.000100
338  0.027478  0.997842  0.638101      0.830724  0.000100
339  0.027053  0.998217  0.667570      0.822407  0.000100
340  0.027208  0.998217  0.701180      0.816047  0.000100
341  0.026959  0.997842  0.665104      0.819472  0.000100
342  0.026264  0.997935  0.702923      0.821429  0.000100
343  0.026062  0.998217  0.691421      0.833659  0.000100
344  0.026570  0.997935  0.775040      0.804305  0.000100
345  0.027875  0.997654  0.801765      0.805773  0.000100
346  0.026765  0.998311  0.789047      0.809198  0.000100
347  0.026437  0.998029  0.733466      0.814090  0.000100
348  0.027660  0.997748  0.701808      0.809687  0.000100
349  0.027840  0.997654  0.792195      0.809687  0.000100

Model: "model_2"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 256, 36)]    0                                            
__________________________________________________________________________________________________
batch_normalization_23 (BatchNo (None, 256, 36)      144         input_3[0][0]                    
__________________________________________________________________________________________________
conv1d_59 (Conv1D)              (None, 256, 32)      1152        batch_normalization_23[0][0]     
__________________________________________________________________________________________________
max_pooling1d_12 (MaxPooling1D) (None, 256, 36)      0           batch_normalization_23[0][0]     
__________________________________________________________________________________________________
conv1d_60 (Conv1D)              (None, 256, 64)      139264      conv1d_59[0][0]                  
__________________________________________________________________________________________________
conv1d_61 (Conv1D)              (None, 256, 64)      69632       conv1d_59[0][0]                  
__________________________________________________________________________________________________
conv1d_62 (Conv1D)              (None, 256, 64)      34816       conv1d_59[0][0]                  
__________________________________________________________________________________________________
conv1d_63 (Conv1D)              (None, 256, 64)      2304        max_pooling1d_12[0][0]           
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 256, 256)     0           conv1d_60[0][0]                  
                                                                 conv1d_61[0][0]                  
                                                                 conv1d_62[0][0]                  
                                                                 conv1d_63[0][0]                  
__________________________________________________________________________________________________
batch_normalization_24 (BatchNo (None, 256, 256)     1024        concatenate_10[0][0]             
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 256, 256)     0           batch_normalization_24[0][0]     
__________________________________________________________________________________________________
conv1d_64 (Conv1D)              (None, 256, 32)      8192        activation_17[0][0]              
__________________________________________________________________________________________________
max_pooling1d_13 (MaxPooling1D) (None, 256, 256)     0           activation_17[0][0]              
__________________________________________________________________________________________________
conv1d_65 (Conv1D)              (None, 256, 64)      139264      conv1d_64[0][0]                  
__________________________________________________________________________________________________
conv1d_66 (Conv1D)              (None, 256, 64)      69632       conv1d_64[0][0]                  
__________________________________________________________________________________________________
conv1d_67 (Conv1D)              (None, 256, 64)      34816       conv1d_64[0][0]                  
__________________________________________________________________________________________________
conv1d_68 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_13[0][0]           
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 256, 256)     0           conv1d_65[0][0]                  
                                                                 conv1d_66[0][0]                  
                                                                 conv1d_67[0][0]                  
                                                                 conv1d_68[0][0]                  
__________________________________________________________________________________________________
batch_normalization_25 (BatchNo (None, 256, 256)     1024        concatenate_11[0][0]             
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 256, 256)     0           batch_normalization_25[0][0]     
__________________________________________________________________________________________________
conv1d_69 (Conv1D)              (None, 256, 32)      8192        activation_18[0][0]              
__________________________________________________________________________________________________
max_pooling1d_14 (MaxPooling1D) (None, 256, 256)     0           activation_18[0][0]              
__________________________________________________________________________________________________
conv1d_70 (Conv1D)              (None, 256, 64)      139264      conv1d_69[0][0]                  
__________________________________________________________________________________________________
conv1d_71 (Conv1D)              (None, 256, 64)      69632       conv1d_69[0][0]                  
__________________________________________________________________________________________________
conv1d_72 (Conv1D)              (None, 256, 64)      34816       conv1d_69[0][0]                  
__________________________________________________________________________________________________
conv1d_73 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_14[0][0]           
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 256, 256)     0           conv1d_70[0][0]                  
                                                                 conv1d_71[0][0]                  
                                                                 conv1d_72[0][0]                  
                                                                 conv1d_73[0][0]                  
__________________________________________________________________________________________________
conv1d_74 (Conv1D)              (None, 256, 256)     9216        batch_normalization_23[0][0]     
__________________________________________________________________________________________________
batch_normalization_26 (BatchNo (None, 256, 256)     1024        concatenate_12[0][0]             
__________________________________________________________________________________________________
batch_normalization_27 (BatchNo (None, 256, 256)     1024        conv1d_74[0][0]                  
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 256, 256)     0           batch_normalization_26[0][0]     
__________________________________________________________________________________________________
add_2 (Add)                     (None, 256, 256)     0           batch_normalization_27[0][0]     
                                                                 activation_19[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 256, 256)     0           add_2[0][0]                      
__________________________________________________________________________________________________
conv1d_75 (Conv1D)              (None, 256, 32)      8192        activation_20[0][0]              
__________________________________________________________________________________________________
max_pooling1d_15 (MaxPooling1D) (None, 256, 256)     0           activation_20[0][0]              
__________________________________________________________________________________________________
conv1d_76 (Conv1D)              (None, 256, 64)      139264      conv1d_75[0][0]                  
__________________________________________________________________________________________________
conv1d_77 (Conv1D)              (None, 256, 64)      69632       conv1d_75[0][0]                  
__________________________________________________________________________________________________
conv1d_78 (Conv1D)              (None, 256, 64)      34816       conv1d_75[0][0]                  
__________________________________________________________________________________________________
conv1d_79 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_15[0][0]           
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 256, 256)     0           conv1d_76[0][0]                  
                                                                 conv1d_77[0][0]                  
                                                                 conv1d_78[0][0]                  
                                                                 conv1d_79[0][0]                  
__________________________________________________________________________________________________
batch_normalization_28 (BatchNo (None, 256, 256)     1024        concatenate_13[0][0]             
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 256, 256)     0           batch_normalization_28[0][0]     
__________________________________________________________________________________________________
conv1d_80 (Conv1D)              (None, 256, 32)      8192        activation_21[0][0]              
__________________________________________________________________________________________________
max_pooling1d_16 (MaxPooling1D) (None, 256, 256)     0           activation_21[0][0]              
__________________________________________________________________________________________________
conv1d_81 (Conv1D)              (None, 256, 64)      139264      conv1d_80[0][0]                  
__________________________________________________________________________________________________
conv1d_82 (Conv1D)              (None, 256, 64)      69632       conv1d_80[0][0]                  
__________________________________________________________________________________________________
conv1d_83 (Conv1D)              (None, 256, 64)      34816       conv1d_80[0][0]                  
__________________________________________________________________________________________________
conv1d_84 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_16[0][0]           
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 256, 256)     0           conv1d_81[0][0]                  
                                                                 conv1d_82[0][0]                  
                                                                 conv1d_83[0][0]                  
                                                                 conv1d_84[0][0]                  
__________________________________________________________________________________________________
batch_normalization_29 (BatchNo (None, 256, 256)     1024        concatenate_14[0][0]             
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 256, 256)     0           batch_normalization_29[0][0]     
__________________________________________________________________________________________________
global_average_pooling1d_2 (Glo (None, 256)          0           activation_22[0][0]              
__________________________________________________________________________________________________
dense_11 (Dense)                (None, 11)           2827        global_average_pooling1d_2[0][0] 
==================================================================================================
Total params: 1,338,651
Trainable params: 1,335,507
Non-trainable params: 3,144
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 87.75193691253662
Test Loss: 0.6807836294174194



Classification Report
                   precision    recall  f1-score   support

            lying       0.98      0.97      0.98       182
          sitting       0.96      0.84      0.90       178
         standing       0.93      0.33      0.49       188
          walking       0.96      0.97      0.96       198
          running       1.00      0.92      0.96       173
          cycling       0.97      0.97      0.97       159
   nordic walking       1.00      0.98      0.99       207
 ascending stairs       0.76      0.91      0.83       104
descending stairs       0.89      0.82      0.85        88
  vacuum cleaning       0.63      0.90      0.74       164
          ironing       0.78      0.96      0.86       294

         accuracy                           0.88      1935
        macro avg       0.89      0.87      0.87      1935
     weighted avg       0.90      0.88      0.87      1935



Confusion Matrix
[[177   1   0   0   0   4   0   0   0   0   0]
 [  0 150   5   0   0   0   0   0   0   3  20]
 [  0   5  62   0   0   0   0   0   0  65  56]
 [  0   0   0 193   0   0   0   4   1   0   0]
 [  3   0   0   1 160   0   0   6   0   0   3]
 [  0   0   0   0   0 155   0   4   0   0   0]
 [  0   0   0   2   0   1 203   1   0   0   0]
 [  0   0   0   4   0   0   0  95   3   2   0]
 [  0   1   0   1   0   0   0   6  72   7   1]
 [  0   0   0   1   0   0   1   9   5 148   0]
 [  0   0   0   0   0   0   0   0   0  11 283]]


Normalised Confusion Matrix: True
                   lying  sitting  standing  walking  running  cycling  \
lying              97.25     0.55      0.00     0.00     0.00     2.20   
sitting             0.00    84.27      2.81     0.00     0.00     0.00   
standing            0.00     2.66     32.98     0.00     0.00     0.00   
walking             0.00     0.00      0.00    97.47     0.00     0.00   
running             1.73     0.00      0.00     0.58    92.49     0.00   
cycling             0.00     0.00      0.00     0.00     0.00    97.48   
nordic walking      0.00     0.00      0.00     0.97     0.00     0.48   
ascending stairs    0.00     0.00      0.00     3.85     0.00     0.00   
descending stairs   0.00     1.14      0.00     1.14     0.00     0.00   
vacuum cleaning     0.00     0.00      0.00     0.61     0.00     0.00   
ironing             0.00     0.00      0.00     0.00     0.00     0.00   

                   nordic walking  ascending stairs  descending stairs  \
lying                        0.00              0.00               0.00   
sitting                      0.00              0.00               0.00   
standing                     0.00              0.00               0.00   
walking                      0.00              2.02               0.51   
running                      0.00              3.47               0.00   
cycling                      0.00              2.52               0.00   
nordic walking              98.07              0.48               0.00   
ascending stairs             0.00             91.35               2.88   
descending stairs            0.00              6.82              81.82   
vacuum cleaning              0.61              5.49               3.05   
ironing                      0.00              0.00               0.00   

                   vacuum cleaning  ironing  
lying                         0.00     0.00  
sitting                       1.69    11.24  
standing                     34.57    29.79  
walking                       0.00     0.00  
running                       0.00     1.73  
cycling                       0.00     0.00  
nordic walking                0.00     0.00  
ascending stairs              1.92     0.00  
descending stairs             7.95     1.14  
vacuum cleaning              90.24     0.00  
ironing                       3.74    96.26  


Finished working on: iSPLInception at: 2021-02-15 15:48:54.999574 -> 3983.3485114574432

Accuracy comparison 
                       0
iSPLInception  87.751937

Loss comparison 
                      0
iSPLInception  0.680784

