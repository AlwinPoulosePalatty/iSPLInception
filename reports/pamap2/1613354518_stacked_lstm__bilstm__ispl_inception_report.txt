This is the report for the pamap2 Dataset

Data Distribution: 

Train:  X -> (10656, 256, 36) Class count -> [1135, 1056, 1117, 1359, 383, 926, 1031, 700, 629, 1015, 1305] 

                   frequency
lying              10.651277
sitting             9.909910
standing           10.482357
walking            12.753378
running             3.594219
cycling             8.689939
nordic walking      9.675301
ascending stairs    6.569069
descending stairs   5.902778
vacuum cleaning     9.525150
ironing            12.246621

Validation:  X -> (2044, 256, 36) Class count -> [184, 209, 173, 239, 189, 190, 202, 112, 99, 190, 257] 

                   frequency
lying               9.001957
sitting            10.225049
standing            8.463797
walking            11.692760
running             9.246575
cycling             9.295500
nordic walking      9.882584
ascending stairs    5.479452
descending stairs   4.843444
vacuum cleaning     9.295500
ironing            12.573385

Test:  X -> (1935, 256, 36) Class count -> [182, 178, 188, 198, 173, 159, 207, 104, 88, 164, 294] 

                   frequency
lying               9.405684
sitting             9.198967
standing            9.715762
walking            10.232558
running             8.940569
cycling             8.217054
nordic walking     10.697675
ascending stairs    5.374677
descending stairs   4.547803
vacuum cleaning     8.475451
ironing            15.193799

sLSTM Model : 2021-02-15 11:20:27.087264
Model History 
         loss  accuracy  val_loss  val_accuracy       lr
0    0.985528  0.712369  1.672082      0.555773  0.00050
1    0.501831  0.866460  0.970168      0.722603  0.00050
2    0.398371  0.893393  0.967400      0.741194  0.00050
3    0.309827  0.919670  0.818786      0.770059  0.00050
4    0.281833  0.922579  1.034281      0.749022  0.00050
5    0.250412  0.932339  1.039344      0.760763  0.00050
6    0.346587  0.909816  0.847840      0.803816  0.00050
7    0.291629  0.923893  0.768099      0.775930  0.00050
8    0.228201  0.938720  0.785424      0.793542  0.00050
9    0.213948  0.944632  0.661813      0.835616  0.00050
10   0.200943  0.947447  0.926179      0.770548  0.00050
11   0.200758  0.944538  0.984015      0.757339  0.00050
12   0.240288  0.934497  1.027968      0.747065  0.00050
13   0.222406  0.940409  1.041349      0.768591  0.00050
14   0.206728  0.944351  0.935308      0.741683  0.00050
15   0.163418  0.956926  0.995070      0.749511  0.00050
16   0.151375  0.960867  0.975491      0.747065  0.00050
17   0.151280  0.959741  0.986241      0.763699  0.00050
18   0.137946  0.963776  0.886351      0.787671  0.00050
19   0.263169  0.928303  0.896638      0.758806  0.00050
20   0.189330  0.950638  0.952144      0.755871  0.00050
21   0.168456  0.956550  0.978967      0.737280  0.00050
22   0.178469  0.953360  1.192372      0.741683  0.00050
23   0.146250  0.961806  0.883771      0.776908  0.00050
24   0.246371  0.935342  0.673720      0.829256  0.00050
25   0.171613  0.955236  0.992362      0.765656  0.00050
26   0.146521  0.961055  1.017889      0.771526  0.00050
27   0.141387  0.964621  0.615500      0.862035  0.00050
28   0.131534  0.966967  0.810366      0.816047  0.00050
29   0.164740  0.955800  0.828935      0.826321  0.00050
30   0.128962  0.966404  0.700426      0.845401  0.00050
31   0.107793  0.973818  0.831511      0.814579  0.00050
32   0.188940  0.953078  0.763769      0.816047  0.00050
33   0.144215  0.962650  0.817750      0.816047  0.00050
34   0.131172  0.965935  0.769450      0.825342  0.00050
35   0.115766  0.971565  0.798870      0.816047  0.00050
36   0.109343  0.972410  0.758197      0.814090  0.00050
37   0.099703  0.975882  0.750703      0.813601  0.00050
38   0.093530  0.977477  0.917052      0.782290  0.00050
39   0.093850  0.978322  0.877469      0.802348  0.00050
40   0.093458  0.977947  0.918550      0.801370  0.00050
41   0.085974  0.979730  0.954516      0.790117  0.00050
42   0.089770  0.979730  0.989953      0.800881  0.00050
43   0.082775  0.982639  0.928680      0.795499  0.00050
44   0.088869  0.979542  0.950686      0.798924  0.00050
45   0.076569  0.983390  0.975369      0.799902  0.00050
46   0.162630  0.959835  0.749399      0.835127  0.00050
47   0.132813  0.965559  0.811040      0.816536  0.00050
48   0.109548  0.974005  0.941497      0.771526  0.00050
49   0.104577  0.973818  0.906377      0.787182  0.00050
50   0.093333  0.977947  0.962537      0.782290  0.00050
51   0.082629  0.981137  1.054372      0.780333  0.00050
52   0.078787  0.982076  0.959369      0.792074  0.00050
53   0.071577  0.985079  0.798451      0.822896  0.00050
54   0.067949  0.986768  0.822605      0.825342  0.00050
55   0.069839  0.986111  1.002286      0.787671  0.00050
56   0.067428  0.985548  0.926160      0.809198  0.00050
57   0.069449  0.984797  1.175949      0.769080  0.00050
58   0.081654  0.981794  1.116854      0.782290  0.00050
59   0.067906  0.986393  1.002292      0.805773  0.00050
60   0.064544  0.987988  0.988228      0.815068  0.00050
61   0.062375  0.988082  1.028559      0.814090  0.00050
62   0.064036  0.986205  1.019408      0.804305  0.00050
63   0.063254  0.986111  1.151645      0.749511  0.00050
64   0.071873  0.984985  0.957729      0.784736  0.00050
65   0.061347  0.988833  0.891007      0.822896  0.00050
66   0.059667  0.988363  0.941333      0.803816  0.00050
67   0.061601  0.987706  1.019234      0.793542  0.00050
68   0.081122  0.982920  1.204832      0.753914  0.00050
69   0.072771  0.984797  0.885130      0.816536  0.00050
70   0.063331  0.986956  1.067307      0.776419  0.00050
71   0.069134  0.985736  1.100180      0.778865  0.00050
72   0.062626  0.987613  1.003631      0.799902  0.00050
73   0.059588  0.988739  0.986360      0.798434  0.00050
74   0.057629  0.988457  1.046640      0.807730  0.00050
75   0.054926  0.989959  0.968497      0.818982  0.00050
76   0.057606  0.989865  0.920384      0.819961  0.00050
77   0.052348  0.990709  1.140453      0.782779  0.00050
78   0.050077  0.992117  1.002728      0.815558  0.00050
79   0.048822  0.991648  0.946808      0.834149  0.00050
80   0.047580  0.992211  0.862189      0.847847  0.00050
81   0.056203  0.990803  0.980730      0.823875  0.00050
82   0.053441  0.989865  0.994064      0.820450  0.00050
83   0.048714  0.992305  0.923691      0.837573  0.00050
84   0.046267  0.993431  0.967022      0.825342  0.00050
85   0.055442  0.989489  1.025015      0.822896  0.00050
86   0.061920  0.988270  1.019765      0.826321  0.00050
87   0.056550  0.990146  1.184341      0.793053  0.00050
88   0.050329  0.992305  1.106333      0.806262  0.00050
89   0.058292  0.989583  1.010660      0.839530  0.00050
90   0.051212  0.991836  0.870668      0.849315  0.00050
91   0.054345  0.990616  1.019610      0.818493  0.00050
92   0.045484  0.992399  1.070783      0.815068  0.00050
93   0.047034  0.993149  0.900792      0.841487  0.00050
94   0.051236  0.990803  1.147489      0.802838  0.00050
95   0.054183  0.989677  1.309725      0.792564  0.00050
96   0.058937  0.988645  1.187814      0.802348  0.00050
97   0.059843  0.987894  1.422274      0.771037  0.00050
98   0.051424  0.990709  1.383924      0.756849  0.00050
99   0.047661  0.992399  1.704850      0.748043  0.00050
100  0.056050  0.989865  1.380083      0.792564  0.00050
101  0.050010  0.991742  1.353335      0.792564  0.00050
102  0.071116  0.985267  1.542607      0.748043  0.00050
103  0.057968  0.988551  1.619639      0.763699  0.00040
104  0.052263  0.991179  1.500095      0.780333  0.00040
105  0.049632  0.991742  1.591364      0.767123  0.00040
106  0.052949  0.991366  1.367858      0.783268  0.00040
107  0.048552  0.991742  1.643932      0.750000  0.00040
108  0.045127  0.993525  1.650720      0.753425  0.00040
109  0.044302  0.993525  1.714569      0.756360  0.00040
110  0.049581  0.991554  1.356083      0.785714  0.00040
111  0.043544  0.993525  1.456247      0.772016  0.00040
112  0.040301  0.994932  1.404855      0.790117  0.00040
113  0.040737  0.995214  1.601562      0.770548  0.00040
114  0.044130  0.993619  1.528299      0.774462  0.00040
115  0.048361  0.992586  1.321060      0.789628  0.00040
116  0.044235  0.993525  1.316161      0.790117  0.00040
117  0.044873  0.993431  1.211991      0.813112  0.00040
118  0.041594  0.993712  1.139038      0.818004  0.00040
119  0.038288  0.995026  1.115379      0.836595  0.00040
120  0.038448  0.994745  1.128093      0.833170  0.00040
121  0.041629  0.994839  1.344591      0.790607  0.00040
122  0.043729  0.993619  1.321259      0.802838  0.00040
123  0.042053  0.994182  1.290287      0.800881  0.00040
124  0.038356  0.995214  1.269964      0.813112  0.00040
125  0.037394  0.995589  1.252233      0.819961  0.00040
126  0.038973  0.995589  1.245720      0.815558  0.00040
127  0.038061  0.995214  1.208037      0.818493  0.00040
128  0.038510  0.994932  1.136585      0.825832  0.00040
129  0.038120  0.995308  1.148372      0.826321  0.00040
130  0.035965  0.996152  1.222627      0.817025  0.00040
131  0.037582  0.995308  1.278902      0.810665  0.00040
132  0.037352  0.995120  1.188984      0.817515  0.00040
133  0.043823  0.993149  1.213312      0.812133  0.00040
134  0.039737  0.994276  1.186516      0.818493  0.00040
135  0.036324  0.995214  1.297967      0.798924  0.00040
136  0.040565  0.994745  1.341201      0.796477  0.00040
137  0.038689  0.994839  1.282503      0.806751  0.00040
138  0.036374  0.996340  1.295248      0.804305  0.00040
139  0.040287  0.995120  1.283270      0.798434  0.00040
140  0.034554  0.996152  1.255861      0.808708  0.00040
141  0.036382  0.995402  1.297596      0.807241  0.00040
142  0.037182  0.995683  1.264274      0.808219  0.00040
143  0.039044  0.995214  1.280526      0.812622  0.00040
144  0.035548  0.996246  1.232134      0.822896  0.00040
145  0.035358  0.996152  1.302006      0.811155  0.00040
146  0.034581  0.996809  1.300588      0.809198  0.00040
147  0.041257  0.994276  1.172348      0.815068  0.00040
148  0.044293  0.992586  1.306417      0.803816  0.00040
149  0.035631  0.996246  1.498893      0.779354  0.00040
150  0.038138  0.994463  1.245596      0.814579  0.00040
151  0.040681  0.994557  1.258126      0.815068  0.00032
152  0.040192  0.994745  1.168512      0.816536  0.00032
153  0.037879  0.994463  1.028701      0.843933  0.00032
154  0.034685  0.995965  1.153717      0.839530  0.00032
155  0.033958  0.996528  1.161003      0.835616  0.00032
156  0.033884  0.996434  1.161380      0.834149  0.00032
157  0.033716  0.996246  1.157978      0.832681  0.00032
158  0.037016  0.995402  1.063044      0.847847  0.00032
159  0.059663  0.989020  1.310833      0.808219  0.00032
160  0.040816  0.994651  1.293396      0.810665  0.00032
161  0.036915  0.995589  1.234514      0.820939  0.00032
162  0.035887  0.995589  1.181830      0.831213  0.00032
163  0.034790  0.995965  1.236549      0.824364  0.00032
164  0.035648  0.995589  1.104380      0.834638  0.00032
165  0.034788  0.995871  1.152971      0.828767  0.00032
166  0.033442  0.996434  1.188405      0.821918  0.00032
167  0.035220  0.995871  1.232543      0.820450  0.00032
168  0.033665  0.996434  1.222194      0.820939  0.00032
169  0.033226  0.996997  1.196860      0.826321  0.00032
170  0.033317  0.996152  1.236377      0.824364  0.00032
171  0.033897  0.996903  1.233457      0.829746  0.00032
172  0.033447  0.996340  1.231503      0.824853  0.00032
173  0.033008  0.996809  1.182030      0.838063  0.00032
174  0.033922  0.996059  1.201690      0.832192  0.00032
175  0.034186  0.995777  1.120010      0.846869  0.00032
176  0.034169  0.996059  1.143932      0.830724  0.00032
177  0.032890  0.996340  1.109749      0.839041  0.00032

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization (BatchNo (None, 256, 36)           144       
_________________________________________________________________
preprocess (Dense)           (None, 256, 100)          3700      
_________________________________________________________________
lstm (LSTM)                  (None, 256, 128)          117248    
_________________________________________________________________
dropout (Dropout)            (None, 256, 128)          0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 128)               131584    
_________________________________________________________________
dense (Dense)                (None, 100)               12900     
_________________________________________________________________
output (Dense)               (None, 11)                1111      
=================================================================
Total params: 266,687
Trainable params: 266,615
Non-trainable params: 72
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 4


Test Accuracy: 87.95865774154663
Test Loss: 0.4658333361148834



Classification Report
                   precision    recall  f1-score   support

            lying       0.95      0.96      0.95       182
          sitting       0.96      0.96      0.96       178
         standing       0.91      0.51      0.65       188
          walking       0.97      0.94      0.96       198
          running       0.96      0.94      0.95       173
          cycling       0.99      0.97      0.98       159
   nordic walking       0.96      0.99      0.97       207
 ascending stairs       0.53      0.88      0.66       104
descending stairs       0.85      0.75      0.80        88
  vacuum cleaning       0.76      0.68      0.71       164
          ironing       0.84      0.97      0.90       294

         accuracy                           0.88      1935
        macro avg       0.88      0.87      0.86      1935
     weighted avg       0.89      0.88      0.88      1935



Confusion Matrix
[[174   1   0   0   0   1   0   5   0   1   0]
 [  0 171   3   0   0   0   0   2   0   2   0]
 [  0   4  96   0   0   0   0  15   1  17  55]
 [  0   0   0 186   5   0   0   7   0   0   0]
 [  6   0   0   1 163   0   0   2   1   0   0]
 [  2   0   1   0   0 155   0   1   0   0   0]
 [  0   0   0   2   0   0 204   0   0   1   0]
 [  0   1   5   1   0   0   2  92   3   0   0]
 [  0   2   0   1   1   0   2   2  66  14   0]
 [  2   0   1   0   0   0   4  39   7 111   0]
 [  0   0   0   0   0   0   0   9   0   1 284]]


Normalised Confusion Matrix: True
                   lying  sitting  standing  walking  running  cycling  \
lying              95.60     0.55      0.00     0.00     0.00     0.55   
sitting             0.00    96.07      1.69     0.00     0.00     0.00   
standing            0.00     2.13     51.06     0.00     0.00     0.00   
walking             0.00     0.00      0.00    93.94     2.53     0.00   
running             3.47     0.00      0.00     0.58    94.22     0.00   
cycling             1.26     0.00      0.63     0.00     0.00    97.48   
nordic walking      0.00     0.00      0.00     0.97     0.00     0.00   
ascending stairs    0.00     0.96      4.81     0.96     0.00     0.00   
descending stairs   0.00     2.27      0.00     1.14     1.14     0.00   
vacuum cleaning     1.22     0.00      0.61     0.00     0.00     0.00   
ironing             0.00     0.00      0.00     0.00     0.00     0.00   

                   nordic walking  ascending stairs  descending stairs  \
lying                        0.00              2.75               0.00   
sitting                      0.00              1.12               0.00   
standing                     0.00              7.98               0.53   
walking                      0.00              3.54               0.00   
running                      0.00              1.16               0.58   
cycling                      0.00              0.63               0.00   
nordic walking              98.55              0.00               0.00   
ascending stairs             1.92             88.46               2.88   
descending stairs            2.27              2.27              75.00   
vacuum cleaning              2.44             23.78               4.27   
ironing                      0.00              3.06               0.00   

                   vacuum cleaning  ironing  
lying                         0.55     0.00  
sitting                       1.12     0.00  
standing                      9.04    29.26  
walking                       0.00     0.00  
running                       0.00     0.00  
cycling                       0.00     0.00  
nordic walking                0.48     0.00  
ascending stairs              0.00     0.00  
descending stairs            15.91     0.00  
vacuum cleaning              67.68     0.00  
ironing                       0.34    96.60  


Finished working on: sLSTM at: 2021-02-15 11:20:30.389475 -> 1111.9924511909485

BiLSTM Model : 2021-02-15 11:40:46.804915
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    0.968440  0.714245  1.019290      0.650196  0.000500
1    0.350043  0.906719  0.731957      0.778865  0.000500
2    0.263883  0.930086  0.650223      0.814090  0.000500
3    0.200792  0.949137  0.697371      0.804305  0.000500
4    0.164616  0.956926  0.658155      0.818493  0.000500
5    0.164018  0.958239  0.662028      0.829746  0.000500
6    0.137025  0.963401  0.676020      0.835616  0.000500
7    0.117913  0.969782  0.658846      0.821429  0.000500
8    0.112994  0.971471  0.724855      0.821429  0.000500
9    0.106575  0.972598  0.647419      0.827789  0.000500
10   0.103698  0.973255  0.650014      0.830724  0.000500
11   0.095600  0.975413  0.662349      0.822896  0.000500
12   0.085733  0.979730  0.750020      0.821918  0.000500
13   0.124850  0.965653  0.722466      0.805773  0.000500
14   0.099682  0.973724  0.703047      0.803816  0.000500
15   0.091086  0.978416  0.747476      0.806262  0.000500
16   0.075313  0.983296  0.683218      0.820939  0.000500
17   0.082134  0.981607  0.655767      0.828278  0.000500
18   0.068661  0.984703  0.703827      0.822407  0.000500
19   0.076445  0.982639  0.749816      0.801859  0.000500
20   0.082024  0.980950  0.769382      0.798924  0.000500
21   0.066582  0.985736  0.833203      0.792564  0.000500
22   0.055912  0.989771  0.842391      0.803816  0.000500
23   0.050674  0.990803  0.924932      0.784736  0.000500
24   0.052097  0.990616  0.959981      0.789628  0.000500
25   0.051862  0.990240  0.844724      0.791585  0.000500
26   0.046677  0.991554  0.891413      0.790117  0.000500
27   0.043403  0.993149  0.912330      0.784736  0.000500
28   0.044019  0.993056  0.845497      0.796967  0.000500
29   0.048546  0.990522  0.858101      0.796477  0.000500
30   0.047054  0.992211  0.912581      0.786204  0.000500
31   0.040339  0.993525  0.993723      0.760763  0.000500
32   0.041808  0.992680  0.901040      0.804795  0.000500
33   0.036720  0.995026  0.816523      0.808708  0.000500
34   0.039231  0.993056  0.793735      0.813601  0.000500
35   0.046823  0.991742  0.790475      0.820450  0.000500
36   0.041384  0.992399  0.855703      0.806751  0.000500
37   0.047132  0.990428  0.816754      0.819961  0.000500
38   0.045180  0.992023  0.845855      0.811644  0.000500
39   0.038013  0.993806  0.791439      0.830235  0.000500
40   0.036551  0.994088  0.891972      0.804795  0.000500
41   0.043336  0.993056  0.936362      0.800881  0.000500
42   0.036621  0.994276  0.927612      0.799902  0.000500
43   0.037234  0.994463  0.952836      0.800391  0.000500
44   0.034511  0.994839  0.904296      0.811155  0.000500
45   0.032824  0.995589  0.874467      0.809687  0.000500
46   0.035486  0.994088  0.978064      0.793542  0.000500
47   0.035932  0.994463  1.009651      0.781800  0.000500
48   0.035871  0.993806  1.047241      0.765656  0.000500
49   0.038928  0.993056  1.018082      0.763209  0.000500
50   0.034926  0.993900  0.983211      0.765656  0.000500
51   0.030520  0.995871  1.008161      0.768591  0.000500
52   0.029700  0.995965  1.083639      0.754892  0.000500
53   0.030517  0.995026  1.032347      0.759785  0.000500
54   0.028838  0.995683  1.062559      0.756360  0.000500
55   0.026620  0.996809  0.980612      0.773483  0.000500
56   0.029111  0.995965  0.942065      0.788650  0.000500
57   0.029656  0.995683  0.940826      0.804305  0.000500
58   0.027001  0.996434  0.914902      0.805773  0.000500
59   0.024508  0.996903  0.953561      0.789139  0.000500
60   0.025682  0.996715  0.911095      0.811644  0.000500
61   0.028230  0.995495  0.909544      0.794521  0.000500
62   0.024659  0.996997  0.906793      0.797456  0.000500
63   0.031094  0.995120  0.934537      0.808219  0.000500
64   0.030263  0.995026  0.916465      0.811644  0.000500
65   0.025676  0.996997  0.952566      0.795499  0.000500
66   0.025449  0.996152  0.982768      0.793542  0.000500
67   0.023263  0.997466  0.925097      0.810176  0.000500
68   0.023757  0.996340  0.911872      0.814579  0.000500
69   0.023925  0.996997  0.935282      0.810665  0.000500
70   0.023910  0.997279  0.948725      0.795010  0.000500
71   0.025093  0.996246  0.919652      0.806751  0.000500
72   0.021491  0.997560  0.947366      0.797456  0.000500
73   0.022836  0.997466  0.995001      0.788650  0.000500
74   0.021725  0.997466  0.923053      0.800391  0.000500
75   0.021723  0.997560  0.976259      0.788160  0.000500
76   0.021991  0.997185  0.947151      0.798924  0.000500
77   0.023586  0.996715  0.973865      0.803327  0.000500
78   0.023147  0.996528  1.052823      0.780333  0.000500
79   0.023671  0.996528  1.002785      0.780333  0.000500
80   0.022909  0.996340  0.988151      0.793542  0.000500
81   0.021587  0.997372  1.059326      0.772994  0.000500
82   0.026262  0.995495  0.997820      0.788650  0.000500
83   0.023267  0.996528  1.010406      0.780822  0.000400
84   0.021284  0.997654  1.031532      0.775440  0.000400
85   0.021010  0.996809  1.007256      0.782779  0.000400
86   0.019702  0.997654  0.980524      0.792564  0.000400
87   0.020616  0.997185  0.992660      0.791096  0.000400
88   0.020715  0.997372  0.945877      0.799902  0.000400
89   0.020886  0.997185  0.977092      0.798434  0.000400
90   0.019488  0.997748  0.939955      0.806751  0.000400
91   0.018740  0.998029  0.945477      0.803816  0.000400
92   0.018655  0.997654  0.938561      0.802348  0.000400
93   0.017311  0.998217  0.946508      0.803816  0.000400
94   0.016650  0.998405  0.970753      0.797945  0.000400
95   0.017752  0.998217  0.987385      0.792564  0.000400
96   0.016935  0.998405  1.015417      0.792074  0.000400
97   0.020420  0.997279  0.955401      0.795010  0.000400
98   0.018296  0.997842  0.949508      0.798924  0.000400
99   0.016867  0.998405  0.961711      0.795499  0.000400
100  0.018003  0.997654  0.965899      0.793053  0.000400
101  0.017703  0.997748  0.962578      0.791585  0.000400
102  0.018769  0.997091  0.923039      0.802348  0.000400
103  0.017373  0.997935  0.930458      0.796477  0.000400
104  0.016269  0.998405  0.947089      0.796967  0.000400
105  0.016315  0.998217  0.975434      0.784247  0.000400
106  0.016475  0.998123  1.003250      0.786693  0.000400
107  0.016798  0.997748  1.018371      0.783757  0.000400
108  0.017266  0.997654  0.991749      0.786204  0.000400
109  0.016728  0.998311  0.949163      0.793053  0.000400
110  0.016737  0.997842  0.934620      0.796967  0.000400
111  0.017452  0.997935  0.919254      0.801859  0.000400
112  0.016010  0.998311  0.876094      0.810176  0.000400
113  0.017251  0.998123  0.950086      0.786204  0.000400
114  0.015868  0.998217  0.936250      0.800391  0.000400
115  0.015968  0.998029  0.946138      0.795499  0.000400
116  0.014710  0.998780  0.923444      0.801859  0.000400
117  0.015196  0.998123  0.895998      0.810665  0.000400
118  0.015387  0.998029  0.904734      0.805773  0.000400
119  0.025380  0.995589  1.074960      0.760763  0.000400
120  0.021294  0.995965  1.077318      0.765166  0.000400
121  0.017557  0.997560  1.056684      0.768102  0.000400
122  0.016842  0.997935  1.043010      0.769080  0.000400
123  0.016883  0.997842  1.030672      0.772505  0.000400
124  0.016930  0.998029  1.041571      0.768102  0.000400
125  0.015842  0.998029  1.019864      0.773973  0.000400
126  0.016540  0.997935  0.998613      0.781311  0.000400
127  0.019173  0.997091  1.032728      0.765656  0.000320
128  0.015606  0.997842  1.016077      0.770548  0.000320
129  0.015711  0.997935  1.008645      0.777397  0.000320
130  0.015651  0.997748  1.063236      0.772505  0.000320
131  0.014670  0.998217  1.040991      0.771037  0.000320
132  0.014221  0.998592  1.054865      0.769080  0.000320
133  0.015363  0.998123  1.083393      0.768102  0.000320
134  0.014899  0.998123  1.077354      0.769080  0.000320
135  0.013733  0.998592  1.087060      0.760763  0.000320
136  0.015139  0.997935  1.064690      0.760274  0.000320
137  0.015101  0.998217  1.093865      0.759785  0.000320
138  0.015493  0.997935  1.111969      0.753425  0.000320
139  0.014737  0.998217  1.073726      0.755382  0.000320
140  0.014413  0.998405  1.090128      0.754403  0.000320
141  0.015110  0.997748  1.079129      0.758317  0.000320
142  0.017102  0.997279  1.108392      0.751957  0.000320
143  0.015682  0.997654  1.113845      0.748532  0.000320
144  0.013851  0.998217  1.174599      0.724070  0.000320
145  0.018042  0.996903  1.035620      0.769569  0.000320
146  0.015271  0.997935  0.995288      0.773973  0.000256
147  0.013388  0.998498  1.001869      0.778865  0.000256
148  0.013756  0.998123  0.980660      0.782779  0.000256
149  0.013816  0.998029  1.002576      0.775440  0.000256
150  0.012921  0.998686  0.970134      0.783268  0.000256
151  0.013648  0.998498  0.988147      0.779843  0.000256
152  0.013589  0.998217  0.993556      0.780333  0.000256
153  0.013330  0.998405  0.979176      0.784736  0.000256
154  0.012783  0.998592  0.976814      0.784247  0.000256
155  0.013173  0.998311  0.997763      0.777397  0.000256
156  0.012763  0.998592  0.986549      0.782779  0.000256
157  0.014875  0.998029  1.025667      0.773483  0.000256
158  0.013038  0.998405  1.044054      0.775440  0.000256
159  0.012741  0.998498  1.032301      0.780333  0.000256
160  0.012824  0.998592  0.993691      0.785714  0.000256
161  0.012882  0.998311  0.974613      0.786693  0.000256
162  0.012970  0.998592  0.983670      0.784247  0.000256
163  0.012263  0.998498  0.970999      0.787182  0.000256
164  0.012323  0.998498  0.983082      0.787182  0.000256
165  0.014098  0.998217  1.021647      0.777887  0.000256
166  0.013183  0.998217  1.014941      0.781311  0.000256
167  0.014670  0.998311  1.085273      0.768591  0.000256
168  0.013695  0.998029  1.079514      0.766145  0.000256
169  0.012859  0.998498  1.085633      0.765166  0.000256
170  0.012579  0.998498  1.068619      0.768591  0.000256
171  0.014257  0.997842  1.067210      0.767123  0.000256
172  0.011853  0.998686  1.107883      0.763209  0.000256
173  0.011974  0.998498  1.059222      0.772016  0.000256
174  0.012260  0.998498  1.062527      0.772994  0.000256
175  0.011606  0.998592  1.031195      0.776419  0.000256
176  0.012182  0.998592  1.234434      0.753425  0.000256
177  0.013281  0.998123  1.147731      0.763699  0.000256
178  0.011636  0.998686  1.130829      0.766145  0.000256
179  0.012273  0.998123  1.121772      0.765656  0.000256
180  0.011861  0.998498  1.076474      0.769080  0.000256
181  0.011716  0.998592  1.081747      0.768102  0.000256
182  0.012095  0.998217  1.075940      0.768102  0.000256
183  0.011561  0.998592  1.046250      0.767123  0.000256
184  0.011762  0.998592  1.066739      0.769080  0.000256
185  0.011725  0.998405  1.052263      0.770548  0.000256
186  0.011095  0.998780  1.084844      0.765656  0.000205
187  0.010470  0.998874  1.084366      0.769569  0.000205
188  0.011921  0.998592  1.090147      0.768591  0.000205
189  0.011359  0.998874  1.073397      0.770548  0.000205
190  0.011164  0.998780  1.074115      0.770548  0.000205
191  0.011419  0.998123  1.071575      0.773973  0.000205
192  0.011242  0.998592  1.082633      0.769569  0.000205
193  0.011106  0.998686  1.086336      0.765656  0.000205
194  0.010781  0.998874  1.089019      0.766634  0.000205
195  0.011251  0.998405  1.085084      0.766145  0.000205
196  0.011020  0.998592  1.107332      0.762720  0.000205
197  0.010948  0.998780  1.089206      0.765656  0.000205
198  0.011464  0.998968  1.138869      0.761252  0.000164
199  0.011026  0.998405  1.163904      0.756360  0.000164
200  0.010599  0.998968  1.140544      0.756849  0.000164
201  0.011279  0.998592  1.157353      0.759296  0.000164
202  0.010481  0.998686  1.130777      0.762720  0.000164
203  0.011059  0.998686  1.115716      0.763209  0.000164
204  0.011326  0.998780  1.148123      0.759785  0.000164
205  0.010268  0.998874  1.152245      0.761252  0.000164
206  0.010615  0.998592  1.160203      0.761252  0.000164
207  0.010499  0.998968  1.164257      0.758806  0.000164
208  0.010628  0.998592  1.158714      0.764188  0.000164
209  0.010601  0.998780  1.117769      0.765166  0.000164

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
batch_normalization_1 (Batch (None, 256, 36)           144       
_________________________________________________________________
bidirectional (Bidirectional (None, 256)               168960    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               25700     
_________________________________________________________________
dense_2 (Dense)              (None, 11)                1111      
=================================================================
Total params: 195,915
Trainable params: 195,843
Non-trainable params: 72
_________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
N hidden: 128
Learning rate: 0.0005
Regularization rate: 9.3e-05
Depth: 1
Merge mode: concat


Test Accuracy: 84.96124148368835
Test Loss: 0.6953771114349365



Classification Report
                   precision    recall  f1-score   support

            lying       0.96      0.95      0.95       182
          sitting       0.87      0.94      0.90       178
         standing       0.87      0.22      0.35       188
          walking       0.94      0.97      0.96       198
          running       1.00      0.91      0.95       173
          cycling       1.00      0.97      0.99       159
   nordic walking       0.97      0.98      0.98       207
 ascending stairs       0.70      0.86      0.77       104
descending stairs       0.60      0.85      0.70        88
  vacuum cleaning       0.86      0.62      0.72       164
          ironing       0.69      0.99      0.81       294

         accuracy                           0.85      1935
        macro avg       0.86      0.84      0.83      1935
     weighted avg       0.87      0.85      0.83      1935



Confusion Matrix
[[173   1   0   0   0   0   0   7   0   1   0]
 [  0 167   4   1   0   0   0   1   3   2   0]
 [  0  13  41   2   0   0   0   2   0   4 126]
 [  0   0   0 193   0   0   0   2   3   0   0]
 [  6   0   0   5 157   0   0   1   4   0   0]
 [  1   0   0   0   0 155   0   0   0   1   2]
 [  0   0   0   1   0   0 203   0   3   0   0]
 [  0   3   2   1   0   0   1  89   6   2   0]
 [  0   6   0   0   0   0   0   2  75   5   0]
 [  1   3   0   2   0   0   4  23  30 101   0]
 [  0   0   0   0   0   0   1   1   1   1 290]]


Normalised Confusion Matrix: True
                   lying  sitting  standing  walking  running  cycling  \
lying              95.05     0.55      0.00     0.00     0.00     0.00   
sitting             0.00    93.82      2.25     0.56     0.00     0.00   
standing            0.00     6.91     21.81     1.06     0.00     0.00   
walking             0.00     0.00      0.00    97.47     0.00     0.00   
running             3.47     0.00      0.00     2.89    90.75     0.00   
cycling             0.63     0.00      0.00     0.00     0.00    97.48   
nordic walking      0.00     0.00      0.00     0.48     0.00     0.00   
ascending stairs    0.00     2.88      1.92     0.96     0.00     0.00   
descending stairs   0.00     6.82      0.00     0.00     0.00     0.00   
vacuum cleaning     0.61     1.83      0.00     1.22     0.00     0.00   
ironing             0.00     0.00      0.00     0.00     0.00     0.00   

                   nordic walking  ascending stairs  descending stairs  \
lying                        0.00              3.85               0.00   
sitting                      0.00              0.56               1.69   
standing                     0.00              1.06               0.00   
walking                      0.00              1.01               1.52   
running                      0.00              0.58               2.31   
cycling                      0.00              0.00               0.00   
nordic walking              98.07              0.00               1.45   
ascending stairs             0.96             85.58               5.77   
descending stairs            0.00              2.27              85.23   
vacuum cleaning              2.44             14.02              18.29   
ironing                      0.34              0.34               0.34   

                   vacuum cleaning  ironing  
lying                         0.55     0.00  
sitting                       1.12     0.00  
standing                      2.13    67.02  
walking                       0.00     0.00  
running                       0.00     0.00  
cycling                       0.63     1.26  
nordic walking                0.00     0.00  
ascending stairs              1.92     0.00  
descending stairs             5.68     0.00  
vacuum cleaning              61.59     0.00  
ironing                       0.34    98.64  


Finished working on: BiLSTM at: 2021-02-15 11:40:49.573602 -> 1219.1841270923615

iSPLInception Model : 2021-02-15 12:46:27.087625
Model History 
         loss  accuracy  val_loss  val_accuracy        lr
0    6.466476  0.880818  5.134768      0.771037  0.000500
1    2.890735  0.933746  2.859728      0.831213  0.000500
2    1.612023  0.948761  1.893988      0.791585  0.000500
3    1.033607  0.958427  1.603871      0.780822  0.000500
4    0.750202  0.965372  1.299630      0.805284  0.000500
5    0.572113  0.967718  1.588961      0.684442  0.000500
6    0.481286  0.969876  1.090593      0.798434  0.000500
7    0.402877  0.970345  0.884407      0.848826  0.000500
8    0.370468  0.971190  0.988233      0.778865  0.000500
9    0.331095  0.973818  1.285138      0.713307  0.000500
10   0.296490  0.976445  0.835865      0.820939  0.000500
11   0.284058  0.977947  0.748148      0.876712  0.000500
12   0.277847  0.976727  1.322485      0.759785  0.000500
13   0.244822  0.980293  0.715980      0.868885  0.000500
14   0.238433  0.977947  0.748124      0.843933  0.000500
15   0.227004  0.978885  0.821490      0.828278  0.000500
16   0.226848  0.978510  0.805176      0.839530  0.000500
17   0.199337  0.985079  0.801195      0.795988  0.000500
18   0.208151  0.980011  0.826286      0.861546  0.000500
19   0.195811  0.981325  0.718987      0.878669  0.000500
20   0.197993  0.980574  0.702826      0.832681  0.000500
21   0.201838  0.980293  0.789652      0.817025  0.000500
22   0.177264  0.985267  0.692465      0.838552  0.000500
23   0.205397  0.980011  0.972625      0.813601  0.000500
24   0.176139  0.985267  0.725941      0.879158  0.000500
25   0.172364  0.983202  0.847921      0.822407  0.000500
26   0.170624  0.984234  0.851153      0.787671  0.000500
27   0.167930  0.983483  1.061829      0.708415  0.000500
28   0.176112  0.981325  0.887238      0.826321  0.000500
29   0.170796  0.983765  1.094376      0.773973  0.000500
30   0.168526  0.982827  1.007368      0.799902  0.000500
31   0.172186  0.983765  0.774700      0.827789  0.000500
32   0.153904  0.985267  0.683072      0.858611  0.000500
33   0.157104  0.984797  0.830755      0.780333  0.000500
34   0.152987  0.986299  1.257728      0.745597  0.000500
35   0.147349  0.986956  0.797661      0.837084  0.000500
36   0.144517  0.986768  0.676131      0.871331  0.000500
37   0.135583  0.988082  0.882134      0.725049  0.000500
38   0.149721  0.985079  0.840089      0.828278  0.000500
39   0.155699  0.984234  0.782570      0.861057  0.000500
40   0.165678  0.983296  0.626325      0.877202  0.000500
41   0.138845  0.988457  0.838468      0.827299  0.000500
42   0.160591  0.983202  0.937169      0.771526  0.000500
43   0.157787  0.986299  0.792123      0.855675  0.000500
44   0.161861  0.985360  0.661412      0.886497  0.000500
45   0.151073  0.986768  0.841213      0.818982  0.000500
46   0.149047  0.987143  0.681206      0.865460  0.000500
47   0.148218  0.986111  0.729598      0.815558  0.000500
48   0.122312  0.991742  0.627587      0.888943  0.000400
49   0.104947  0.992305  0.663824      0.877202  0.000400
50   0.098990  0.992492  0.633773      0.904110  0.000400
51   0.090075  0.994651  0.699973      0.841977  0.000400
52   0.092618  0.993056  0.730460      0.858611  0.000400
53   0.099573  0.990803  0.648816      0.880626  0.000400
54   0.092857  0.991929  0.747155      0.819961  0.000400
55   0.103181  0.989396  0.718710      0.863992  0.000400
56   0.120848  0.988270  0.850980      0.809198  0.000400
57   0.116112  0.988551  0.534772      0.886986  0.000400
58   0.116803  0.989114  0.629139      0.901663  0.000400
59   0.116326  0.989396  0.498128      0.899217  0.000400
60   0.103631  0.990897  0.774716      0.826810  0.000400
61   0.102254  0.990146  1.013779      0.765166  0.000400
62   0.092294  0.992305  0.595300      0.864481  0.000320
63   0.090744  0.992868  0.599617      0.881115  0.000320
64   0.080434  0.994745  0.680574      0.844912  0.000320
65   0.093201  0.991742  0.623430      0.876712  0.000320
66   0.098235  0.991929  0.698075      0.870842  0.000320
67   0.106739  0.989396  0.882695      0.825342  0.000320
68   0.097557  0.991554  0.794041      0.811155  0.000320
69   0.095074  0.991273  0.698134      0.835616  0.000320
70   0.080777  0.994369  0.671933      0.844912  0.000320
71   0.075994  0.993994  0.652321      0.865949  0.000320
72   0.081730  0.992680  0.658942      0.862035  0.000320
73   0.091715  0.991366  0.757033      0.866928  0.000320
74   0.089460  0.991085  0.683358      0.856164  0.000320
75   0.080477  0.994182  0.727139      0.826321  0.000320
76   0.087618  0.992586  0.538655      0.869863  0.000320
77   0.080963  0.993243  0.580727      0.864971  0.000320
78   0.087642  0.990428  0.652450      0.837573  0.000320
79   0.085338  0.991273  0.759721      0.819472  0.000320
80   0.086027  0.992399  0.638570      0.867906  0.000320
81   0.078860  0.993056  0.664773      0.866438  0.000320
82   0.068080  0.995589  0.586223      0.882094  0.000256
83   0.064173  0.995683  0.543522      0.877691  0.000256
84   0.066187  0.994276  0.760409      0.843444  0.000256
85   0.064244  0.995589  0.603261      0.884540  0.000256
86   0.067703  0.994182  0.503557      0.888943  0.000256
87   0.061656  0.995871  0.610206      0.859100  0.000256
88   0.061835  0.994932  0.681529      0.858121  0.000256
89   0.059692  0.994839  0.679646      0.863014  0.000256
90   0.064165  0.994369  0.661350      0.851272  0.000256
91   0.069229  0.993712  0.818306      0.836106  0.000256
92   0.070531  0.993431  0.817047      0.829256  0.000256
93   0.070005  0.994182  0.758455      0.832681  0.000256
94   0.068784  0.993806  0.735764      0.833170  0.000256
95   0.060331  0.995402  0.707477      0.857143  0.000256
96   0.057586  0.995589  0.744360      0.844423  0.000256
97   0.059182  0.994745  0.659726      0.874755  0.000256
98   0.057760  0.995589  0.855710      0.795010  0.000256
99   0.057749  0.995214  0.751317      0.838063  0.000256
100  0.064083  0.994745  0.877436      0.825832  0.000256
101  0.066416  0.994088  0.864853      0.839530  0.000256
102  0.085787  0.990053  0.575378      0.891879  0.000256
103  0.075596  0.992586  0.615651      0.858611  0.000256
104  0.069254  0.993900  0.619509      0.874755  0.000256
105  0.074352  0.992399  0.615617      0.871331  0.000256
106  0.071048  0.992962  0.678987      0.835127  0.000256
107  0.066744  0.995308  0.739056      0.846380  0.000205
108  0.058366  0.995777  0.866476      0.834638  0.000205
109  0.054695  0.996152  0.706816      0.833170  0.000205
110  0.051730  0.996903  0.653183      0.851761  0.000205
111  0.050062  0.996434  0.678850      0.837084  0.000205
112  0.051878  0.995965  0.639525      0.849804  0.000205
113  0.052694  0.996059  0.725006      0.832681  0.000205
114  0.050378  0.996246  0.580679      0.879158  0.000205
115  0.049494  0.995777  0.564180      0.876223  0.000205
116  0.051801  0.996246  0.633580      0.844423  0.000205
117  0.050999  0.996340  0.681815      0.845401  0.000205
118  0.054012  0.995214  0.667285      0.825342  0.000205
119  0.049836  0.995965  0.703871      0.820450  0.000205
120  0.056189  0.995214  0.670543      0.825342  0.000205
121  0.054881  0.995120  0.611531      0.831703  0.000205
122  0.052353  0.995777  0.544996      0.875245  0.000205
123  0.052629  0.995495  0.647084      0.830235  0.000205
124  0.053832  0.994839  0.745326      0.829256  0.000205
125  0.059247  0.994182  0.640968      0.848826  0.000205
126  0.049742  0.996528  0.492183      0.895793  0.000164
127  0.046439  0.997091  0.586840      0.869863  0.000164
128  0.045609  0.996997  0.659330      0.831703  0.000164
129  0.052006  0.996059  0.605581      0.854697  0.000164
130  0.046104  0.996809  0.708449      0.832192  0.000164
131  0.043165  0.997654  0.565541      0.878669  0.000164
132  0.045649  0.996152  0.589017      0.859589  0.000164
133  0.044523  0.996903  0.567499      0.885519  0.000164
134  0.043868  0.996528  0.590097      0.869863  0.000164
135  0.042601  0.996246  0.664010      0.838552  0.000164
136  0.044431  0.996903  0.640744      0.827299  0.000164
137  0.044256  0.996622  0.541766      0.882094  0.000164
138  0.043342  0.996622  0.558995      0.893346  0.000164
139  0.045719  0.995308  0.576215      0.887965  0.000164
140  0.041012  0.997466  0.561901      0.881115  0.000164
141  0.040897  0.997466  0.567216      0.886497  0.000164
142  0.040845  0.996809  0.519310      0.896282  0.000164
143  0.041607  0.996246  0.901523      0.790607  0.000164
144  0.047161  0.995120  0.648731      0.857143  0.000164
145  0.044554  0.995965  0.613046      0.867417  0.000164
146  0.050774  0.994745  0.867706      0.789139  0.000164
147  0.052037  0.995308  0.588850      0.829746  0.000164
148  0.049839  0.995777  0.630027      0.858121  0.000164
149  0.052043  0.994839  0.611756      0.846869  0.000164
150  0.050531  0.995495  0.783880      0.810665  0.000164
151  0.046894  0.995777  0.651014      0.855675  0.000164
152  0.047838  0.996340  0.638629      0.876712  0.000131
153  0.046188  0.995683  0.599337      0.881605  0.000131
154  0.044793  0.996434  0.508166      0.893836  0.000131
155  0.045297  0.996152  0.600867      0.875734  0.000131
156  0.042629  0.996903  0.622456      0.874266  0.000131
157  0.040796  0.997466  0.671567      0.859589  0.000131
158  0.038888  0.997279  0.610648      0.877202  0.000131
159  0.040577  0.996903  0.708564      0.840998  0.000131
160  0.039164  0.997560  0.553431      0.885029  0.000131
161  0.039845  0.996997  0.597569      0.863014  0.000131
162  0.042408  0.996340  0.579594      0.879158  0.000131
163  0.038989  0.996997  0.608597      0.867417  0.000131
164  0.040001  0.996903  0.564461      0.886986  0.000131
165  0.039767  0.996528  0.897525      0.762231  0.000131
166  0.038515  0.996903  0.615848      0.866928  0.000131
167  0.038567  0.997185  0.574911      0.865949  0.000131
168  0.037505  0.997091  0.552763      0.868885  0.000131
169  0.036979  0.997842  0.633916      0.855186  0.000131
170  0.038738  0.997279  0.529198      0.882583  0.000131
171  0.035795  0.997466  0.569526      0.884540  0.000131
172  0.037221  0.997654  0.551596      0.877691  0.000131
173  0.034570  0.997935  0.527619      0.884051  0.000131
174  0.035989  0.997279  0.560475      0.889922  0.000131
175  0.037408  0.997560  0.547478      0.886008  0.000131
176  0.034512  0.997654  0.574985      0.890411  0.000131
177  0.033841  0.997935  0.573316      0.877691  0.000131
178  0.039040  0.996340  0.740745      0.842955  0.000131
179  0.049508  0.994369  0.535543      0.885519  0.000131
180  0.043914  0.996715  0.583055      0.873288  0.000131
181  0.040996  0.996622  0.658973      0.863014  0.000131
182  0.042465  0.996528  0.663233      0.861057  0.000131
183  0.039501  0.996903  0.557080      0.886008  0.000131
184  0.038715  0.997091  0.597830      0.869374  0.000131
185  0.037898  0.996903  0.559417      0.875734  0.000131
186  0.035820  0.997935  0.547080      0.875245  0.000131
187  0.038704  0.996528  0.629817      0.848826  0.000131
188  0.037493  0.996903  0.580746      0.856654  0.000105
189  0.035797  0.997091  0.901738      0.810665  0.000105
190  0.033702  0.997842  0.708014      0.829256  0.000105
191  0.034248  0.998123  0.640549      0.827299  0.000105
192  0.035273  0.997372  0.606227      0.837084  0.000105
193  0.034602  0.997372  0.594258      0.837084  0.000105
194  0.034901  0.997185  0.603637      0.829746  0.000105
195  0.034465  0.997185  0.613022      0.857632  0.000105
196  0.033217  0.997842  0.664706      0.825342  0.000105
197  0.033071  0.998217  0.727133      0.811644  0.000105
198  0.034040  0.997372  0.694589      0.837573  0.000105
199  0.034136  0.997560  0.662433      0.854207  0.000105
200  0.041110  0.996434  0.715414      0.805773  0.000105
201  0.041143  0.996622  0.631587      0.829746  0.000105
202  0.037840  0.997279  0.541308      0.887476  0.000105
203  0.036881  0.996903  0.566199      0.878180  0.000105
204  0.036014  0.997185  0.608647      0.863992  0.000105
205  0.034324  0.997466  0.667130      0.840020  0.000105
206  0.033874  0.997748  0.624744      0.842955  0.000105
207  0.033265  0.997279  0.602572      0.843933  0.000105
208  0.033267  0.997560  0.600561      0.852251  0.000100
209  0.035721  0.996903  0.658490      0.822896  0.000100
210  0.033127  0.997654  0.571059      0.872798  0.000100
211  0.032392  0.997560  0.591177      0.869374  0.000100
212  0.033129  0.997185  0.652893      0.852251  0.000100
213  0.033743  0.997466  0.591094      0.868885  0.000100
214  0.033326  0.997466  0.640197      0.846869  0.000100
215  0.032013  0.998123  0.641146      0.835127  0.000100
216  0.034642  0.997372  0.583291      0.858611  0.000100
217  0.033522  0.997185  0.505686      0.877202  0.000100
218  0.032130  0.997842  0.556354      0.868885  0.000100
219  0.035926  0.996809  0.614962      0.861057  0.000100
220  0.036546  0.996715  0.637297      0.857143  0.000100
221  0.036329  0.996715  0.846336      0.814090  0.000100
222  0.039610  0.996246  0.588906      0.837084  0.000100
223  0.035917  0.997466  0.544819      0.850294  0.000100
224  0.035757  0.997185  0.683007      0.823386  0.000100
225  0.032613  0.997842  0.619390      0.832681  0.000100
226  0.032186  0.997842  0.510905      0.882583  0.000100
227  0.034281  0.997185  0.569856      0.874266  0.000100
228  0.031858  0.997842  0.600380      0.860568  0.000100
229  0.031988  0.997372  0.589810      0.871820  0.000100
230  0.033571  0.997279  0.675494      0.851272  0.000100
231  0.031847  0.997842  0.587347      0.861546  0.000100
232  0.032973  0.997372  0.713900      0.821429  0.000100
233  0.033712  0.997372  0.624799      0.862035  0.000100
234  0.034276  0.997466  0.636745      0.860568  0.000100
235  0.033089  0.997185  0.639451      0.852740  0.000100
236  0.033238  0.997372  0.657192      0.832192  0.000100
237  0.033606  0.997748  0.603037      0.859589  0.000100
238  0.031911  0.997748  0.588738      0.873777  0.000100
239  0.031223  0.998029  0.591309      0.880626  0.000100
240  0.038127  0.996059  0.524394      0.877691  0.000100
241  0.033981  0.996715  0.538902      0.888454  0.000100
242  0.032770  0.997748  0.739480      0.845890  0.000100
243  0.034404  0.996997  0.586479      0.884540  0.000100
244  0.033639  0.997654  0.654843      0.874755  0.000100
245  0.037275  0.996152  0.595125      0.867417  0.000100
246  0.033304  0.997748  0.654146      0.868885  0.000100
247  0.032064  0.997748  0.592754      0.872798  0.000100
248  0.031805  0.998123  0.567412      0.877202  0.000100
249  0.031822  0.997748  0.658837      0.858121  0.000100
250  0.031445  0.997560  0.645827      0.861057  0.000100
251  0.031107  0.997654  0.612422      0.866928  0.000100
252  0.031948  0.997560  0.610627      0.876223  0.000100
253  0.030910  0.998029  0.668336      0.857632  0.000100
254  0.031041  0.997654  0.579831      0.877691  0.000100
255  0.031506  0.997842  0.615458      0.873288  0.000100
256  0.031794  0.997748  0.620129      0.870842  0.000100
257  0.032125  0.997560  0.594934      0.870352  0.000100
258  0.034483  0.997279  0.543716      0.884540  0.000100
259  0.033138  0.997372  0.639218      0.855186  0.000100
260  0.034734  0.996340  0.629919      0.867417  0.000100
261  0.034618  0.996903  0.514032      0.894814  0.000100
262  0.033588  0.997091  0.724147      0.847847  0.000100
263  0.033437  0.997748  0.587934      0.860568  0.000100
264  0.030910  0.997466  0.568475      0.863503  0.000100
265  0.030831  0.997748  0.562843      0.878669  0.000100
266  0.032580  0.997466  0.613907      0.872798  0.000100
267  0.031637  0.997654  0.600905      0.868395  0.000100
268  0.030948  0.997560  0.565982      0.872798  0.000100
269  0.030921  0.997842  0.548826      0.873288  0.000100
270  0.030121  0.997560  0.572381      0.867417  0.000100
271  0.030956  0.997842  0.582598      0.864971  0.000100
272  0.029985  0.997842  0.609157      0.858611  0.000100
273  0.029760  0.998217  0.594523      0.842955  0.000100
274  0.030202  0.997842  0.545712      0.869374  0.000100
275  0.030000  0.997935  0.607969      0.867417  0.000100
276  0.030070  0.997935  0.579446      0.868395  0.000100
277  0.032913  0.997466  0.559788      0.860568  0.000100
278  0.037334  0.996340  0.623649      0.855186  0.000100
279  0.040053  0.995402  0.594484      0.863503  0.000100
280  0.036232  0.996622  0.573779      0.883562  0.000100
281  0.033227  0.997372  0.664375      0.851761  0.000100
282  0.032289  0.997842  0.573703      0.885029  0.000100
283  0.031716  0.997654  0.597793      0.868395  0.000100
284  0.030471  0.998311  0.550281      0.874755  0.000100
285  0.031151  0.997466  0.539781      0.878669  0.000100
286  0.031890  0.997935  0.510595      0.880626  0.000100
287  0.029708  0.998217  0.577509      0.879158  0.000100
288  0.031121  0.997748  0.614121      0.840020  0.000100
289  0.030575  0.997748  0.579878      0.862035  0.000100
290  0.030198  0.997654  0.656163      0.835616  0.000100
291  0.029563  0.998123  0.645328      0.837573  0.000100
292  0.030113  0.997748  0.588886      0.836595  0.000100
293  0.032609  0.997466  0.612054      0.840020  0.000100
294  0.030057  0.998311  0.587611      0.829256  0.000100
295  0.031459  0.996903  0.675819      0.817025  0.000100
296  0.035110  0.996715  0.545641      0.876223  0.000100
297  0.032165  0.997842  0.557618      0.879158  0.000100
298  0.030058  0.997935  0.539868      0.881115  0.000100
299  0.033064  0.996997  0.543691      0.869374  0.000100
300  0.037510  0.996434  0.639112      0.826810  0.000100
301  0.033034  0.997185  0.745947      0.802838  0.000100
302  0.033474  0.997372  0.636048      0.827299  0.000100
303  0.030914  0.997654  0.718302      0.807730  0.000100
304  0.030413  0.997935  0.719778      0.818982  0.000100
305  0.031203  0.997466  0.618773      0.864481  0.000100
306  0.032213  0.997185  0.617375      0.841487  0.000100
307  0.030132  0.998311  0.576072      0.863014  0.000100
308  0.030655  0.998029  0.745556      0.810176  0.000100
309  0.030143  0.997748  0.548497      0.879648  0.000100
310  0.030174  0.997748  0.611426      0.839530  0.000100
311  0.029435  0.998405  0.531876      0.864971  0.000100
312  0.031396  0.997372  0.500626      0.888454  0.000100
313  0.030379  0.997748  0.568956      0.862524  0.000100
314  0.029875  0.997935  0.594118      0.869374  0.000100
315  0.029580  0.997935  0.658767      0.837573  0.000100
316  0.031417  0.997372  0.601881      0.867906  0.000100
317  0.030627  0.997560  0.571099      0.863503  0.000100
318  0.034516  0.996715  0.621973      0.836106  0.000100
319  0.033483  0.997279  0.673764      0.829256  0.000100
320  0.031675  0.998123  0.529128      0.878669  0.000100
321  0.030282  0.997466  0.585972      0.870352  0.000100
322  0.035391  0.997091  0.624695      0.875245  0.000100
323  0.032931  0.997185  0.649365      0.864481  0.000100
324  0.031719  0.997279  0.545444      0.885519  0.000100
325  0.034158  0.996997  0.630286      0.856164  0.000100
326  0.031307  0.997279  0.589420      0.874755  0.000100
327  0.029992  0.997842  0.671184      0.853229  0.000100
328  0.029150  0.998311  0.574665      0.873777  0.000100
329  0.031243  0.997842  0.572500      0.880137  0.000100
330  0.030549  0.997748  0.559775      0.876712  0.000100
331  0.029673  0.998123  0.589663      0.873288  0.000100
332  0.028637  0.998217  0.625394      0.864481  0.000100
333  0.029902  0.997935  0.589196      0.875734  0.000100
334  0.029984  0.997372  0.605849      0.848826  0.000100
335  0.035394  0.996903  0.505253      0.885029  0.000100
336  0.031715  0.997466  0.552481      0.878180  0.000100
337  0.030992  0.997654  0.564045      0.853718  0.000100
338  0.029181  0.998029  0.654626      0.818493  0.000100
339  0.031986  0.996903  0.689226      0.829256  0.000100
340  0.032162  0.997748  0.733098      0.805284  0.000100
341  0.031215  0.997372  0.587689      0.857143  0.000100
342  0.032721  0.996997  0.546852      0.868885  0.000100
343  0.030872  0.997466  0.631780      0.850294  0.000100
344  0.031280  0.997466  0.634827      0.830235  0.000100
345  0.029815  0.998029  0.554449      0.869374  0.000100
346  0.028423  0.998123  0.646732      0.822896  0.000100
347  0.034973  0.996528  0.588568      0.862035  0.000100
348  0.033208  0.996997  0.731902      0.815558  0.000100
349  0.035278  0.996622  0.562054      0.873777  0.000100

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 256, 36)]    0                                            
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 256, 36)      144         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 256, 32)      1152        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 256, 36)      0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 256, 64)      139264      conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 256, 64)      69632       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 256, 64)      34816       conv1d[0][0]                     
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 256, 64)      2304        max_pooling1d[0][0]              
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 256, 256)     0           conv1d_1[0][0]                   
                                                                 conv1d_2[0][0]                   
                                                                 conv1d_3[0][0]                   
                                                                 conv1d_4[0][0]                   
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 256, 256)     1024        concatenate[0][0]                
__________________________________________________________________________________________________
activation (Activation)         (None, 256, 256)     0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 256, 32)      8192        activation[0][0]                 
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 256, 256)     0           activation[0][0]                 
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 256, 64)      139264      conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 256, 64)      69632       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 256, 64)      34816       conv1d_5[0][0]                   
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 256, 64)      16384       max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 256, 256)     0           conv1d_6[0][0]                   
                                                                 conv1d_7[0][0]                   
                                                                 conv1d_8[0][0]                   
                                                                 conv1d_9[0][0]                   
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 256, 256)     1024        concatenate_1[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 256, 256)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 256, 32)      8192        activation_1[0][0]               
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 256, 256)     0           activation_1[0][0]               
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 256, 64)      139264      conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_12 (Conv1D)              (None, 256, 64)      69632       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_13 (Conv1D)              (None, 256, 64)      34816       conv1d_10[0][0]                  
__________________________________________________________________________________________________
conv1d_14 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256, 256)     0           conv1d_11[0][0]                  
                                                                 conv1d_12[0][0]                  
                                                                 conv1d_13[0][0]                  
                                                                 conv1d_14[0][0]                  
__________________________________________________________________________________________________
conv1d_15 (Conv1D)              (None, 256, 256)     9216        batch_normalization_2[0][0]      
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 256, 256)     1024        concatenate_2[0][0]              
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 256, 256)     1024        conv1d_15[0][0]                  
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 256, 256)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
add (Add)                       (None, 256, 256)     0           batch_normalization_6[0][0]      
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 256, 256)     0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_16 (Conv1D)              (None, 256, 32)      8192        activation_3[0][0]               
__________________________________________________________________________________________________
max_pooling1d_3 (MaxPooling1D)  (None, 256, 256)     0           activation_3[0][0]               
__________________________________________________________________________________________________
conv1d_17 (Conv1D)              (None, 256, 64)      139264      conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_18 (Conv1D)              (None, 256, 64)      69632       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_19 (Conv1D)              (None, 256, 64)      34816       conv1d_16[0][0]                  
__________________________________________________________________________________________________
conv1d_20 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_3[0][0]            
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256)     0           conv1d_17[0][0]                  
                                                                 conv1d_18[0][0]                  
                                                                 conv1d_19[0][0]                  
                                                                 conv1d_20[0][0]                  
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 256, 256)     1024        concatenate_3[0][0]              
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 256, 256)     0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv1d_21 (Conv1D)              (None, 256, 32)      8192        activation_4[0][0]               
__________________________________________________________________________________________________
max_pooling1d_4 (MaxPooling1D)  (None, 256, 256)     0           activation_4[0][0]               
__________________________________________________________________________________________________
conv1d_22 (Conv1D)              (None, 256, 64)      139264      conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_23 (Conv1D)              (None, 256, 64)      69632       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_24 (Conv1D)              (None, 256, 64)      34816       conv1d_21[0][0]                  
__________________________________________________________________________________________________
conv1d_25 (Conv1D)              (None, 256, 64)      16384       max_pooling1d_4[0][0]            
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 256, 256)     0           conv1d_22[0][0]                  
                                                                 conv1d_23[0][0]                  
                                                                 conv1d_24[0][0]                  
                                                                 conv1d_25[0][0]                  
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 256, 256)     1024        concatenate_4[0][0]              
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 256, 256)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 256)          0           activation_5[0][0]               
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 11)           2827        global_average_pooling1d[0][0]   
==================================================================================================
Total params: 1,338,651
Trainable params: 1,335,507
Non-trainable params: 3,144
__________________________________________________________________________________________________

+++Hyperparameters+++
Number of Epochs: 350
Batch Size: 64
Learning rate: 0.0005
Regularization rate: 0.00593
Network depth: 5
Filters number: 64
Max kernel size: 68
Use residual: True
Use bottleneck: True


Test Accuracy: 87.80362010002136
Test Loss: 0.6309902667999268



Classification Report
                   precision    recall  f1-score   support

            lying       0.97      0.95      0.96       182
          sitting       0.96      0.92      0.94       178
         standing       0.91      0.32      0.47       188
          walking       0.96      0.97      0.96       198
          running       1.00      0.92      0.96       173
          cycling       0.97      0.97      0.97       159
   nordic walking       0.97      0.99      0.98       207
 ascending stairs       0.84      0.83      0.83       104
descending stairs       0.73      0.86      0.79        88
  vacuum cleaning       0.63      0.85      0.73       164
          ironing       0.80      0.98      0.88       294

         accuracy                           0.88      1935
        macro avg       0.89      0.87      0.86      1935
     weighted avg       0.89      0.88      0.87      1935



Confusion Matrix
[[173   1   0   0   0   4   0   2   0   2   0]
 [  0 164   5   0   0   0   0   0   0   4   5]
 [  0   2  60   0   0   0   0   0   0  62  64]
 [  0   0   0 192   0   0   0   2   4   0   0]
 [  1   0   0   2 160   0   0   0   9   0   1]
 [  4   0   0   0   0 155   0   0   0   0   0]
 [  0   0   0   2   0   0 205   0   0   0   0]
 [  0   1   0   5   0   0   0  86  10   0   2]
 [  0   3   0   0   0   0   0   1  76   7   1]
 [  0   0   1   0   0   0   6  11   5 140   1]
 [  0   0   0   0   0   0   0   0   0   6 288]]


Normalised Confusion Matrix: True
                   lying  sitting  standing  walking  running  cycling  \
lying              95.05     0.55      0.00     0.00     0.00     2.20   
sitting             0.00    92.13      2.81     0.00     0.00     0.00   
standing            0.00     1.06     31.91     0.00     0.00     0.00   
walking             0.00     0.00      0.00    96.97     0.00     0.00   
running             0.58     0.00      0.00     1.16    92.49     0.00   
cycling             2.52     0.00      0.00     0.00     0.00    97.48   
nordic walking      0.00     0.00      0.00     0.97     0.00     0.00   
ascending stairs    0.00     0.96      0.00     4.81     0.00     0.00   
descending stairs   0.00     3.41      0.00     0.00     0.00     0.00   
vacuum cleaning     0.00     0.00      0.61     0.00     0.00     0.00   
ironing             0.00     0.00      0.00     0.00     0.00     0.00   

                   nordic walking  ascending stairs  descending stairs  \
lying                        0.00              1.10               0.00   
sitting                      0.00              0.00               0.00   
standing                     0.00              0.00               0.00   
walking                      0.00              1.01               2.02   
running                      0.00              0.00               5.20   
cycling                      0.00              0.00               0.00   
nordic walking              99.03              0.00               0.00   
ascending stairs             0.00             82.69               9.62   
descending stairs            0.00              1.14              86.36   
vacuum cleaning              3.66              6.71               3.05   
ironing                      0.00              0.00               0.00   

                   vacuum cleaning  ironing  
lying                         1.10     0.00  
sitting                       2.25     2.81  
standing                     32.98    34.04  
walking                       0.00     0.00  
running                       0.00     0.58  
cycling                       0.00     0.00  
nordic walking                0.00     0.00  
ascending stairs              0.00     1.92  
descending stairs             7.95     1.14  
vacuum cleaning              85.37     0.61  
ironing                       2.04    97.96  


Finished working on: iSPLInception at: 2021-02-15 12:46:29.773927 -> 3940.1993267536163

